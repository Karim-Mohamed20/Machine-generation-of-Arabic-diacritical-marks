{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate -O example.zip https://drive.google.com/uc?id=1VTGxBlf-wyO6CjAKWikaVnZZ-YPQpYJB\n",
        "!unzip example.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwNip-5A9Xf_",
        "outputId": "5a9c19c4-3a99-4ab7-a944-bd726f56789b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-01 21:01:12--  https://drive.google.com/uc?id=1VTGxBlf-wyO6CjAKWikaVnZZ-YPQpYJB\n",
            "Resolving drive.google.com (drive.google.com)... 142.251.175.100, 142.251.175.102, 142.251.175.139, ...\n",
            "Connecting to drive.google.com (drive.google.com)|142.251.175.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-14-0s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/p0lfqal86mnbb22r5js8sm8kat937bs1/1704142875000/16601359149676800360/*/1VTGxBlf-wyO6CjAKWikaVnZZ-YPQpYJB?uuid=d6a6366c-3a76-4bb0-823a-91eb0bddae7e [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2024-01-01 21:01:15--  https://doc-14-0s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/p0lfqal86mnbb22r5js8sm8kat937bs1/1704142875000/16601359149676800360/*/1VTGxBlf-wyO6CjAKWikaVnZZ-YPQpYJB?uuid=d6a6366c-3a76-4bb0-823a-91eb0bddae7e\n",
            "Resolving doc-14-0s-docs.googleusercontent.com (doc-14-0s-docs.googleusercontent.com)... 74.125.200.132, 2404:6800:4003:c00::84\n",
            "Connecting to doc-14-0s-docs.googleusercontent.com (doc-14-0s-docs.googleusercontent.com)|74.125.200.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9519939 (9.1M) [application/x-zip-compressed]\n",
            "Saving to: ‘example.zip’\n",
            "\n",
            "example.zip         100%[===================>]   9.08M  24.5MB/s    in 0.4s    \n",
            "\n",
            "2024-01-01 21:01:16 (24.5 MB/s) - ‘example.zip’ saved [9519939/9519939]\n",
            "\n",
            "Archive:  example.zip\n",
            "  inflating: train.txt               \n",
            "  inflating: val.txt                 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nitRY9riXhIk",
        "outputId": "c97f2b7e-80d5-4354-f8d2-e0d274906788"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/68.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/68.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m61.4/68.8 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m764.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.11.1-py3-none-any.whl (227 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.23.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp310-cp310-linux_x86_64.whl size=4199777 sha256=63f9f5c66ff1516556ce7638c4af80651080f7988782934d6d9188a076b9f48e\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/13/75/f811c84a8ab36eedbaef977a6a58a98990e8e0f1967f98f394\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.11.1\n"
          ]
        }
      ],
      "source": [
        "# %pip uninstall tensorflow\n",
        "# %pip install tensorflow\n",
        "# %pip install keras\n",
        "# %pip install gensim\n",
        "# %pip install nltk\n",
        "# %pip install torch\n",
        "%pip install fasttext\n",
        "\n",
        "import re\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# import tensorflow as tf\n",
        "import nltk, re\n",
        "# from keras.preprocessing.text import Tokenizer\n",
        "from datetime import datetime\n",
        "from gensim.models import *\n",
        "import logging\n",
        "import fasttext\n",
        "# from rnn_utils import *\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "''' D_NAMES: This is a list containing names of various Arabic diacritics. Each\n",
        " element of the list represents a specific diacritic type. '''\n",
        "D_NAMES = ['Fathatan', 'Dammatan', 'Kasratan', 'Fatha', 'Damma', 'Kasra', 'Shadda', 'Sukun']\n",
        "\n",
        "##############################################################################################\n",
        "\n",
        "''' NAME2DIACRITIC: This uses a dictionary comprehension to create a mapping\n",
        "from diacritic names to their corresponding Unicode characters.'''\n",
        "NAME2DIACRITIC = dict((name, chr(code)) for name, code in zip(D_NAMES, range(0x064B, 0x0653)))\n",
        "\n",
        "##############################################################################################\n",
        "\n",
        "''' DIACRITIC2NAME: This is the inverse of the previous dictionary.'''\n",
        "DIACRITIC2NAME = dict((code, name) for name, code in NAME2DIACRITIC.items())\n",
        "\n",
        "##############################################################################################\n",
        "\n",
        "''' ARABIC_DIACRITICS: This creates a frozenset containing the Unicode\n",
        " characters of all the diacritics.'''\n",
        "ARABIC_DIACRITICS = frozenset(NAME2DIACRITIC.values())\n",
        "\n",
        "\n",
        "# Remove all standard diacritics from the text, leaving the letters only.\n",
        "def clear_diacritics(text):\n",
        "    assert isinstance(text, str)\n",
        "    return ''.join([l for l in text if l not in ARABIC_DIACRITICS])\n",
        "\n",
        "\n",
        "# Return the diacritics from the text while keeping their original positions.\n",
        "def extract_diacritics(text):\n",
        "    assert isinstance(text, str)\n",
        "    diacritics = []\n",
        "    classes = []\n",
        "    temp = ''\n",
        "    for i in range(1, len(text)):\n",
        "        temp = ''\n",
        "        if text[i] in ARABIC_DIACRITICS:\n",
        "            if text[i-1] == NAME2DIACRITIC['Shadda']:\n",
        "                diacritics[-1] = (DIACRITIC2NAME[text[i-1]], DIACRITIC2NAME[text[i]])\n",
        "                temp = (DIACRITIC2NAME[text[i-1]], DIACRITIC2NAME[text[i]])\n",
        "                if (temp == ('Shadda', 'Fatha')):\n",
        "                    classes.pop()\n",
        "                    classes.append(8)\n",
        "                elif (temp == ('Shadda', 'Fathatan')):\n",
        "                    classes.pop()\n",
        "                    classes.append(9)\n",
        "                elif (temp == ('Shadda', 'Damma')):\n",
        "                    classes.pop()\n",
        "                    classes.append(10)\n",
        "                elif (temp == ('Shadda', 'Dammatan')):\n",
        "                    classes.pop()\n",
        "                    classes.append(11)\n",
        "                elif (temp == ('Shadda', 'Kasra')):\n",
        "                    classes.pop()\n",
        "                    classes.append(12)\n",
        "                elif (temp == ('Shadda', 'Kasratan')):\n",
        "                    classes.pop()\n",
        "                    classes.append(13)\n",
        "            else:\n",
        "                diacritics.append(DIACRITIC2NAME[text[i]])\n",
        "                temp = DIACRITIC2NAME[text[i]]\n",
        "                if (temp == 'Fatha'):\n",
        "                    classes.append(0)\n",
        "                elif (temp == 'Fathatan'):\n",
        "                    classes.append(1)\n",
        "                elif (temp == 'Damma'):\n",
        "                    classes.append(2)\n",
        "                elif (temp == 'Dammatan'):\n",
        "                    classes.append(3)\n",
        "                elif (temp == 'Kasra'):\n",
        "                    classes.append(4)\n",
        "                elif (temp == 'Kasratan'):\n",
        "                    classes.append(5)\n",
        "                elif (temp == 'Sukun'):\n",
        "                    classes.append(6)\n",
        "                elif (temp == 'Shadda'):\n",
        "                    classes.append(7)\n",
        "        elif text[i - 1] not in ARABIC_DIACRITICS:\n",
        "            diacritics.append('')\n",
        "            classes.append(14)\n",
        "\n",
        "    if text[-1] not in ARABIC_DIACRITICS:\n",
        "        diacritics.append('')\n",
        "        classes.append(14)\n",
        "    return diacritics, classes\n",
        "\n",
        "\n",
        "def extract_arabic_words2(text):\n",
        "    arabic_pattern = re.compile('[\\u0600-\\u06FF]+')\n",
        "    arabic_matches = arabic_pattern.findall(text)\n",
        "    result = ' '.join(arabic_matches)\n",
        "    processed_text = re.sub(r'[؛،\\.؟]+', '', result)\n",
        "    final_processed_text = re.sub(r'\\s+', ' ', processed_text)\n",
        "    return final_processed_text\n",
        "\n",
        "\n",
        "input_file_path = \"train.txt\"  # Replace with your input file path\n",
        "\n",
        "def get_vectors_labels(input_file_path):\n",
        "    with open(input_file_path, \"r\", encoding=\"utf-8\") as input_file:\n",
        "        input_text = input_file.read()\n",
        "\n",
        "    arabic_words = extract_arabic_words2(input_text)\n",
        "\n",
        "    output_words = clear_diacritics(arabic_words)\n",
        "    words = output_words.split()\n",
        "    words2 = arabic_words.split()\n",
        "    words_array = [list(word) for word in words]\n",
        "    words_array2 = [list(word2) for word2 in words2]\n",
        "\n",
        "    output_without_spaces = arabic_words.replace(\" \", \"\")\n",
        "    output_without_spaces2 = output_words.replace(\" \", \"\")\n",
        "    array_of_chars = list(output_without_spaces)\n",
        "    _,classes_extraction = extract_diacritics (output_without_spaces)\n",
        "\n",
        "\n",
        "    num_feature = 30\n",
        "    min_word_count = 1\n",
        "    num_thread = 5\n",
        "    window_size = 10\n",
        "    down_sampling = 0.001\n",
        "    iteration = 20\n",
        "\n",
        "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "    model_fastText = FastText(words_array,\n",
        "                            vector_size=num_feature,\n",
        "                            window=window_size,\n",
        "                            min_count=min_word_count,\n",
        "                            workers=num_thread)\n",
        "\n",
        "\n",
        "    j=0\n",
        "    chars =[]\n",
        "    char_vectors =[]\n",
        "    char_classes=[]\n",
        "    for word in words_array:\n",
        "        for char in word:\n",
        "            chars.append(char)\n",
        "            char_classes.append(classes_extraction[j])\n",
        "            vector = model_fastText.wv[char]\n",
        "            char_vectors.append(vector)\n",
        "            j=j+1\n",
        "    return chars, char_classes, char_vectors\n",
        "\n",
        "# print (j)\n",
        "# print(chars[1])\n",
        "# print(char_classes[1])\n",
        "# print(char_vectors[1])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_chars , train_char_classes, train_char_vectors = get_vectors_labels(input_file_path)\n",
        "cv_chars , cv_char_classes, cv_char_vectors = get_vectors_labels(\"val.txt\")\n",
        "\n",
        "print (len(train_chars))\n",
        "print (len(train_char_classes))\n",
        "print (len(train_char_vectors))\n",
        "print (train_chars[1])\n",
        "print (train_char_classes[1])\n",
        "print (train_char_vectors[1])\n",
        "\n",
        "print (len(cv_chars))\n",
        "print (len(cv_char_classes))\n",
        "print (len(cv_char_vectors))\n"
      ],
      "metadata": {
        "id": "FipGctwbI_su",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c86fec94-64ea-4294-bf18-75fa4a5ae858"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8351478\n",
            "8351478\n",
            "8351478\n",
            "و\n",
            "6\n",
            "[ 0.15983047  0.45807773  0.39134502  0.06237854  0.25864896 -0.4497411\n",
            " -0.10579058  0.13231084 -0.29463574 -0.40685922 -0.57261354 -0.2514992\n",
            " -0.01655538  0.18161273 -0.2533154   0.23622485  0.45504826 -0.31396192\n",
            " -0.13635027  0.11519387  0.00759403  0.30333874 -0.19819988  0.39450607\n",
            " -0.30448318 -0.33920914 -0.7183431  -0.2891009  -0.36085957 -0.1004525 ]\n",
            "421099\n",
            "421099\n",
            "421099\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# old code\n",
        "\n",
        "# !pip install tensorflow\n",
        "# !pip install keras\n",
        "# !pip install gensim\n",
        "# import re\n",
        "# from collections import Counter\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import tensorflow as tf\n",
        "# import nltk, re\n",
        "# from keras.preprocessing.text import Tokenizer\n",
        "# from datetime import datetime\n",
        "# from gensim.models import *\n",
        "# import logging\n",
        "# # from rnn_utils import *\n",
        "# %matplotlib inline\n",
        "\n",
        "\n",
        "# ''' D_NAMES: This is a list containing names of various Arabic diacritics. Each\n",
        "#  element of the list represents a specific diacritic type. '''\n",
        "# D_NAMES = ['Fathatan', 'Dammatan', 'Kasratan', 'Fatha', 'Damma', 'Kasra', 'Shadda', 'Sukun']\n",
        "\n",
        "# ##############################################################################################\n",
        "\n",
        "# ''' NAME2DIACRITIC: This uses a dictionary comprehension to create a mapping\n",
        "# from diacritic names to their corresponding Unicode characters.'''\n",
        "# NAME2DIACRITIC = dict((name, chr(code)) for name, code in zip(D_NAMES, range(0x064B, 0x0653)))\n",
        "\n",
        "# ##############################################################################################\n",
        "\n",
        "# ''' DIACRITIC2NAME: This is the inverse of the previous dictionary.'''\n",
        "# DIACRITIC2NAME = dict((code, name) for name, code in NAME2DIACRITIC.items())\n",
        "\n",
        "# ##############################################################################################\n",
        "\n",
        "# ''' ARABIC_DIACRITICS: This creates a frozenset containing the Unicode\n",
        "#  characters of all the diacritics.'''\n",
        "# ARABIC_DIACRITICS = frozenset(NAME2DIACRITIC.values())\n",
        "\n",
        "\n",
        "# # Remove all standard diacritics from the text, leaving the letters only.\n",
        "# def clear_diacritics(text):\n",
        "#     assert isinstance(text, str)\n",
        "#     return ''.join([l for l in text if l not in ARABIC_DIACRITICS])\n",
        "\n",
        "\n",
        "# # Return the diacritics from the text while keeping their original positions.\n",
        "# def extract_diacritics(text):\n",
        "#     assert isinstance(text, str)\n",
        "#     diacritics = []\n",
        "#     classes = []\n",
        "#     temp = ''\n",
        "#     for i in range(1, len(text)):\n",
        "#         temp = ''\n",
        "#         if text[i] in ARABIC_DIACRITICS:\n",
        "#             if text[i-1] == NAME2DIACRITIC['Shadda']:\n",
        "#                 diacritics[-1] = (DIACRITIC2NAME[text[i-1]], DIACRITIC2NAME[text[i]])\n",
        "#                 temp = (DIACRITIC2NAME[text[i-1]], DIACRITIC2NAME[text[i]])\n",
        "#                 if (temp == ('Shadda', 'Fatha')):\n",
        "#                     classes.pop()\n",
        "#                     classes.append(8)\n",
        "#                 elif (temp == ('Shadda', 'Fathatan')):\n",
        "#                     classes.pop()\n",
        "#                     classes.append(9)\n",
        "#                 elif (temp == ('Shadda', 'Damma')):\n",
        "#                     classes.pop()\n",
        "#                     classes.append(10)\n",
        "#                 elif (temp == ('Shadda', 'Dammatan')):\n",
        "#                     classes.pop()\n",
        "#                     classes.append(11)\n",
        "#                 elif (temp == ('Shadda', 'Kasra')):\n",
        "#                     classes.pop()\n",
        "#                     classes.append(12)\n",
        "#                 elif (temp == ('Shadda', 'Kasratan')):\n",
        "#                     classes.pop()\n",
        "#                     classes.append(13)\n",
        "#             else:\n",
        "#                 diacritics.append(DIACRITIC2NAME[text[i]])\n",
        "#                 temp = DIACRITIC2NAME[text[i]]\n",
        "#                 if (temp == 'Fatha'):\n",
        "#                     classes.append(0)\n",
        "#                 elif (temp == 'Fathatan'):\n",
        "#                     classes.append(1)\n",
        "#                 elif (temp == 'Damma'):\n",
        "#                     classes.append(2)\n",
        "#                 elif (temp == 'Dammatan'):\n",
        "#                     classes.append(3)\n",
        "#                 elif (temp == 'Kasra'):\n",
        "#                     classes.append(4)\n",
        "#                 elif (temp == 'Kasratan'):\n",
        "#                     classes.append(5)\n",
        "#                 elif (temp == 'Sukun'):\n",
        "#                     classes.append(6)\n",
        "#                 elif (temp == 'Shadda'):\n",
        "#                     classes.append(7)\n",
        "#         elif text[i - 1] not in ARABIC_DIACRITICS:\n",
        "#             diacritics.append('')\n",
        "#             classes.append(14)\n",
        "\n",
        "#     if text[-1] not in ARABIC_DIACRITICS:\n",
        "#         diacritics.append('')\n",
        "#         classes.append(14)\n",
        "#     return diacritics, classes\n",
        "\n",
        "\n",
        "# def extract_arabic_words2(text):\n",
        "#     arabic_pattern = re.compile('[\\u0600-\\u06FF]+')\n",
        "#     arabic_matches = arabic_pattern.findall(text)\n",
        "#     result = ' '.join(arabic_matches)\n",
        "#     processed_text = re.sub(r'[؛،\\.]+', '', result)\n",
        "#     final_processed_text = re.sub(r'\\s+', ' ', processed_text)\n",
        "#     return final_processed_text\n",
        "\n",
        "\n",
        "# input_file_path = \"train.txt\"  # Replace with your input file path\n",
        "# with open(input_file_path, \"r\", encoding=\"utf-8\") as input_file:\n",
        "#     input_text = input_file.read()\n",
        "\n",
        "# arabic_words = extract_arabic_words2(input_text)\n",
        "\n",
        "# output_words = clear_diacritics(arabic_words)\n",
        "# words = output_words.split()\n",
        "# words2 = arabic_words.split()\n",
        "# words_array = [list(word) for word in words]\n",
        "# words_array2 = [list(word2) for word2 in words2]\n",
        "\n",
        "# output_without_spaces = arabic_words.replace(\" \", \"\")\n",
        "# output_without_spaces2 = output_words.replace(\" \", \"\")\n",
        "# array_of_chars = [char for char in output_without_spaces]\n",
        "# _,classes_extraction = extract_diacritics (output_without_spaces)\n",
        "\n",
        "\n",
        "# num_feature = 10\n",
        "# min_word_count = 1\n",
        "# num_thread = 5\n",
        "# window_size = 10\n",
        "# down_sampling = 0.001\n",
        "# iteration = 20\n",
        "\n",
        "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "# model_fastText = FastText(words_array,\n",
        "#                         vector_size=num_feature,\n",
        "#                         window=window_size,\n",
        "#                         min_count=min_word_count,\n",
        "#                         workers=num_thread)\n",
        "\n",
        "\n",
        "# j=0\n",
        "# chars =[]\n",
        "# char_vectors =[]\n",
        "# char_classes=[]\n",
        "# for word in words_array:\n",
        "#   for char in word:\n",
        "#     chars.append(char)\n",
        "#     char_classes.append(classes_extraction[j])\n",
        "#     vector = model_fastText.wv[char]\n",
        "#     char_vectors.append(vector)\n",
        "#     j=j+1\n",
        "\n",
        "# print (j)\n",
        "# print(chars[1])\n",
        "# print(char_classes[1])\n",
        "# print(char_vectors[1])\n"
      ],
      "metadata": {
        "id": "6BP4jM98Vms5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # prompt: save char_vectors and char_classes to disk to free some ram\n",
        "\n",
        "# import pickle\n",
        "# with open('char_vectors.pickle', 'wb') as handle:\n",
        "#   pickle.dump(char_vectors, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# with open('char_classes.pickle', 'wb') as handle:\n",
        "#   pickle.dump(char_classes, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7P0X8WiyERmk",
        "outputId": "7bbd71b2-3f37-481b-ac71-3616f3bb8976"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function _xla_gc_callback at 0x7ffb3eca1120>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/lib/__init__.py\", line 97, in _xla_gc_callback\n",
            "    def _xla_gc_callback(*args):\n",
            "KeyboardInterrupt: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(char_vectors))\n",
        "print(len(char_classes))\n",
        "print(char_vectors[:4])\n",
        "print(char_classes[:4])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZZy2t_rZpNp",
        "outputId": "5d356d7f-3fc0-4489-9d27-796473e9e702"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8353805\n",
            "8353805\n",
            "[array([-1.1812098 , -0.60070044, -0.7897395 , -0.5603433 ,  1.2305504 ,\n",
            "       -0.25741428,  1.4790611 ,  1.2493821 ,  0.37425482,  0.92615455],\n",
            "      dtype=float32), array([ 0.13610631,  0.5593434 , -0.1405848 ,  0.26972613, -0.57897645,\n",
            "       -0.26760852, -0.11910333, -0.18808556,  0.08450229,  0.14508054],\n",
            "      dtype=float32), array([ 0.87826335, -0.60334164,  0.44808203,  1.0347401 , -0.5278637 ,\n",
            "       -0.16722888, -0.10494297, -0.30079252, -0.88877517,  0.59041756],\n",
            "      dtype=float32), array([ 0.01499041,  0.26116854, -0.66562414, -0.68907225, -0.6339151 ,\n",
            "       -0.5744119 ,  0.07547407, -0.7158053 ,  0.20950483, -0.5022661 ],\n",
            "      dtype=float32)]\n",
            "[0, 6, 2, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: generate pytorch class \"TashkelaSet\" that inherits from Dataset that takes char_vector as input X and char_classes as labels + define a function prepare_data that takes the path for the train.txt and val.txt and returns dataloaders\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import nltk, re\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from datetime import datetime\n",
        "from gensim.models import *\n",
        "import logging\n",
        "\n",
        "class TashkelaSet(Dataset):\n",
        "  def __init__(self, X, y):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.X[idx], self.y[idx]\n",
        "\n",
        "def prepare_data(train_X, train_y):\n",
        "\n",
        "  train_set = TashkelaSet(train_X, train_y)\n",
        "\n",
        "  train_loader = DataLoader(train_set, batch_size=100, shuffle=True)\n",
        "\n",
        "  return train_loader\n",
        "\n"
      ],
      "metadata": {
        "id": "wUiTrqSAaH9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model**"
      ],
      "metadata": {
        "id": "txsFRUhBYD50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
        "        super(SimpleRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        # RNN layer\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, h0):\n",
        "        # Forward pass through the RNN\n",
        "        out, hn = self.rnn(x, h0)\n",
        "\n",
        "        # Select the output from the last time step\n",
        "        out = out[:, -1, :]\n",
        "\n",
        "        # Fully connected layer\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out, hn\n",
        "\n",
        "#####################\n",
        "def train_model(model, train_loader):\n",
        "    \"\"\"\n",
        "    Function for training the model\n",
        "    \"\"\"\n",
        "    # define the optimization\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "    # define the loss function\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # epochs\n",
        "    epochs = 10\n",
        "    # loop over the epochs\n",
        "    for epoch in range(epochs):\n",
        "        # initialize the hidden state\n",
        "        h0 = torch.zeros(1, 3, model.hidden_size)\n",
        "        # loop over the dataset\n",
        "        for inputs, labels in train_loader:\n",
        "            # zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "            # compute the model output\n",
        "            yhat, h0 = model(inputs, h0)\n",
        "            # calculate loss\n",
        "            loss = criterion(yhat, labels)\n",
        "            # credit assignment\n",
        "            loss.backward()\n",
        "            # update model weights\n",
        "            optimizer.step()\n",
        "        # print the loss\n",
        "        print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')\n",
        "#####################\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    \"\"\"\n",
        "    Function for evaluating the model\n",
        "    \"\"\"\n",
        "    # initialize the hidden state\n",
        "    h0 = torch.zeros(1, 3, model.hidden_size)\n",
        "    # initialize the accuracy\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    # deactivating autograd\n",
        "    with torch.no_grad():\n",
        "        # loop over the test dataset\n",
        "        for inputs, labels in test_loader:\n",
        "            # compute the model output\n",
        "            yhat, h0 = model(inputs, h0)\n",
        "            # get predictions from the maximum value\n",
        "            _, predicted = torch.max(yhat.data, 1)\n",
        "            # update total\n",
        "            total += labels.size(0)\n",
        "            # update correct\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    # compute the accuracy\n",
        "    accuracy = 100 * correct / total\n",
        "    # print the accuracy\n",
        "    print(f'Accuracy: {accuracy:.2f}')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fl6RxXODYHQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # prompt: the above model consumes ram , rewrite it fixing this issue , and test it on 'train.txt' and 'val.txt'   with reasonable batch number\n",
        "\n",
        "\n",
        "# import re\n",
        "# from collections import Counter\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import tensorflow as tf\n",
        "# import nltk, re\n",
        "# from keras.preprocessing.text import Tokenizer\n",
        "# from datetime import datetime\n",
        "# from gensim.models import *\n",
        "# import logging\n",
        "# from diacritic names to their corresponding Unicode characters.'''\n",
        "# import torch\n",
        "# from torch.utils.data import Dataset\n",
        "# from torch.utils.data import DataLoader\n",
        "# import torch.nn as nn\n",
        "# print(len(char_vectors))\n",
        "# print(len(char_classes))\n",
        "# print(char_vectors[:4])\n",
        "# print(char_classes[:4])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# class TashkelaSet(Dataset):\n",
        "#   def __init__(self, X, y):\n",
        "#     self.X = X\n",
        "#     self.y = y\n",
        "\n",
        "#   def __len__(self):\n",
        "#     return len(self.X)\n",
        "\n",
        "#   def __getitem__(self, idx):\n",
        "#     return self.X[idx], self.y[idx]\n",
        "\n",
        "# def prepare_data(train_X, train_y):\n",
        "\n",
        "#   train_set = TashkelaSet(train_X, train_y)\n",
        "\n",
        "#   train_loader = DataLoader(train_set, batch_size=4*256*10, shuffle=True)\n",
        "\n",
        "#   return train_loader\n",
        "\n",
        "\n",
        "# # **Model**\n",
        "\n",
        "\n",
        "# class SimpleRNN(nn.Module):\n",
        "#     def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
        "#         super(SimpleRNN, self).__init__()\n",
        "\n",
        "#         # RNN layer\n",
        "#         self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "#         # Fully connected layer\n",
        "#         self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "#     def forward(self, x, h0):\n",
        "#         # Forward pass through the RNN\n",
        "#         out, hn = self.rnn(x, h0)\n",
        "\n",
        "#         # Select the output from the last time step\n",
        "#         out = out[:, -1, :]\n",
        "\n",
        "#         # Fully connected layer\n",
        "#         out = self.fc(out)\n",
        "\n",
        "#         return out, hn\n",
        "\n",
        "# #####################\n",
        "# def train_model(model, train_loader):\n",
        "#     \"\"\"\n",
        "#     Function for training the model\n",
        "#     \"\"\"\n",
        "#     # define the optimization\n",
        "#     optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "#     # define the loss function\n",
        "#     criterion = nn.CrossEntropyLoss()\n",
        "#     # epochs\n",
        "#     epochs = 10\n",
        "#     # loop over the epochs\n",
        "#     for epoch in range(epochs):\n",
        "#         # initialize the hidden state\n",
        "#         h0 = torch.zeros(1, 3, hidden_size)\n",
        "#         # loop over the dataset\n",
        "#         for inputs, labels in train_loader:\n",
        "#             # zero the gradients\n",
        "#             optimizer.zero_grad()\n",
        "#             # compute the model output\n",
        "#             yhat, h0 = model(inputs, h0)\n",
        "#             # calculate loss\n",
        "#             loss = criterion(yhat, labels)\n",
        "#             # credit assignment\n",
        "#             loss.backward()\n",
        "#             # update model weights\n",
        "#             optimizer.step()\n",
        "#         # print the loss\n",
        "#         print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')\n",
        "# #####################\n",
        "\n",
        "# def evaluate_model(model, test_loader):\n",
        "#     \"\"\"\n",
        "#     Function for evaluating the model\n",
        "#     \"\"\"\n",
        "#     # initialize the hidden state\n",
        "#     h0 = torch.zeros(1, 3, hidden_size)\n",
        "#     # initialize the accuracy\n",
        "#     correct = 0\n",
        "#     total = 0\n",
        "#     # deactivating autograd\n",
        "#     with torch.no_grad():\n",
        "#         # loop over the test dataset\n",
        "#         for inputs, labels in test_loader:\n",
        "#             # compute the model output\n",
        "#             yhat, h0 = model(inputs, h0)\n",
        "#             # get predictions from the maximum value\n",
        "#             _, predicted = torch.max(yhat.data, 1)\n",
        "#             # update total\n",
        "#             total += labels.size(0)\n",
        "#             # update correct\n",
        "#             correct += (predicted == labels).sum().item()\n",
        "#     # compute the accuracy\n",
        "#     accuracy = 100 * correct / total\n",
        "#     # print the accuracy\n",
        "#     print(f'Accuracy: {accuracy:.2f}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# train_loader = prepare_data(train_X, train_y)\n",
        "# test_loader = prepare_data(test_X, test_y)\n",
        "# model = SimpleRNN(input_size=3, hidden_size=128, output_size=4)\n",
        "# train_model(model, train_loader)\n",
        "# evaluate_model(model, test_loader)\n"
      ],
      "metadata": {
        "id": "y1hhMpKp-KzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XI165qYIco7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: now test the whole code in action , train , validate/evaluate and feel free to add necessary code , that name of the train file is train.txt  and validation set is val.txt\n",
        "\n",
        "# **Data Preparation**\n",
        "\n",
        "train_path = 'train.txt'\n",
        "val_path = 'val.txt'\n",
        "\n",
        "train_loader = prepare_data(char_vectors, char_classes)\n",
        "\n",
        "# **Model Definition**\n",
        "\n",
        "input_size = len(char_vectors)\n",
        "hidden_size = 128\n",
        "output_size = len(char_classes)\n",
        "\n",
        "model = SimpleRNN(input_size, hidden_size, output_size)\n",
        "\n",
        "# **Training**\n",
        "\n",
        "# train_model(model, train_loader)\n",
        "\n",
        "# **Evaluation**\n",
        "\n",
        "# evaluate_model(model, val_loader)\n"
      ],
      "metadata": {
        "id": "oNV27l-9dA1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ju-QqFznOGNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# new tests\n"
      ],
      "metadata": {
        "id": "zWGpdISgWJ_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: generate pytorch class \"TashkelaSet\" that inherits from Dataset that takes char_vector as input X and char_classes as labels + define a function prepare_data that takes the path for the train.txt and val.txt and returns dataloaders\n",
        "\n",
        "\n",
        "# import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# # import tensorflow as tf\n",
        "# import nltk, re\n",
        "# from keras.preprocessing.text import Tokenizer\n",
        "# from datetime import datetime\n",
        "# from gensim.models import *\n",
        "# import logging\n",
        "\n",
        "class TashkelaSet(Dataset):\n",
        "  def __init__(self, X, y):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.X[idx], self.y[idx]\n",
        "\n",
        "def prepare_data(train_X, train_y, cv_X, cv_y):\n",
        "\n",
        "  train_set = TashkelaSet(train_X, train_y)\n",
        "  cv_set = TashkelaSet(cv_X, cv_y)\n",
        "\n",
        "  train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
        "  cv_loader = DataLoader(cv_set, batch_size=32, shuffle=True)\n",
        "  return train_loader, cv_loader\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jvjfDTPKXRL_"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, cv_loader = prepare_data(train_char_vectors, train_char_classes, cv_char_vectors, cv_char_classes)\n"
      ],
      "metadata": {
        "id": "4htG9NaVJiio"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)#,bidirectional=True)\n",
        "        # self.batch_norm = nn.BatchNorm1d(2 * hidden_size)  # Add Batch Normalization\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "        #softmax layer\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Set initial hidden and cell states\n",
        "        # h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        # c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        h0 = torch.zeros(self.num_layers , x.size(0), self.hidden_size).to(device)  # Adjust the num_layers\n",
        "        c0 = torch.zeros(self.num_layers , x.size(0), self.hidden_size).to(device)  # Adjust the num_layers\n",
        "        #adjust the shape of the input to be (batch_size, seq_length, input_size)\n",
        "        x = x.view(x.size(0), -1, x.size(1))\n",
        "\n",
        "\n",
        "        # Forward propagate LSTM\n",
        "        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
        "        # out = self.batch_norm(out[:, -1, :])\n",
        "\n",
        "        # Decode the hidden state of the last time step\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        out = self.softmax(out)\n",
        "        return out\n",
        "# def train_model(model, train_loader, cv_loader, criterion, optimizer, num_epochs=4):\n",
        "#     train_losses = []\n",
        "#     cv_losses = []\n",
        "#     for epoch in range(num_epochs):\n",
        "#         model.train()\n",
        "#         running_loss = 0.0\n",
        "#         for inputs, labels in train_loader:\n",
        "#             inputs = inputs.to(device)\n",
        "#             labels = labels.to(device)\n",
        "#             # Forward pass\n",
        "#             outputs = model(inputs)\n",
        "#             loss = criterion(outputs, labels)\n",
        "#             # Backward and optimize\n",
        "#             optimizer.zero_grad()\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "#             running_loss += loss.item()\n",
        "#         train_losses.append(running_loss / len(train_loader))\n",
        "#         model.eval()\n",
        "#         with torch.no_grad():\n",
        "#             running_loss = 0.0\n",
        "#             for inputs, labels in cv_loader:\n",
        "#                 inputs = inputs.to(device)\n",
        "#                 labels = labels.to(device)\n",
        "#                 outputs = model(inputs)\n",
        "#                 loss = criterion(outputs, labels)\n",
        "#                 running_loss += loss.item()\n",
        "#             cv_losses.append(running_loss / len(cv_loader))\n",
        "#         print('Epoch [{}/{}], Train Loss: {:.4f}, CV Loss: {:.4f}'.format(epoch+1, num_epochs, train_losses[-1], cv_losses[-1]))\n",
        "#         scheduler.step()\n",
        "#     # Save the model state dictionary\n",
        "#     torch.save(model.state_dict(), 'LSTM_model_state.pth')\n",
        "#     return train_losses, cv_losses\n",
        "def train_model(model, train_loader, cv_loader, criterion, optimizer, num_epochs=4):\n",
        "    train_losses = []\n",
        "    cv_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            if batch_idx % 100 == 0:\n",
        "                print(f'Train Epoch [{epoch + 1}/{num_epochs}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "        train_losses.append(running_loss / len(train_loader))\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            running_loss = 0.0\n",
        "\n",
        "            for batch_idx, (inputs, labels) in enumerate(cv_loader):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                running_loss += loss.item()\n",
        "\n",
        "                if batch_idx % 50 == 0:\n",
        "                    print(f'Validation Epoch [{epoch + 1}/{num_epochs}], Batch [{batch_idx}/{len(cv_loader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "            cv_losses.append(running_loss / len(cv_loader))\n",
        "\n",
        "        print('Epoch [{}/{}], Train Loss: {:.4f}, CV Loss: {:.4f}'.format(epoch + 1, num_epochs, train_losses[-1], cv_losses[-1]))\n",
        "        # scheduler.step()\n",
        "\n",
        "    # Save the model state dictionary\n",
        "    torch.save(model.state_dict(), 'LSTM_model_state.pth')\n",
        "\n",
        "    return train_losses, cv_losses\n",
        "\n",
        "\n",
        "\n",
        "'''to load:\n",
        "# Create an instance of the model\n",
        "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "\n",
        "# Load the saved state dictionary\n",
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "\n",
        "'''\n",
        "\n",
        "def evaluate_model(model, loader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum()\n",
        "    return 100 * correct / total\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IeLmlLGPWN1h"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "# Add this before the training loop\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "input_size = 30\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "num_classes = 15\n",
        "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# scheduler = StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "train_losses, cv_losses = train_model(model, train_loader, cv_loader, criterion, optimizer,num_epochs=4)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(train_losses, label='Training loss')\n",
        "plt.plot(cv_losses, label='CV loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(f'Train Accuracy: {evaluate_model(model, train_loader)}%')\n",
        "print(f'CV Accuracy: {evaluate_model(model, cv_loader)}%')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IL4Rv9RFWLde",
        "outputId": "227bcfa5-0556-430f-faaa-b6b6f5aad969"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Train Epoch [3/4], Batch [75000/260984], Loss: 2.3284\n",
            "Train Epoch [3/4], Batch [75100/260984], Loss: 2.2012\n",
            "Train Epoch [3/4], Batch [75200/260984], Loss: 2.1651\n",
            "Train Epoch [3/4], Batch [75300/260984], Loss: 2.1048\n",
            "Train Epoch [3/4], Batch [75400/260984], Loss: 2.3217\n",
            "Train Epoch [3/4], Batch [75500/260984], Loss: 2.1981\n",
            "Train Epoch [3/4], Batch [75600/260984], Loss: 2.3505\n",
            "Train Epoch [3/4], Batch [75700/260984], Loss: 2.3158\n",
            "Train Epoch [3/4], Batch [75800/260984], Loss: 2.2398\n",
            "Train Epoch [3/4], Batch [75900/260984], Loss: 2.3009\n",
            "Train Epoch [3/4], Batch [76000/260984], Loss: 2.2875\n",
            "Train Epoch [3/4], Batch [76100/260984], Loss: 2.3979\n",
            "Train Epoch [3/4], Batch [76200/260984], Loss: 2.2618\n",
            "Train Epoch [3/4], Batch [76300/260984], Loss: 2.1949\n",
            "Train Epoch [3/4], Batch [76400/260984], Loss: 2.2344\n",
            "Train Epoch [3/4], Batch [76500/260984], Loss: 2.2794\n",
            "Train Epoch [3/4], Batch [76600/260984], Loss: 2.4009\n",
            "Train Epoch [3/4], Batch [76700/260984], Loss: 2.2531\n",
            "Train Epoch [3/4], Batch [76800/260984], Loss: 2.3376\n",
            "Train Epoch [3/4], Batch [76900/260984], Loss: 2.4668\n",
            "Train Epoch [3/4], Batch [77000/260984], Loss: 2.2521\n",
            "Train Epoch [3/4], Batch [77100/260984], Loss: 2.3059\n",
            "Train Epoch [3/4], Batch [77200/260984], Loss: 2.2813\n",
            "Train Epoch [3/4], Batch [77300/260984], Loss: 2.4723\n",
            "Train Epoch [3/4], Batch [77400/260984], Loss: 2.3157\n",
            "Train Epoch [3/4], Batch [77500/260984], Loss: 2.4575\n",
            "Train Epoch [3/4], Batch [77600/260984], Loss: 2.3575\n",
            "Train Epoch [3/4], Batch [77700/260984], Loss: 2.2592\n",
            "Train Epoch [3/4], Batch [77800/260984], Loss: 2.3249\n",
            "Train Epoch [3/4], Batch [77900/260984], Loss: 2.3779\n",
            "Train Epoch [3/4], Batch [78000/260984], Loss: 2.3813\n",
            "Train Epoch [3/4], Batch [78100/260984], Loss: 2.4140\n",
            "Train Epoch [3/4], Batch [78200/260984], Loss: 2.2333\n",
            "Train Epoch [3/4], Batch [78300/260984], Loss: 2.3144\n",
            "Train Epoch [3/4], Batch [78400/260984], Loss: 2.2524\n",
            "Train Epoch [3/4], Batch [78500/260984], Loss: 2.3211\n",
            "Train Epoch [3/4], Batch [78600/260984], Loss: 2.2211\n",
            "Train Epoch [3/4], Batch [78700/260984], Loss: 2.3194\n",
            "Train Epoch [3/4], Batch [78800/260984], Loss: 2.3118\n",
            "Train Epoch [3/4], Batch [78900/260984], Loss: 2.2240\n",
            "Train Epoch [3/4], Batch [79000/260984], Loss: 2.3157\n",
            "Train Epoch [3/4], Batch [79100/260984], Loss: 2.2863\n",
            "Train Epoch [3/4], Batch [79200/260984], Loss: 2.4400\n",
            "Train Epoch [3/4], Batch [79300/260984], Loss: 2.4898\n",
            "Train Epoch [3/4], Batch [79400/260984], Loss: 2.2518\n",
            "Train Epoch [3/4], Batch [79500/260984], Loss: 2.4078\n",
            "Train Epoch [3/4], Batch [79600/260984], Loss: 2.2778\n",
            "Train Epoch [3/4], Batch [79700/260984], Loss: 2.2412\n",
            "Train Epoch [3/4], Batch [79800/260984], Loss: 2.1940\n",
            "Train Epoch [3/4], Batch [79900/260984], Loss: 2.2673\n",
            "Train Epoch [3/4], Batch [80000/260984], Loss: 2.2575\n",
            "Train Epoch [3/4], Batch [80100/260984], Loss: 2.4006\n",
            "Train Epoch [3/4], Batch [80200/260984], Loss: 2.3144\n",
            "Train Epoch [3/4], Batch [80300/260984], Loss: 2.3061\n",
            "Train Epoch [3/4], Batch [80400/260984], Loss: 2.3752\n",
            "Train Epoch [3/4], Batch [80500/260984], Loss: 2.3596\n",
            "Train Epoch [3/4], Batch [80600/260984], Loss: 2.3479\n",
            "Train Epoch [3/4], Batch [80700/260984], Loss: 2.3889\n",
            "Train Epoch [3/4], Batch [80800/260984], Loss: 2.2873\n",
            "Train Epoch [3/4], Batch [80900/260984], Loss: 2.3098\n",
            "Train Epoch [3/4], Batch [81000/260984], Loss: 2.2181\n",
            "Train Epoch [3/4], Batch [81100/260984], Loss: 2.3754\n",
            "Train Epoch [3/4], Batch [81200/260984], Loss: 2.2262\n",
            "Train Epoch [3/4], Batch [81300/260984], Loss: 2.1475\n",
            "Train Epoch [3/4], Batch [81400/260984], Loss: 2.3496\n",
            "Train Epoch [3/4], Batch [81500/260984], Loss: 2.2957\n",
            "Train Epoch [3/4], Batch [81600/260984], Loss: 2.2231\n",
            "Train Epoch [3/4], Batch [81700/260984], Loss: 2.4413\n",
            "Train Epoch [3/4], Batch [81800/260984], Loss: 2.2941\n",
            "Train Epoch [3/4], Batch [81900/260984], Loss: 2.4709\n",
            "Train Epoch [3/4], Batch [82000/260984], Loss: 2.4650\n",
            "Train Epoch [3/4], Batch [82100/260984], Loss: 2.4977\n",
            "Train Epoch [3/4], Batch [82200/260984], Loss: 2.1966\n",
            "Train Epoch [3/4], Batch [82300/260984], Loss: 2.2158\n",
            "Train Epoch [3/4], Batch [82400/260984], Loss: 2.3863\n",
            "Train Epoch [3/4], Batch [82500/260984], Loss: 2.4716\n",
            "Train Epoch [3/4], Batch [82600/260984], Loss: 2.4411\n",
            "Train Epoch [3/4], Batch [82700/260984], Loss: 2.2604\n",
            "Train Epoch [3/4], Batch [82800/260984], Loss: 2.2756\n",
            "Train Epoch [3/4], Batch [82900/260984], Loss: 2.3829\n",
            "Train Epoch [3/4], Batch [83000/260984], Loss: 2.2714\n",
            "Train Epoch [3/4], Batch [83100/260984], Loss: 2.3894\n",
            "Train Epoch [3/4], Batch [83200/260984], Loss: 2.3225\n",
            "Train Epoch [3/4], Batch [83300/260984], Loss: 2.4219\n",
            "Train Epoch [3/4], Batch [83400/260984], Loss: 2.2578\n",
            "Train Epoch [3/4], Batch [83500/260984], Loss: 2.2831\n",
            "Train Epoch [3/4], Batch [83600/260984], Loss: 2.3022\n",
            "Train Epoch [3/4], Batch [83700/260984], Loss: 2.2443\n",
            "Train Epoch [3/4], Batch [83800/260984], Loss: 2.3026\n",
            "Train Epoch [3/4], Batch [83900/260984], Loss: 2.1597\n",
            "Train Epoch [3/4], Batch [84000/260984], Loss: 2.2829\n",
            "Train Epoch [3/4], Batch [84100/260984], Loss: 2.3042\n",
            "Train Epoch [3/4], Batch [84200/260984], Loss: 2.3485\n",
            "Train Epoch [3/4], Batch [84300/260984], Loss: 2.2657\n",
            "Train Epoch [3/4], Batch [84400/260984], Loss: 2.2893\n",
            "Train Epoch [3/4], Batch [84500/260984], Loss: 2.3149\n",
            "Train Epoch [3/4], Batch [84600/260984], Loss: 2.1957\n",
            "Train Epoch [3/4], Batch [84700/260984], Loss: 2.3551\n",
            "Train Epoch [3/4], Batch [84800/260984], Loss: 2.2155\n",
            "Train Epoch [3/4], Batch [84900/260984], Loss: 2.2557\n",
            "Train Epoch [3/4], Batch [85000/260984], Loss: 2.1399\n",
            "Train Epoch [3/4], Batch [85100/260984], Loss: 2.2431\n",
            "Train Epoch [3/4], Batch [85200/260984], Loss: 2.3216\n",
            "Train Epoch [3/4], Batch [85300/260984], Loss: 2.3825\n",
            "Train Epoch [3/4], Batch [85400/260984], Loss: 2.2676\n",
            "Train Epoch [3/4], Batch [85500/260984], Loss: 2.2847\n",
            "Train Epoch [3/4], Batch [85600/260984], Loss: 2.3167\n",
            "Train Epoch [3/4], Batch [85700/260984], Loss: 2.3747\n",
            "Train Epoch [3/4], Batch [85800/260984], Loss: 2.2055\n",
            "Train Epoch [3/4], Batch [85900/260984], Loss: 2.2973\n",
            "Train Epoch [3/4], Batch [86000/260984], Loss: 2.3212\n",
            "Train Epoch [3/4], Batch [86100/260984], Loss: 2.3397\n",
            "Train Epoch [3/4], Batch [86200/260984], Loss: 2.3193\n",
            "Train Epoch [3/4], Batch [86300/260984], Loss: 2.4128\n",
            "Train Epoch [3/4], Batch [86400/260984], Loss: 2.2654\n",
            "Train Epoch [3/4], Batch [86500/260984], Loss: 2.3501\n",
            "Train Epoch [3/4], Batch [86600/260984], Loss: 2.2254\n",
            "Train Epoch [3/4], Batch [86700/260984], Loss: 2.4345\n",
            "Train Epoch [3/4], Batch [86800/260984], Loss: 2.1854\n",
            "Train Epoch [3/4], Batch [86900/260984], Loss: 2.3076\n",
            "Train Epoch [3/4], Batch [87000/260984], Loss: 2.2457\n",
            "Train Epoch [3/4], Batch [87100/260984], Loss: 2.4423\n",
            "Train Epoch [3/4], Batch [87200/260984], Loss: 2.1868\n",
            "Train Epoch [3/4], Batch [87300/260984], Loss: 2.1014\n",
            "Train Epoch [3/4], Batch [87400/260984], Loss: 2.4678\n",
            "Train Epoch [3/4], Batch [87500/260984], Loss: 2.4017\n",
            "Train Epoch [3/4], Batch [87600/260984], Loss: 2.4240\n",
            "Train Epoch [3/4], Batch [87700/260984], Loss: 2.3768\n",
            "Train Epoch [3/4], Batch [87800/260984], Loss: 2.2831\n",
            "Train Epoch [3/4], Batch [87900/260984], Loss: 2.4723\n",
            "Train Epoch [3/4], Batch [88000/260984], Loss: 2.3136\n",
            "Train Epoch [3/4], Batch [88100/260984], Loss: 2.2220\n",
            "Train Epoch [3/4], Batch [88200/260984], Loss: 2.4131\n",
            "Train Epoch [3/4], Batch [88300/260984], Loss: 2.2368\n",
            "Train Epoch [3/4], Batch [88400/260984], Loss: 2.4523\n",
            "Train Epoch [3/4], Batch [88500/260984], Loss: 2.3958\n",
            "Train Epoch [3/4], Batch [88600/260984], Loss: 2.1976\n",
            "Train Epoch [3/4], Batch [88700/260984], Loss: 2.2713\n",
            "Train Epoch [3/4], Batch [88800/260984], Loss: 2.3558\n",
            "Train Epoch [3/4], Batch [88900/260984], Loss: 2.2242\n",
            "Train Epoch [3/4], Batch [89000/260984], Loss: 2.4401\n",
            "Train Epoch [3/4], Batch [89100/260984], Loss: 2.2419\n",
            "Train Epoch [3/4], Batch [89200/260984], Loss: 2.3972\n",
            "Train Epoch [3/4], Batch [89300/260984], Loss: 2.2911\n",
            "Train Epoch [3/4], Batch [89400/260984], Loss: 2.1818\n",
            "Train Epoch [3/4], Batch [89500/260984], Loss: 2.2411\n",
            "Train Epoch [3/4], Batch [89600/260984], Loss: 2.1910\n",
            "Train Epoch [3/4], Batch [89700/260984], Loss: 2.3185\n",
            "Train Epoch [3/4], Batch [89800/260984], Loss: 2.2460\n",
            "Train Epoch [3/4], Batch [89900/260984], Loss: 2.4291\n",
            "Train Epoch [3/4], Batch [90000/260984], Loss: 2.3738\n",
            "Train Epoch [3/4], Batch [90100/260984], Loss: 2.2914\n",
            "Train Epoch [3/4], Batch [90200/260984], Loss: 2.5385\n",
            "Train Epoch [3/4], Batch [90300/260984], Loss: 2.3006\n",
            "Train Epoch [3/4], Batch [90400/260984], Loss: 2.3345\n",
            "Train Epoch [3/4], Batch [90500/260984], Loss: 2.3562\n",
            "Train Epoch [3/4], Batch [90600/260984], Loss: 2.3615\n",
            "Train Epoch [3/4], Batch [90700/260984], Loss: 2.2896\n",
            "Train Epoch [3/4], Batch [90800/260984], Loss: 2.3808\n",
            "Train Epoch [3/4], Batch [90900/260984], Loss: 2.3755\n",
            "Train Epoch [3/4], Batch [91000/260984], Loss: 2.3527\n",
            "Train Epoch [3/4], Batch [91100/260984], Loss: 2.1976\n",
            "Train Epoch [3/4], Batch [91200/260984], Loss: 2.2456\n",
            "Train Epoch [3/4], Batch [91300/260984], Loss: 2.3955\n",
            "Train Epoch [3/4], Batch [91400/260984], Loss: 2.3247\n",
            "Train Epoch [3/4], Batch [91500/260984], Loss: 2.3017\n",
            "Train Epoch [3/4], Batch [91600/260984], Loss: 2.3642\n",
            "Train Epoch [3/4], Batch [91700/260984], Loss: 2.2137\n",
            "Train Epoch [3/4], Batch [91800/260984], Loss: 2.3548\n",
            "Train Epoch [3/4], Batch [91900/260984], Loss: 2.2381\n",
            "Train Epoch [3/4], Batch [92000/260984], Loss: 2.3422\n",
            "Train Epoch [3/4], Batch [92100/260984], Loss: 2.4217\n",
            "Train Epoch [3/4], Batch [92200/260984], Loss: 2.2821\n",
            "Train Epoch [3/4], Batch [92300/260984], Loss: 2.2448\n",
            "Train Epoch [3/4], Batch [92400/260984], Loss: 2.4674\n",
            "Train Epoch [3/4], Batch [92500/260984], Loss: 2.3406\n",
            "Train Epoch [3/4], Batch [92600/260984], Loss: 2.2521\n",
            "Train Epoch [3/4], Batch [92700/260984], Loss: 2.4067\n",
            "Train Epoch [3/4], Batch [92800/260984], Loss: 2.2997\n",
            "Train Epoch [3/4], Batch [92900/260984], Loss: 2.2897\n",
            "Train Epoch [3/4], Batch [93000/260984], Loss: 2.2271\n",
            "Train Epoch [3/4], Batch [93100/260984], Loss: 2.1884\n",
            "Train Epoch [3/4], Batch [93200/260984], Loss: 2.3429\n",
            "Train Epoch [3/4], Batch [93300/260984], Loss: 2.2698\n",
            "Train Epoch [3/4], Batch [93400/260984], Loss: 2.4749\n",
            "Train Epoch [3/4], Batch [93500/260984], Loss: 2.3473\n",
            "Train Epoch [3/4], Batch [93600/260984], Loss: 2.2083\n",
            "Train Epoch [3/4], Batch [93700/260984], Loss: 2.1000\n",
            "Train Epoch [3/4], Batch [93800/260984], Loss: 2.3916\n",
            "Train Epoch [3/4], Batch [93900/260984], Loss: 2.2704\n",
            "Train Epoch [3/4], Batch [94000/260984], Loss: 2.1774\n",
            "Train Epoch [3/4], Batch [94100/260984], Loss: 2.2461\n",
            "Train Epoch [3/4], Batch [94200/260984], Loss: 2.3713\n",
            "Train Epoch [3/4], Batch [94300/260984], Loss: 2.2587\n",
            "Train Epoch [3/4], Batch [94400/260984], Loss: 2.3450\n",
            "Train Epoch [3/4], Batch [94500/260984], Loss: 2.2916\n",
            "Train Epoch [3/4], Batch [94600/260984], Loss: 2.3342\n",
            "Train Epoch [3/4], Batch [94700/260984], Loss: 2.2580\n",
            "Train Epoch [3/4], Batch [94800/260984], Loss: 2.4733\n",
            "Train Epoch [3/4], Batch [94900/260984], Loss: 2.2006\n",
            "Train Epoch [3/4], Batch [95000/260984], Loss: 2.4605\n",
            "Train Epoch [3/4], Batch [95100/260984], Loss: 2.2495\n",
            "Train Epoch [3/4], Batch [95200/260984], Loss: 2.2882\n",
            "Train Epoch [3/4], Batch [95300/260984], Loss: 2.2684\n",
            "Train Epoch [3/4], Batch [95400/260984], Loss: 2.2879\n",
            "Train Epoch [3/4], Batch [95500/260984], Loss: 2.3229\n",
            "Train Epoch [3/4], Batch [95600/260984], Loss: 2.3155\n",
            "Train Epoch [3/4], Batch [95700/260984], Loss: 2.2688\n",
            "Train Epoch [3/4], Batch [95800/260984], Loss: 2.1984\n",
            "Train Epoch [3/4], Batch [95900/260984], Loss: 2.2307\n",
            "Train Epoch [3/4], Batch [96000/260984], Loss: 2.3846\n",
            "Train Epoch [3/4], Batch [96100/260984], Loss: 2.2224\n",
            "Train Epoch [3/4], Batch [96200/260984], Loss: 2.2542\n",
            "Train Epoch [3/4], Batch [96300/260984], Loss: 2.1282\n",
            "Train Epoch [3/4], Batch [96400/260984], Loss: 2.2213\n",
            "Train Epoch [3/4], Batch [96500/260984], Loss: 2.2496\n",
            "Train Epoch [3/4], Batch [96600/260984], Loss: 2.4190\n",
            "Train Epoch [3/4], Batch [96700/260984], Loss: 2.3666\n",
            "Train Epoch [3/4], Batch [96800/260984], Loss: 2.4046\n",
            "Train Epoch [3/4], Batch [96900/260984], Loss: 2.3347\n",
            "Train Epoch [3/4], Batch [97000/260984], Loss: 2.2526\n",
            "Train Epoch [3/4], Batch [97100/260984], Loss: 2.2340\n",
            "Train Epoch [3/4], Batch [97200/260984], Loss: 2.2833\n",
            "Train Epoch [3/4], Batch [97300/260984], Loss: 2.1626\n",
            "Train Epoch [3/4], Batch [97400/260984], Loss: 2.3279\n",
            "Train Epoch [3/4], Batch [97500/260984], Loss: 2.2312\n",
            "Train Epoch [3/4], Batch [97600/260984], Loss: 2.2458\n",
            "Train Epoch [3/4], Batch [97700/260984], Loss: 2.3065\n",
            "Train Epoch [3/4], Batch [97800/260984], Loss: 2.2219\n",
            "Train Epoch [3/4], Batch [97900/260984], Loss: 2.3733\n",
            "Train Epoch [3/4], Batch [98000/260984], Loss: 2.3459\n",
            "Train Epoch [3/4], Batch [98100/260984], Loss: 2.1662\n",
            "Train Epoch [3/4], Batch [98200/260984], Loss: 2.2848\n",
            "Train Epoch [3/4], Batch [98300/260984], Loss: 2.1202\n",
            "Train Epoch [3/4], Batch [98400/260984], Loss: 2.3454\n",
            "Train Epoch [3/4], Batch [98500/260984], Loss: 2.0749\n",
            "Train Epoch [3/4], Batch [98600/260984], Loss: 2.4421\n",
            "Train Epoch [3/4], Batch [98700/260984], Loss: 2.4123\n",
            "Train Epoch [3/4], Batch [98800/260984], Loss: 2.4374\n",
            "Train Epoch [3/4], Batch [98900/260984], Loss: 2.4096\n",
            "Train Epoch [3/4], Batch [99000/260984], Loss: 2.4089\n",
            "Train Epoch [3/4], Batch [99100/260984], Loss: 2.3856\n",
            "Train Epoch [3/4], Batch [99200/260984], Loss: 2.3356\n",
            "Train Epoch [3/4], Batch [99300/260984], Loss: 2.3097\n",
            "Train Epoch [3/4], Batch [99400/260984], Loss: 2.3719\n",
            "Train Epoch [3/4], Batch [99500/260984], Loss: 2.2616\n",
            "Train Epoch [3/4], Batch [99600/260984], Loss: 2.4132\n",
            "Train Epoch [3/4], Batch [99700/260984], Loss: 2.4024\n",
            "Train Epoch [3/4], Batch [99800/260984], Loss: 2.3185\n",
            "Train Epoch [3/4], Batch [99900/260984], Loss: 2.4000\n",
            "Train Epoch [3/4], Batch [100000/260984], Loss: 2.2784\n",
            "Train Epoch [3/4], Batch [100100/260984], Loss: 2.2412\n",
            "Train Epoch [3/4], Batch [100200/260984], Loss: 2.5120\n",
            "Train Epoch [3/4], Batch [100300/260984], Loss: 2.1754\n",
            "Train Epoch [3/4], Batch [100400/260984], Loss: 2.2953\n",
            "Train Epoch [3/4], Batch [100500/260984], Loss: 2.2709\n",
            "Train Epoch [3/4], Batch [100600/260984], Loss: 2.2080\n",
            "Train Epoch [3/4], Batch [100700/260984], Loss: 2.3665\n",
            "Train Epoch [3/4], Batch [100800/260984], Loss: 2.2816\n",
            "Train Epoch [3/4], Batch [100900/260984], Loss: 2.2583\n",
            "Train Epoch [3/4], Batch [101000/260984], Loss: 2.4575\n",
            "Train Epoch [3/4], Batch [101100/260984], Loss: 2.2209\n",
            "Train Epoch [3/4], Batch [101200/260984], Loss: 2.1899\n",
            "Train Epoch [3/4], Batch [101300/260984], Loss: 2.2982\n",
            "Train Epoch [3/4], Batch [101400/260984], Loss: 2.2365\n",
            "Train Epoch [3/4], Batch [101500/260984], Loss: 2.3481\n",
            "Train Epoch [3/4], Batch [101600/260984], Loss: 2.2998\n",
            "Train Epoch [3/4], Batch [101700/260984], Loss: 2.3467\n",
            "Train Epoch [3/4], Batch [101800/260984], Loss: 2.2868\n",
            "Train Epoch [3/4], Batch [101900/260984], Loss: 2.3278\n",
            "Train Epoch [3/4], Batch [102000/260984], Loss: 2.2252\n",
            "Train Epoch [3/4], Batch [102100/260984], Loss: 2.4540\n",
            "Train Epoch [3/4], Batch [102200/260984], Loss: 2.2245\n",
            "Train Epoch [3/4], Batch [102300/260984], Loss: 2.3763\n",
            "Train Epoch [3/4], Batch [102400/260984], Loss: 2.2673\n",
            "Train Epoch [3/4], Batch [102500/260984], Loss: 2.3386\n",
            "Train Epoch [3/4], Batch [102600/260984], Loss: 2.3152\n",
            "Train Epoch [3/4], Batch [102700/260984], Loss: 2.4518\n",
            "Train Epoch [3/4], Batch [102800/260984], Loss: 2.2550\n",
            "Train Epoch [3/4], Batch [102900/260984], Loss: 2.3374\n",
            "Train Epoch [3/4], Batch [103000/260984], Loss: 2.3766\n",
            "Train Epoch [3/4], Batch [103100/260984], Loss: 2.2860\n",
            "Train Epoch [3/4], Batch [103200/260984], Loss: 2.3442\n",
            "Train Epoch [3/4], Batch [103300/260984], Loss: 2.3779\n",
            "Train Epoch [3/4], Batch [103400/260984], Loss: 2.3154\n",
            "Train Epoch [3/4], Batch [103500/260984], Loss: 2.4419\n",
            "Train Epoch [3/4], Batch [103600/260984], Loss: 2.2578\n",
            "Train Epoch [3/4], Batch [103700/260984], Loss: 2.3527\n",
            "Train Epoch [3/4], Batch [103800/260984], Loss: 2.3195\n",
            "Train Epoch [3/4], Batch [103900/260984], Loss: 2.1566\n",
            "Train Epoch [3/4], Batch [104000/260984], Loss: 2.4623\n",
            "Train Epoch [3/4], Batch [104100/260984], Loss: 2.2709\n",
            "Train Epoch [3/4], Batch [104200/260984], Loss: 2.3713\n",
            "Train Epoch [3/4], Batch [104300/260984], Loss: 2.2322\n",
            "Train Epoch [3/4], Batch [104400/260984], Loss: 2.2589\n",
            "Train Epoch [3/4], Batch [104500/260984], Loss: 2.4744\n",
            "Train Epoch [3/4], Batch [104600/260984], Loss: 2.4996\n",
            "Train Epoch [3/4], Batch [104700/260984], Loss: 2.2076\n",
            "Train Epoch [3/4], Batch [104800/260984], Loss: 2.1985\n",
            "Train Epoch [3/4], Batch [104900/260984], Loss: 2.2251\n",
            "Train Epoch [3/4], Batch [105000/260984], Loss: 2.1368\n",
            "Train Epoch [3/4], Batch [105100/260984], Loss: 2.3513\n",
            "Train Epoch [3/4], Batch [105200/260984], Loss: 2.3868\n",
            "Train Epoch [3/4], Batch [105300/260984], Loss: 2.3423\n",
            "Train Epoch [3/4], Batch [105400/260984], Loss: 2.3439\n",
            "Train Epoch [3/4], Batch [105500/260984], Loss: 2.2559\n",
            "Train Epoch [3/4], Batch [105600/260984], Loss: 2.2818\n",
            "Train Epoch [3/4], Batch [105700/260984], Loss: 2.3666\n",
            "Train Epoch [3/4], Batch [105800/260984], Loss: 2.4777\n",
            "Train Epoch [3/4], Batch [105900/260984], Loss: 2.3932\n",
            "Train Epoch [3/4], Batch [106000/260984], Loss: 2.2819\n",
            "Train Epoch [3/4], Batch [106100/260984], Loss: 2.2883\n",
            "Train Epoch [3/4], Batch [106200/260984], Loss: 2.2899\n",
            "Train Epoch [3/4], Batch [106300/260984], Loss: 2.0951\n",
            "Train Epoch [3/4], Batch [106400/260984], Loss: 2.4137\n",
            "Train Epoch [3/4], Batch [106500/260984], Loss: 2.1960\n",
            "Train Epoch [3/4], Batch [106600/260984], Loss: 2.2849\n",
            "Train Epoch [3/4], Batch [106700/260984], Loss: 2.3856\n",
            "Train Epoch [3/4], Batch [106800/260984], Loss: 2.3504\n",
            "Train Epoch [3/4], Batch [106900/260984], Loss: 2.2716\n",
            "Train Epoch [3/4], Batch [107000/260984], Loss: 2.2753\n",
            "Train Epoch [3/4], Batch [107100/260984], Loss: 2.3112\n",
            "Train Epoch [3/4], Batch [107200/260984], Loss: 2.2386\n",
            "Train Epoch [3/4], Batch [107300/260984], Loss: 2.2517\n",
            "Train Epoch [3/4], Batch [107400/260984], Loss: 2.3477\n",
            "Train Epoch [3/4], Batch [107500/260984], Loss: 2.2934\n",
            "Train Epoch [3/4], Batch [107600/260984], Loss: 2.2800\n",
            "Train Epoch [3/4], Batch [107700/260984], Loss: 2.2654\n",
            "Train Epoch [3/4], Batch [107800/260984], Loss: 2.2080\n",
            "Train Epoch [3/4], Batch [107900/260984], Loss: 2.2669\n",
            "Train Epoch [3/4], Batch [108000/260984], Loss: 2.3663\n",
            "Train Epoch [3/4], Batch [108100/260984], Loss: 2.2561\n",
            "Train Epoch [3/4], Batch [108200/260984], Loss: 2.1926\n",
            "Train Epoch [3/4], Batch [108300/260984], Loss: 2.2955\n",
            "Train Epoch [3/4], Batch [108400/260984], Loss: 2.1900\n",
            "Train Epoch [3/4], Batch [108500/260984], Loss: 2.3567\n",
            "Train Epoch [3/4], Batch [108600/260984], Loss: 2.2248\n",
            "Train Epoch [3/4], Batch [108700/260984], Loss: 2.3452\n",
            "Train Epoch [3/4], Batch [108800/260984], Loss: 2.2748\n",
            "Train Epoch [3/4], Batch [108900/260984], Loss: 2.4004\n",
            "Train Epoch [3/4], Batch [109000/260984], Loss: 2.2707\n",
            "Train Epoch [3/4], Batch [109100/260984], Loss: 2.2846\n",
            "Train Epoch [3/4], Batch [109200/260984], Loss: 2.3825\n",
            "Train Epoch [3/4], Batch [109300/260984], Loss: 2.2154\n",
            "Train Epoch [3/4], Batch [109400/260984], Loss: 2.4460\n",
            "Train Epoch [3/4], Batch [109500/260984], Loss: 2.3645\n",
            "Train Epoch [3/4], Batch [109600/260984], Loss: 2.3511\n",
            "Train Epoch [3/4], Batch [109700/260984], Loss: 2.1965\n",
            "Train Epoch [3/4], Batch [109800/260984], Loss: 2.3509\n",
            "Train Epoch [3/4], Batch [109900/260984], Loss: 2.2778\n",
            "Train Epoch [3/4], Batch [110000/260984], Loss: 2.1241\n",
            "Train Epoch [3/4], Batch [110100/260984], Loss: 2.3720\n",
            "Train Epoch [3/4], Batch [110200/260984], Loss: 2.2666\n",
            "Train Epoch [3/4], Batch [110300/260984], Loss: 2.3774\n",
            "Train Epoch [3/4], Batch [110400/260984], Loss: 2.1937\n",
            "Train Epoch [3/4], Batch [110500/260984], Loss: 2.3987\n",
            "Train Epoch [3/4], Batch [110600/260984], Loss: 2.3288\n",
            "Train Epoch [3/4], Batch [110700/260984], Loss: 2.3780\n",
            "Train Epoch [3/4], Batch [110800/260984], Loss: 2.1544\n",
            "Train Epoch [3/4], Batch [110900/260984], Loss: 2.1627\n",
            "Train Epoch [3/4], Batch [111000/260984], Loss: 2.3220\n",
            "Train Epoch [3/4], Batch [111100/260984], Loss: 2.3060\n",
            "Train Epoch [3/4], Batch [111200/260984], Loss: 2.3824\n",
            "Train Epoch [3/4], Batch [111300/260984], Loss: 2.3521\n",
            "Train Epoch [3/4], Batch [111400/260984], Loss: 2.2843\n",
            "Train Epoch [3/4], Batch [111500/260984], Loss: 2.2558\n",
            "Train Epoch [3/4], Batch [111600/260984], Loss: 2.4559\n",
            "Train Epoch [3/4], Batch [111700/260984], Loss: 2.4121\n",
            "Train Epoch [3/4], Batch [111800/260984], Loss: 2.2478\n",
            "Train Epoch [3/4], Batch [111900/260984], Loss: 2.2129\n",
            "Train Epoch [3/4], Batch [112000/260984], Loss: 2.5228\n",
            "Train Epoch [3/4], Batch [112100/260984], Loss: 2.2512\n",
            "Train Epoch [3/4], Batch [112200/260984], Loss: 2.2136\n",
            "Train Epoch [3/4], Batch [112300/260984], Loss: 2.4281\n",
            "Train Epoch [3/4], Batch [112400/260984], Loss: 2.3389\n",
            "Train Epoch [3/4], Batch [112500/260984], Loss: 2.3062\n",
            "Train Epoch [3/4], Batch [112600/260984], Loss: 2.2853\n",
            "Train Epoch [3/4], Batch [112700/260984], Loss: 2.2661\n",
            "Train Epoch [3/4], Batch [112800/260984], Loss: 2.2612\n",
            "Train Epoch [3/4], Batch [112900/260984], Loss: 2.2200\n",
            "Train Epoch [3/4], Batch [113000/260984], Loss: 2.4354\n",
            "Train Epoch [3/4], Batch [113100/260984], Loss: 2.3963\n",
            "Train Epoch [3/4], Batch [113200/260984], Loss: 2.4089\n",
            "Train Epoch [3/4], Batch [113300/260984], Loss: 2.2996\n",
            "Train Epoch [3/4], Batch [113400/260984], Loss: 2.3159\n",
            "Train Epoch [3/4], Batch [113500/260984], Loss: 2.3002\n",
            "Train Epoch [3/4], Batch [113600/260984], Loss: 2.4442\n",
            "Train Epoch [3/4], Batch [113700/260984], Loss: 2.2464\n",
            "Train Epoch [3/4], Batch [113800/260984], Loss: 2.3360\n",
            "Train Epoch [3/4], Batch [113900/260984], Loss: 2.2656\n",
            "Train Epoch [3/4], Batch [114000/260984], Loss: 2.2838\n",
            "Train Epoch [3/4], Batch [114100/260984], Loss: 2.3037\n",
            "Train Epoch [3/4], Batch [114200/260984], Loss: 2.3513\n",
            "Train Epoch [3/4], Batch [114300/260984], Loss: 2.5275\n",
            "Train Epoch [3/4], Batch [114400/260984], Loss: 2.4078\n",
            "Train Epoch [3/4], Batch [114500/260984], Loss: 2.2737\n",
            "Train Epoch [3/4], Batch [114600/260984], Loss: 2.1844\n",
            "Train Epoch [3/4], Batch [114700/260984], Loss: 2.3434\n",
            "Train Epoch [3/4], Batch [114800/260984], Loss: 2.2580\n",
            "Train Epoch [3/4], Batch [114900/260984], Loss: 2.2726\n",
            "Train Epoch [3/4], Batch [115000/260984], Loss: 2.2534\n",
            "Train Epoch [3/4], Batch [115100/260984], Loss: 2.2473\n",
            "Train Epoch [3/4], Batch [115200/260984], Loss: 2.1007\n",
            "Train Epoch [3/4], Batch [115300/260984], Loss: 2.2845\n",
            "Train Epoch [3/4], Batch [115400/260984], Loss: 2.2524\n",
            "Train Epoch [3/4], Batch [115500/260984], Loss: 2.3684\n",
            "Train Epoch [3/4], Batch [115600/260984], Loss: 2.4462\n",
            "Train Epoch [3/4], Batch [115700/260984], Loss: 2.3454\n",
            "Train Epoch [3/4], Batch [115800/260984], Loss: 2.2638\n",
            "Train Epoch [3/4], Batch [115900/260984], Loss: 2.2822\n",
            "Train Epoch [3/4], Batch [116000/260984], Loss: 2.4256\n",
            "Train Epoch [3/4], Batch [116100/260984], Loss: 2.2240\n",
            "Train Epoch [3/4], Batch [116200/260984], Loss: 2.3189\n",
            "Train Epoch [3/4], Batch [116300/260984], Loss: 2.3967\n",
            "Train Epoch [3/4], Batch [116400/260984], Loss: 2.1757\n",
            "Train Epoch [3/4], Batch [116500/260984], Loss: 2.2122\n",
            "Train Epoch [3/4], Batch [116600/260984], Loss: 2.2981\n",
            "Train Epoch [3/4], Batch [116700/260984], Loss: 2.3780\n",
            "Train Epoch [3/4], Batch [116800/260984], Loss: 2.2258\n",
            "Train Epoch [3/4], Batch [116900/260984], Loss: 2.3339\n",
            "Train Epoch [3/4], Batch [117000/260984], Loss: 2.4760\n",
            "Train Epoch [3/4], Batch [117100/260984], Loss: 2.1480\n",
            "Train Epoch [3/4], Batch [117200/260984], Loss: 2.1467\n",
            "Train Epoch [3/4], Batch [117300/260984], Loss: 2.3517\n",
            "Train Epoch [3/4], Batch [117400/260984], Loss: 2.2511\n",
            "Train Epoch [3/4], Batch [117500/260984], Loss: 2.1915\n",
            "Train Epoch [3/4], Batch [117600/260984], Loss: 2.3304\n",
            "Train Epoch [3/4], Batch [117700/260984], Loss: 2.3648\n",
            "Train Epoch [3/4], Batch [117800/260984], Loss: 2.3054\n",
            "Train Epoch [3/4], Batch [117900/260984], Loss: 2.3676\n",
            "Train Epoch [3/4], Batch [118000/260984], Loss: 2.2614\n",
            "Train Epoch [3/4], Batch [118100/260984], Loss: 2.3780\n",
            "Train Epoch [3/4], Batch [118200/260984], Loss: 2.3593\n",
            "Train Epoch [3/4], Batch [118300/260984], Loss: 2.2011\n",
            "Train Epoch [3/4], Batch [118400/260984], Loss: 2.3943\n",
            "Train Epoch [3/4], Batch [118500/260984], Loss: 2.3772\n",
            "Train Epoch [3/4], Batch [118600/260984], Loss: 2.2752\n",
            "Train Epoch [3/4], Batch [118700/260984], Loss: 2.3451\n",
            "Train Epoch [3/4], Batch [118800/260984], Loss: 2.3989\n",
            "Train Epoch [3/4], Batch [118900/260984], Loss: 2.1850\n",
            "Train Epoch [3/4], Batch [119000/260984], Loss: 2.3767\n",
            "Train Epoch [3/4], Batch [119100/260984], Loss: 2.2575\n",
            "Train Epoch [3/4], Batch [119200/260984], Loss: 2.3353\n",
            "Train Epoch [3/4], Batch [119300/260984], Loss: 2.1569\n",
            "Train Epoch [3/4], Batch [119400/260984], Loss: 2.2447\n",
            "Train Epoch [3/4], Batch [119500/260984], Loss: 2.3454\n",
            "Train Epoch [3/4], Batch [119600/260984], Loss: 2.1905\n",
            "Train Epoch [3/4], Batch [119700/260984], Loss: 2.3964\n",
            "Train Epoch [3/4], Batch [119800/260984], Loss: 2.4281\n",
            "Train Epoch [3/4], Batch [119900/260984], Loss: 2.3540\n",
            "Train Epoch [3/4], Batch [120000/260984], Loss: 2.2931\n",
            "Train Epoch [3/4], Batch [120100/260984], Loss: 2.2337\n",
            "Train Epoch [3/4], Batch [120200/260984], Loss: 2.3448\n",
            "Train Epoch [3/4], Batch [120300/260984], Loss: 2.2861\n",
            "Train Epoch [3/4], Batch [120400/260984], Loss: 2.3114\n",
            "Train Epoch [3/4], Batch [120500/260984], Loss: 2.3231\n",
            "Train Epoch [3/4], Batch [120600/260984], Loss: 2.2816\n",
            "Train Epoch [3/4], Batch [120700/260984], Loss: 2.3522\n",
            "Train Epoch [3/4], Batch [120800/260984], Loss: 2.4085\n",
            "Train Epoch [3/4], Batch [120900/260984], Loss: 2.3513\n",
            "Train Epoch [3/4], Batch [121000/260984], Loss: 2.2842\n",
            "Train Epoch [3/4], Batch [121100/260984], Loss: 2.4401\n",
            "Train Epoch [3/4], Batch [121200/260984], Loss: 2.3387\n",
            "Train Epoch [3/4], Batch [121300/260984], Loss: 2.3498\n",
            "Train Epoch [3/4], Batch [121400/260984], Loss: 2.3689\n",
            "Train Epoch [3/4], Batch [121500/260984], Loss: 2.1070\n",
            "Train Epoch [3/4], Batch [121600/260984], Loss: 2.3737\n",
            "Train Epoch [3/4], Batch [121700/260984], Loss: 2.3635\n",
            "Train Epoch [3/4], Batch [121800/260984], Loss: 2.3178\n",
            "Train Epoch [3/4], Batch [121900/260984], Loss: 2.2235\n",
            "Train Epoch [3/4], Batch [122000/260984], Loss: 2.3191\n",
            "Train Epoch [3/4], Batch [122100/260984], Loss: 2.2328\n",
            "Train Epoch [3/4], Batch [122200/260984], Loss: 2.3555\n",
            "Train Epoch [3/4], Batch [122300/260984], Loss: 2.1663\n",
            "Train Epoch [3/4], Batch [122400/260984], Loss: 2.3356\n",
            "Train Epoch [3/4], Batch [122500/260984], Loss: 2.4080\n",
            "Train Epoch [3/4], Batch [122600/260984], Loss: 2.1648\n",
            "Train Epoch [3/4], Batch [122700/260984], Loss: 2.3581\n",
            "Train Epoch [3/4], Batch [122800/260984], Loss: 2.2575\n",
            "Train Epoch [3/4], Batch [122900/260984], Loss: 2.2609\n",
            "Train Epoch [3/4], Batch [123000/260984], Loss: 2.3144\n",
            "Train Epoch [3/4], Batch [123100/260984], Loss: 2.1500\n",
            "Train Epoch [3/4], Batch [123200/260984], Loss: 2.2892\n",
            "Train Epoch [3/4], Batch [123300/260984], Loss: 2.2144\n",
            "Train Epoch [3/4], Batch [123400/260984], Loss: 2.2099\n",
            "Train Epoch [3/4], Batch [123500/260984], Loss: 2.2232\n",
            "Train Epoch [3/4], Batch [123600/260984], Loss: 2.2714\n",
            "Train Epoch [3/4], Batch [123700/260984], Loss: 2.3184\n",
            "Train Epoch [3/4], Batch [123800/260984], Loss: 2.4761\n",
            "Train Epoch [3/4], Batch [123900/260984], Loss: 2.1639\n",
            "Train Epoch [3/4], Batch [124000/260984], Loss: 2.4996\n",
            "Train Epoch [3/4], Batch [124100/260984], Loss: 2.3750\n",
            "Train Epoch [3/4], Batch [124200/260984], Loss: 2.2240\n",
            "Train Epoch [3/4], Batch [124300/260984], Loss: 2.1812\n",
            "Train Epoch [3/4], Batch [124400/260984], Loss: 2.3023\n",
            "Train Epoch [3/4], Batch [124500/260984], Loss: 2.2836\n",
            "Train Epoch [3/4], Batch [124600/260984], Loss: 2.3414\n",
            "Train Epoch [3/4], Batch [124700/260984], Loss: 2.3849\n",
            "Train Epoch [3/4], Batch [124800/260984], Loss: 2.2033\n",
            "Train Epoch [3/4], Batch [124900/260984], Loss: 2.1881\n",
            "Train Epoch [3/4], Batch [125000/260984], Loss: 2.2553\n",
            "Train Epoch [3/4], Batch [125100/260984], Loss: 2.3456\n",
            "Train Epoch [3/4], Batch [125200/260984], Loss: 2.4447\n",
            "Train Epoch [3/4], Batch [125300/260984], Loss: 2.2162\n",
            "Train Epoch [3/4], Batch [125400/260984], Loss: 2.3043\n",
            "Train Epoch [3/4], Batch [125500/260984], Loss: 2.3355\n",
            "Train Epoch [3/4], Batch [125600/260984], Loss: 2.3782\n",
            "Train Epoch [3/4], Batch [125700/260984], Loss: 2.2799\n",
            "Train Epoch [3/4], Batch [125800/260984], Loss: 2.4473\n",
            "Train Epoch [3/4], Batch [125900/260984], Loss: 2.2566\n",
            "Train Epoch [3/4], Batch [126000/260984], Loss: 2.3806\n",
            "Train Epoch [3/4], Batch [126100/260984], Loss: 2.2480\n",
            "Train Epoch [3/4], Batch [126200/260984], Loss: 2.1902\n",
            "Train Epoch [3/4], Batch [126300/260984], Loss: 2.3286\n",
            "Train Epoch [3/4], Batch [126400/260984], Loss: 2.2242\n",
            "Train Epoch [3/4], Batch [126500/260984], Loss: 2.2768\n",
            "Train Epoch [3/4], Batch [126600/260984], Loss: 2.1837\n",
            "Train Epoch [3/4], Batch [126700/260984], Loss: 2.4068\n",
            "Train Epoch [3/4], Batch [126800/260984], Loss: 2.1910\n",
            "Train Epoch [3/4], Batch [126900/260984], Loss: 2.3803\n",
            "Train Epoch [3/4], Batch [127000/260984], Loss: 2.2849\n",
            "Train Epoch [3/4], Batch [127100/260984], Loss: 2.4626\n",
            "Train Epoch [3/4], Batch [127200/260984], Loss: 2.2805\n",
            "Train Epoch [3/4], Batch [127300/260984], Loss: 2.3876\n",
            "Train Epoch [3/4], Batch [127400/260984], Loss: 2.3625\n",
            "Train Epoch [3/4], Batch [127500/260984], Loss: 2.3807\n",
            "Train Epoch [3/4], Batch [127600/260984], Loss: 2.3843\n",
            "Train Epoch [3/4], Batch [127700/260984], Loss: 2.3160\n",
            "Train Epoch [3/4], Batch [127800/260984], Loss: 2.2673\n",
            "Train Epoch [3/4], Batch [127900/260984], Loss: 2.1498\n",
            "Train Epoch [3/4], Batch [128000/260984], Loss: 2.5061\n",
            "Train Epoch [3/4], Batch [128100/260984], Loss: 2.2816\n",
            "Train Epoch [3/4], Batch [128200/260984], Loss: 2.2222\n",
            "Train Epoch [3/4], Batch [128300/260984], Loss: 2.2559\n",
            "Train Epoch [3/4], Batch [128400/260984], Loss: 2.4210\n",
            "Train Epoch [3/4], Batch [128500/260984], Loss: 2.3330\n",
            "Train Epoch [3/4], Batch [128600/260984], Loss: 2.1323\n",
            "Train Epoch [3/4], Batch [128700/260984], Loss: 2.3104\n",
            "Train Epoch [3/4], Batch [128800/260984], Loss: 2.2778\n",
            "Train Epoch [3/4], Batch [128900/260984], Loss: 2.3459\n",
            "Train Epoch [3/4], Batch [129000/260984], Loss: 2.3292\n",
            "Train Epoch [3/4], Batch [129100/260984], Loss: 2.2279\n",
            "Train Epoch [3/4], Batch [129200/260984], Loss: 2.2691\n",
            "Train Epoch [3/4], Batch [129300/260984], Loss: 2.5204\n",
            "Train Epoch [3/4], Batch [129400/260984], Loss: 2.4108\n",
            "Train Epoch [3/4], Batch [129500/260984], Loss: 2.4818\n",
            "Train Epoch [3/4], Batch [129600/260984], Loss: 2.3729\n",
            "Train Epoch [3/4], Batch [129700/260984], Loss: 2.3475\n",
            "Train Epoch [3/4], Batch [129800/260984], Loss: 2.3156\n",
            "Train Epoch [3/4], Batch [129900/260984], Loss: 2.1612\n",
            "Train Epoch [3/4], Batch [130000/260984], Loss: 2.2549\n",
            "Train Epoch [3/4], Batch [130100/260984], Loss: 2.1413\n",
            "Train Epoch [3/4], Batch [130200/260984], Loss: 2.2358\n",
            "Train Epoch [3/4], Batch [130300/260984], Loss: 2.3196\n",
            "Train Epoch [3/4], Batch [130400/260984], Loss: 2.1340\n",
            "Train Epoch [3/4], Batch [130500/260984], Loss: 2.3282\n",
            "Train Epoch [3/4], Batch [130600/260984], Loss: 2.1787\n",
            "Train Epoch [3/4], Batch [130700/260984], Loss: 2.2106\n",
            "Train Epoch [3/4], Batch [130800/260984], Loss: 2.1706\n",
            "Train Epoch [3/4], Batch [130900/260984], Loss: 2.2889\n",
            "Train Epoch [3/4], Batch [131000/260984], Loss: 2.4226\n",
            "Train Epoch [3/4], Batch [131100/260984], Loss: 2.3616\n",
            "Train Epoch [3/4], Batch [131200/260984], Loss: 2.2520\n",
            "Train Epoch [3/4], Batch [131300/260984], Loss: 2.3413\n",
            "Train Epoch [3/4], Batch [131400/260984], Loss: 2.4290\n",
            "Train Epoch [3/4], Batch [131500/260984], Loss: 2.1942\n",
            "Train Epoch [3/4], Batch [131600/260984], Loss: 2.3521\n",
            "Train Epoch [3/4], Batch [131700/260984], Loss: 2.1596\n",
            "Train Epoch [3/4], Batch [131800/260984], Loss: 2.1885\n",
            "Train Epoch [3/4], Batch [131900/260984], Loss: 2.2216\n",
            "Train Epoch [3/4], Batch [132000/260984], Loss: 2.2220\n",
            "Train Epoch [3/4], Batch [132100/260984], Loss: 2.3753\n",
            "Train Epoch [3/4], Batch [132200/260984], Loss: 2.2695\n",
            "Train Epoch [3/4], Batch [132300/260984], Loss: 2.3478\n",
            "Train Epoch [3/4], Batch [132400/260984], Loss: 2.4804\n",
            "Train Epoch [3/4], Batch [132500/260984], Loss: 2.3641\n",
            "Train Epoch [3/4], Batch [132600/260984], Loss: 2.3511\n",
            "Train Epoch [3/4], Batch [132700/260984], Loss: 2.4144\n",
            "Train Epoch [3/4], Batch [132800/260984], Loss: 2.3008\n",
            "Train Epoch [3/4], Batch [132900/260984], Loss: 2.4089\n",
            "Train Epoch [3/4], Batch [133000/260984], Loss: 2.4608\n",
            "Train Epoch [3/4], Batch [133100/260984], Loss: 2.3522\n",
            "Train Epoch [3/4], Batch [133200/260984], Loss: 2.2930\n",
            "Train Epoch [3/4], Batch [133300/260984], Loss: 2.3385\n",
            "Train Epoch [3/4], Batch [133400/260984], Loss: 2.3223\n",
            "Train Epoch [3/4], Batch [133500/260984], Loss: 2.3309\n",
            "Train Epoch [3/4], Batch [133600/260984], Loss: 2.2461\n",
            "Train Epoch [3/4], Batch [133700/260984], Loss: 2.2848\n",
            "Train Epoch [3/4], Batch [133800/260984], Loss: 2.2657\n",
            "Train Epoch [3/4], Batch [133900/260984], Loss: 2.4081\n",
            "Train Epoch [3/4], Batch [134000/260984], Loss: 2.4681\n",
            "Train Epoch [3/4], Batch [134100/260984], Loss: 2.3654\n",
            "Train Epoch [3/4], Batch [134200/260984], Loss: 2.4431\n",
            "Train Epoch [3/4], Batch [134300/260984], Loss: 2.2217\n",
            "Train Epoch [3/4], Batch [134400/260984], Loss: 2.4395\n",
            "Train Epoch [3/4], Batch [134500/260984], Loss: 2.4072\n",
            "Train Epoch [3/4], Batch [134600/260984], Loss: 2.4247\n",
            "Train Epoch [3/4], Batch [134700/260984], Loss: 2.4021\n",
            "Train Epoch [3/4], Batch [134800/260984], Loss: 2.3999\n",
            "Train Epoch [3/4], Batch [134900/260984], Loss: 2.3392\n",
            "Train Epoch [3/4], Batch [135000/260984], Loss: 2.3162\n",
            "Train Epoch [3/4], Batch [135100/260984], Loss: 2.3039\n",
            "Train Epoch [3/4], Batch [135200/260984], Loss: 2.2707\n",
            "Train Epoch [3/4], Batch [135300/260984], Loss: 2.4012\n",
            "Train Epoch [3/4], Batch [135400/260984], Loss: 2.3598\n",
            "Train Epoch [3/4], Batch [135500/260984], Loss: 2.2517\n",
            "Train Epoch [3/4], Batch [135600/260984], Loss: 2.4220\n",
            "Train Epoch [3/4], Batch [135700/260984], Loss: 2.4059\n",
            "Train Epoch [3/4], Batch [135800/260984], Loss: 2.3276\n",
            "Train Epoch [3/4], Batch [135900/260984], Loss: 2.2213\n",
            "Train Epoch [3/4], Batch [136000/260984], Loss: 2.3678\n",
            "Train Epoch [3/4], Batch [136100/260984], Loss: 2.2034\n",
            "Train Epoch [3/4], Batch [136200/260984], Loss: 2.2165\n",
            "Train Epoch [3/4], Batch [136300/260984], Loss: 2.3744\n",
            "Train Epoch [3/4], Batch [136400/260984], Loss: 2.4127\n",
            "Train Epoch [3/4], Batch [136500/260984], Loss: 2.4084\n",
            "Train Epoch [3/4], Batch [136600/260984], Loss: 2.4044\n",
            "Train Epoch [3/4], Batch [136700/260984], Loss: 2.2898\n",
            "Train Epoch [3/4], Batch [136800/260984], Loss: 2.2410\n",
            "Train Epoch [3/4], Batch [136900/260984], Loss: 2.3137\n",
            "Train Epoch [3/4], Batch [137000/260984], Loss: 2.3048\n",
            "Train Epoch [3/4], Batch [137100/260984], Loss: 2.1881\n",
            "Train Epoch [3/4], Batch [137200/260984], Loss: 2.2822\n",
            "Train Epoch [3/4], Batch [137300/260984], Loss: 2.4446\n",
            "Train Epoch [3/4], Batch [137400/260984], Loss: 2.3112\n",
            "Train Epoch [3/4], Batch [137500/260984], Loss: 2.3442\n",
            "Train Epoch [3/4], Batch [137600/260984], Loss: 2.3424\n",
            "Train Epoch [3/4], Batch [137700/260984], Loss: 2.2619\n",
            "Train Epoch [3/4], Batch [137800/260984], Loss: 2.2509\n",
            "Train Epoch [3/4], Batch [137900/260984], Loss: 2.3151\n",
            "Train Epoch [3/4], Batch [138000/260984], Loss: 2.1613\n",
            "Train Epoch [3/4], Batch [138100/260984], Loss: 2.3508\n",
            "Train Epoch [3/4], Batch [138200/260984], Loss: 2.3904\n",
            "Train Epoch [3/4], Batch [138300/260984], Loss: 2.5688\n",
            "Train Epoch [3/4], Batch [138400/260984], Loss: 2.3377\n",
            "Train Epoch [3/4], Batch [138500/260984], Loss: 2.1955\n",
            "Train Epoch [3/4], Batch [138600/260984], Loss: 2.3075\n",
            "Train Epoch [3/4], Batch [138700/260984], Loss: 2.4101\n",
            "Train Epoch [3/4], Batch [138800/260984], Loss: 2.2219\n",
            "Train Epoch [3/4], Batch [138900/260984], Loss: 2.4867\n",
            "Train Epoch [3/4], Batch [139000/260984], Loss: 2.2666\n",
            "Train Epoch [3/4], Batch [139100/260984], Loss: 2.2275\n",
            "Train Epoch [3/4], Batch [139200/260984], Loss: 2.1623\n",
            "Train Epoch [3/4], Batch [139300/260984], Loss: 2.1326\n",
            "Train Epoch [3/4], Batch [139400/260984], Loss: 2.3448\n",
            "Train Epoch [3/4], Batch [139500/260984], Loss: 2.3137\n",
            "Train Epoch [3/4], Batch [139600/260984], Loss: 2.1989\n",
            "Train Epoch [3/4], Batch [139700/260984], Loss: 2.1088\n",
            "Train Epoch [3/4], Batch [139800/260984], Loss: 2.3962\n",
            "Train Epoch [3/4], Batch [139900/260984], Loss: 2.2394\n",
            "Train Epoch [3/4], Batch [140000/260984], Loss: 2.2762\n",
            "Train Epoch [3/4], Batch [140100/260984], Loss: 2.4140\n",
            "Train Epoch [3/4], Batch [140200/260984], Loss: 2.3505\n",
            "Train Epoch [3/4], Batch [140300/260984], Loss: 2.1642\n",
            "Train Epoch [3/4], Batch [140400/260984], Loss: 2.2639\n",
            "Train Epoch [3/4], Batch [140500/260984], Loss: 2.2030\n",
            "Train Epoch [3/4], Batch [140600/260984], Loss: 2.2511\n",
            "Train Epoch [3/4], Batch [140700/260984], Loss: 2.3970\n",
            "Train Epoch [3/4], Batch [140800/260984], Loss: 2.2776\n",
            "Train Epoch [3/4], Batch [140900/260984], Loss: 2.2518\n",
            "Train Epoch [3/4], Batch [141000/260984], Loss: 2.2268\n",
            "Train Epoch [3/4], Batch [141100/260984], Loss: 2.2357\n",
            "Train Epoch [3/4], Batch [141200/260984], Loss: 2.3103\n",
            "Train Epoch [3/4], Batch [141300/260984], Loss: 2.1765\n",
            "Train Epoch [3/4], Batch [141400/260984], Loss: 2.3058\n",
            "Train Epoch [3/4], Batch [141500/260984], Loss: 2.2820\n",
            "Train Epoch [3/4], Batch [141600/260984], Loss: 2.1909\n",
            "Train Epoch [3/4], Batch [141700/260984], Loss: 2.3477\n",
            "Train Epoch [3/4], Batch [141800/260984], Loss: 2.3245\n",
            "Train Epoch [3/4], Batch [141900/260984], Loss: 2.2972\n",
            "Train Epoch [3/4], Batch [142000/260984], Loss: 2.2570\n",
            "Train Epoch [3/4], Batch [142100/260984], Loss: 2.2200\n",
            "Train Epoch [3/4], Batch [142200/260984], Loss: 2.3126\n",
            "Train Epoch [3/4], Batch [142300/260984], Loss: 2.3118\n",
            "Train Epoch [3/4], Batch [142400/260984], Loss: 2.4368\n",
            "Train Epoch [3/4], Batch [142500/260984], Loss: 2.2828\n",
            "Train Epoch [3/4], Batch [142600/260984], Loss: 2.2222\n",
            "Train Epoch [3/4], Batch [142700/260984], Loss: 2.3374\n",
            "Train Epoch [3/4], Batch [142800/260984], Loss: 2.4085\n",
            "Train Epoch [3/4], Batch [142900/260984], Loss: 2.2301\n",
            "Train Epoch [3/4], Batch [143000/260984], Loss: 2.2524\n",
            "Train Epoch [3/4], Batch [143100/260984], Loss: 2.4670\n",
            "Train Epoch [3/4], Batch [143200/260984], Loss: 2.3133\n",
            "Train Epoch [3/4], Batch [143300/260984], Loss: 2.3646\n",
            "Train Epoch [3/4], Batch [143400/260984], Loss: 2.4682\n",
            "Train Epoch [3/4], Batch [143500/260984], Loss: 2.3780\n",
            "Train Epoch [3/4], Batch [143600/260984], Loss: 2.4492\n",
            "Train Epoch [3/4], Batch [143700/260984], Loss: 2.4096\n",
            "Train Epoch [3/4], Batch [143800/260984], Loss: 2.3259\n",
            "Train Epoch [3/4], Batch [143900/260984], Loss: 2.3744\n",
            "Train Epoch [3/4], Batch [144000/260984], Loss: 2.2880\n",
            "Train Epoch [3/4], Batch [144100/260984], Loss: 2.2558\n",
            "Train Epoch [3/4], Batch [144200/260984], Loss: 2.3400\n",
            "Train Epoch [3/4], Batch [144300/260984], Loss: 2.4830\n",
            "Train Epoch [3/4], Batch [144400/260984], Loss: 2.2639\n",
            "Train Epoch [3/4], Batch [144500/260984], Loss: 2.2548\n",
            "Train Epoch [3/4], Batch [144600/260984], Loss: 2.3368\n",
            "Train Epoch [3/4], Batch [144700/260984], Loss: 2.6520\n",
            "Train Epoch [3/4], Batch [144800/260984], Loss: 2.1908\n",
            "Train Epoch [3/4], Batch [144900/260984], Loss: 2.4407\n",
            "Train Epoch [3/4], Batch [145000/260984], Loss: 2.4716\n",
            "Train Epoch [3/4], Batch [145100/260984], Loss: 2.4096\n",
            "Train Epoch [3/4], Batch [145200/260984], Loss: 2.2541\n",
            "Train Epoch [3/4], Batch [145300/260984], Loss: 2.2330\n",
            "Train Epoch [3/4], Batch [145400/260984], Loss: 2.3997\n",
            "Train Epoch [3/4], Batch [145500/260984], Loss: 2.3879\n",
            "Train Epoch [3/4], Batch [145600/260984], Loss: 2.3101\n",
            "Train Epoch [3/4], Batch [145700/260984], Loss: 2.2871\n",
            "Train Epoch [3/4], Batch [145800/260984], Loss: 2.2220\n",
            "Train Epoch [3/4], Batch [145900/260984], Loss: 2.4006\n",
            "Train Epoch [3/4], Batch [146000/260984], Loss: 2.4174\n",
            "Train Epoch [3/4], Batch [146100/260984], Loss: 2.1396\n",
            "Train Epoch [3/4], Batch [146200/260984], Loss: 2.1021\n",
            "Train Epoch [3/4], Batch [146300/260984], Loss: 2.4138\n",
            "Train Epoch [3/4], Batch [146400/260984], Loss: 2.2884\n",
            "Train Epoch [3/4], Batch [146500/260984], Loss: 2.2275\n",
            "Train Epoch [3/4], Batch [146600/260984], Loss: 2.3465\n",
            "Train Epoch [3/4], Batch [146700/260984], Loss: 2.2940\n",
            "Train Epoch [3/4], Batch [146800/260984], Loss: 2.3503\n",
            "Train Epoch [3/4], Batch [146900/260984], Loss: 2.3887\n",
            "Train Epoch [3/4], Batch [147000/260984], Loss: 2.3692\n",
            "Train Epoch [3/4], Batch [147100/260984], Loss: 2.3376\n",
            "Train Epoch [3/4], Batch [147200/260984], Loss: 2.3892\n",
            "Train Epoch [3/4], Batch [147300/260984], Loss: 2.2880\n",
            "Train Epoch [3/4], Batch [147400/260984], Loss: 2.1590\n",
            "Train Epoch [3/4], Batch [147500/260984], Loss: 2.1731\n",
            "Train Epoch [3/4], Batch [147600/260984], Loss: 2.3955\n",
            "Train Epoch [3/4], Batch [147700/260984], Loss: 2.4079\n",
            "Train Epoch [3/4], Batch [147800/260984], Loss: 2.2724\n",
            "Train Epoch [3/4], Batch [147900/260984], Loss: 2.1863\n",
            "Train Epoch [3/4], Batch [148000/260984], Loss: 2.3539\n",
            "Train Epoch [3/4], Batch [148100/260984], Loss: 2.2363\n",
            "Train Epoch [3/4], Batch [148200/260984], Loss: 2.3142\n",
            "Train Epoch [3/4], Batch [148300/260984], Loss: 2.3449\n",
            "Train Epoch [3/4], Batch [148400/260984], Loss: 2.3389\n",
            "Train Epoch [3/4], Batch [148500/260984], Loss: 2.3829\n",
            "Train Epoch [3/4], Batch [148600/260984], Loss: 2.1949\n",
            "Train Epoch [3/4], Batch [148700/260984], Loss: 2.3346\n",
            "Train Epoch [3/4], Batch [148800/260984], Loss: 2.4048\n",
            "Train Epoch [3/4], Batch [148900/260984], Loss: 2.2449\n",
            "Train Epoch [3/4], Batch [149000/260984], Loss: 2.2484\n",
            "Train Epoch [3/4], Batch [149100/260984], Loss: 2.2765\n",
            "Train Epoch [3/4], Batch [149200/260984], Loss: 2.2059\n",
            "Train Epoch [3/4], Batch [149300/260984], Loss: 2.2774\n",
            "Train Epoch [3/4], Batch [149400/260984], Loss: 2.2545\n",
            "Train Epoch [3/4], Batch [149500/260984], Loss: 2.4616\n",
            "Train Epoch [3/4], Batch [149600/260984], Loss: 2.2047\n",
            "Train Epoch [3/4], Batch [149700/260984], Loss: 2.2775\n",
            "Train Epoch [3/4], Batch [149800/260984], Loss: 2.2274\n",
            "Train Epoch [3/4], Batch [149900/260984], Loss: 2.2202\n",
            "Train Epoch [3/4], Batch [150000/260984], Loss: 2.2513\n",
            "Train Epoch [3/4], Batch [150100/260984], Loss: 2.2634\n",
            "Train Epoch [3/4], Batch [150200/260984], Loss: 2.2247\n",
            "Train Epoch [3/4], Batch [150300/260984], Loss: 2.1280\n",
            "Train Epoch [3/4], Batch [150400/260984], Loss: 2.1592\n",
            "Train Epoch [3/4], Batch [150500/260984], Loss: 2.2474\n",
            "Train Epoch [3/4], Batch [150600/260984], Loss: 2.4646\n",
            "Train Epoch [3/4], Batch [150700/260984], Loss: 2.4887\n",
            "Train Epoch [3/4], Batch [150800/260984], Loss: 2.3550\n",
            "Train Epoch [3/4], Batch [150900/260984], Loss: 2.2803\n",
            "Train Epoch [3/4], Batch [151000/260984], Loss: 2.2755\n",
            "Train Epoch [3/4], Batch [151100/260984], Loss: 2.2274\n",
            "Train Epoch [3/4], Batch [151200/260984], Loss: 2.4661\n",
            "Train Epoch [3/4], Batch [151300/260984], Loss: 2.3122\n",
            "Train Epoch [3/4], Batch [151400/260984], Loss: 2.3151\n",
            "Train Epoch [3/4], Batch [151500/260984], Loss: 2.3600\n",
            "Train Epoch [3/4], Batch [151600/260984], Loss: 2.3734\n",
            "Train Epoch [3/4], Batch [151700/260984], Loss: 2.2552\n",
            "Train Epoch [3/4], Batch [151800/260984], Loss: 2.2527\n",
            "Train Epoch [3/4], Batch [151900/260984], Loss: 2.3293\n",
            "Train Epoch [3/4], Batch [152000/260984], Loss: 2.2923\n",
            "Train Epoch [3/4], Batch [152100/260984], Loss: 2.4273\n",
            "Train Epoch [3/4], Batch [152200/260984], Loss: 2.4157\n",
            "Train Epoch [3/4], Batch [152300/260984], Loss: 2.3388\n",
            "Train Epoch [3/4], Batch [152400/260984], Loss: 2.1142\n",
            "Train Epoch [3/4], Batch [152500/260984], Loss: 2.1394\n",
            "Train Epoch [3/4], Batch [152600/260984], Loss: 2.3194\n",
            "Train Epoch [3/4], Batch [152700/260984], Loss: 2.3535\n",
            "Train Epoch [3/4], Batch [152800/260984], Loss: 2.2239\n",
            "Train Epoch [3/4], Batch [152900/260984], Loss: 2.1413\n",
            "Train Epoch [3/4], Batch [153000/260984], Loss: 2.3854\n",
            "Train Epoch [3/4], Batch [153100/260984], Loss: 2.2784\n",
            "Train Epoch [3/4], Batch [153200/260984], Loss: 2.3765\n",
            "Train Epoch [3/4], Batch [153300/260984], Loss: 2.5812\n",
            "Train Epoch [3/4], Batch [153400/260984], Loss: 2.4269\n",
            "Train Epoch [3/4], Batch [153500/260984], Loss: 2.2048\n",
            "Train Epoch [3/4], Batch [153600/260984], Loss: 2.2841\n",
            "Train Epoch [3/4], Batch [153700/260984], Loss: 2.3775\n",
            "Train Epoch [3/4], Batch [153800/260984], Loss: 2.2500\n",
            "Train Epoch [3/4], Batch [153900/260984], Loss: 2.2696\n",
            "Train Epoch [3/4], Batch [154000/260984], Loss: 2.2954\n",
            "Train Epoch [3/4], Batch [154100/260984], Loss: 2.2853\n",
            "Train Epoch [3/4], Batch [154200/260984], Loss: 2.1898\n",
            "Train Epoch [3/4], Batch [154300/260984], Loss: 2.3194\n",
            "Train Epoch [3/4], Batch [154400/260984], Loss: 2.4115\n",
            "Train Epoch [3/4], Batch [154500/260984], Loss: 2.2782\n",
            "Train Epoch [3/4], Batch [154600/260984], Loss: 2.3285\n",
            "Train Epoch [3/4], Batch [154700/260984], Loss: 2.3453\n",
            "Train Epoch [3/4], Batch [154800/260984], Loss: 2.4491\n",
            "Train Epoch [3/4], Batch [154900/260984], Loss: 2.2764\n",
            "Train Epoch [3/4], Batch [155000/260984], Loss: 2.3102\n",
            "Train Epoch [3/4], Batch [155100/260984], Loss: 2.4257\n",
            "Train Epoch [3/4], Batch [155200/260984], Loss: 2.3567\n",
            "Train Epoch [3/4], Batch [155300/260984], Loss: 2.2819\n",
            "Train Epoch [3/4], Batch [155400/260984], Loss: 2.1551\n",
            "Train Epoch [3/4], Batch [155500/260984], Loss: 2.3320\n",
            "Train Epoch [3/4], Batch [155600/260984], Loss: 2.3567\n",
            "Train Epoch [3/4], Batch [155700/260984], Loss: 2.2441\n",
            "Train Epoch [3/4], Batch [155800/260984], Loss: 2.3458\n",
            "Train Epoch [3/4], Batch [155900/260984], Loss: 2.1285\n",
            "Train Epoch [3/4], Batch [156000/260984], Loss: 2.2826\n",
            "Train Epoch [3/4], Batch [156100/260984], Loss: 2.5414\n",
            "Train Epoch [3/4], Batch [156200/260984], Loss: 2.2909\n",
            "Train Epoch [3/4], Batch [156300/260984], Loss: 2.3270\n",
            "Train Epoch [3/4], Batch [156400/260984], Loss: 2.2541\n",
            "Train Epoch [3/4], Batch [156500/260984], Loss: 2.3389\n",
            "Train Epoch [3/4], Batch [156600/260984], Loss: 2.2503\n",
            "Train Epoch [3/4], Batch [156700/260984], Loss: 2.2375\n",
            "Train Epoch [3/4], Batch [156800/260984], Loss: 2.3929\n",
            "Train Epoch [3/4], Batch [156900/260984], Loss: 2.2491\n",
            "Train Epoch [3/4], Batch [157000/260984], Loss: 2.4395\n",
            "Train Epoch [3/4], Batch [157100/260984], Loss: 2.3826\n",
            "Train Epoch [3/4], Batch [157200/260984], Loss: 2.3121\n",
            "Train Epoch [3/4], Batch [157300/260984], Loss: 2.1938\n",
            "Train Epoch [3/4], Batch [157400/260984], Loss: 2.2863\n",
            "Train Epoch [3/4], Batch [157500/260984], Loss: 2.3156\n",
            "Train Epoch [3/4], Batch [157600/260984], Loss: 2.2536\n",
            "Train Epoch [3/4], Batch [157700/260984], Loss: 2.1933\n",
            "Train Epoch [3/4], Batch [157800/260984], Loss: 2.1847\n",
            "Train Epoch [3/4], Batch [157900/260984], Loss: 2.4109\n",
            "Train Epoch [3/4], Batch [158000/260984], Loss: 2.2311\n",
            "Train Epoch [3/4], Batch [158100/260984], Loss: 2.2454\n",
            "Train Epoch [3/4], Batch [158200/260984], Loss: 2.2921\n",
            "Train Epoch [3/4], Batch [158300/260984], Loss: 2.2773\n",
            "Train Epoch [3/4], Batch [158400/260984], Loss: 2.2104\n",
            "Train Epoch [3/4], Batch [158500/260984], Loss: 2.2987\n",
            "Train Epoch [3/4], Batch [158600/260984], Loss: 2.2847\n",
            "Train Epoch [3/4], Batch [158700/260984], Loss: 2.3476\n",
            "Train Epoch [3/4], Batch [158800/260984], Loss: 2.2574\n",
            "Train Epoch [3/4], Batch [158900/260984], Loss: 2.5004\n",
            "Train Epoch [3/4], Batch [159000/260984], Loss: 2.3717\n",
            "Train Epoch [3/4], Batch [159100/260984], Loss: 2.2549\n",
            "Train Epoch [3/4], Batch [159200/260984], Loss: 2.4076\n",
            "Train Epoch [3/4], Batch [159300/260984], Loss: 2.5911\n",
            "Train Epoch [3/4], Batch [159400/260984], Loss: 2.3678\n",
            "Train Epoch [3/4], Batch [159500/260984], Loss: 2.3086\n",
            "Train Epoch [3/4], Batch [159600/260984], Loss: 2.3256\n",
            "Train Epoch [3/4], Batch [159700/260984], Loss: 2.1973\n",
            "Train Epoch [3/4], Batch [159800/260984], Loss: 2.2002\n",
            "Train Epoch [3/4], Batch [159900/260984], Loss: 2.3070\n",
            "Train Epoch [3/4], Batch [160000/260984], Loss: 2.2499\n",
            "Train Epoch [3/4], Batch [160100/260984], Loss: 2.1658\n",
            "Train Epoch [3/4], Batch [160200/260984], Loss: 2.3887\n",
            "Train Epoch [3/4], Batch [160300/260984], Loss: 2.1458\n",
            "Train Epoch [3/4], Batch [160400/260984], Loss: 2.2920\n",
            "Train Epoch [3/4], Batch [160500/260984], Loss: 2.3815\n",
            "Train Epoch [3/4], Batch [160600/260984], Loss: 2.3928\n",
            "Train Epoch [3/4], Batch [160700/260984], Loss: 2.2696\n",
            "Train Epoch [3/4], Batch [160800/260984], Loss: 2.1658\n",
            "Train Epoch [3/4], Batch [160900/260984], Loss: 2.4615\n",
            "Train Epoch [3/4], Batch [161000/260984], Loss: 2.3597\n",
            "Train Epoch [3/4], Batch [161100/260984], Loss: 2.3771\n",
            "Train Epoch [3/4], Batch [161200/260984], Loss: 2.3118\n",
            "Train Epoch [3/4], Batch [161300/260984], Loss: 2.3854\n",
            "Train Epoch [3/4], Batch [161400/260984], Loss: 2.3057\n",
            "Train Epoch [3/4], Batch [161500/260984], Loss: 2.2472\n",
            "Train Epoch [3/4], Batch [161600/260984], Loss: 2.3517\n",
            "Train Epoch [3/4], Batch [161700/260984], Loss: 2.5918\n",
            "Train Epoch [3/4], Batch [161800/260984], Loss: 2.3011\n",
            "Train Epoch [3/4], Batch [161900/260984], Loss: 2.3447\n",
            "Train Epoch [3/4], Batch [162000/260984], Loss: 2.2089\n",
            "Train Epoch [3/4], Batch [162100/260984], Loss: 2.1579\n",
            "Train Epoch [3/4], Batch [162200/260984], Loss: 2.3835\n",
            "Train Epoch [3/4], Batch [162300/260984], Loss: 2.2775\n",
            "Train Epoch [3/4], Batch [162400/260984], Loss: 2.2315\n",
            "Train Epoch [3/4], Batch [162500/260984], Loss: 2.4935\n",
            "Train Epoch [3/4], Batch [162600/260984], Loss: 2.3743\n",
            "Train Epoch [3/4], Batch [162700/260984], Loss: 2.2532\n",
            "Train Epoch [3/4], Batch [162800/260984], Loss: 2.3559\n",
            "Train Epoch [3/4], Batch [162900/260984], Loss: 2.2327\n",
            "Train Epoch [3/4], Batch [163000/260984], Loss: 2.3459\n",
            "Train Epoch [3/4], Batch [163100/260984], Loss: 2.3816\n",
            "Train Epoch [3/4], Batch [163200/260984], Loss: 2.4405\n",
            "Train Epoch [3/4], Batch [163300/260984], Loss: 2.1712\n",
            "Train Epoch [3/4], Batch [163400/260984], Loss: 2.2236\n",
            "Train Epoch [3/4], Batch [163500/260984], Loss: 2.3446\n",
            "Train Epoch [3/4], Batch [163600/260984], Loss: 2.3454\n",
            "Train Epoch [3/4], Batch [163700/260984], Loss: 2.2001\n",
            "Train Epoch [3/4], Batch [163800/260984], Loss: 2.3145\n",
            "Train Epoch [3/4], Batch [163900/260984], Loss: 2.3942\n",
            "Train Epoch [3/4], Batch [164000/260984], Loss: 2.2219\n",
            "Train Epoch [3/4], Batch [164100/260984], Loss: 2.4367\n",
            "Train Epoch [3/4], Batch [164200/260984], Loss: 2.3463\n",
            "Train Epoch [3/4], Batch [164300/260984], Loss: 2.3110\n",
            "Train Epoch [3/4], Batch [164400/260984], Loss: 2.4370\n",
            "Train Epoch [3/4], Batch [164500/260984], Loss: 2.4304\n",
            "Train Epoch [3/4], Batch [164600/260984], Loss: 2.3383\n",
            "Train Epoch [3/4], Batch [164700/260984], Loss: 2.3828\n",
            "Train Epoch [3/4], Batch [164800/260984], Loss: 2.4126\n",
            "Train Epoch [3/4], Batch [164900/260984], Loss: 2.2901\n",
            "Train Epoch [3/4], Batch [165000/260984], Loss: 2.1809\n",
            "Train Epoch [3/4], Batch [165100/260984], Loss: 2.5140\n",
            "Train Epoch [3/4], Batch [165200/260984], Loss: 2.2600\n",
            "Train Epoch [3/4], Batch [165300/260984], Loss: 2.4018\n",
            "Train Epoch [3/4], Batch [165400/260984], Loss: 2.4301\n",
            "Train Epoch [3/4], Batch [165500/260984], Loss: 2.2933\n",
            "Train Epoch [3/4], Batch [165600/260984], Loss: 2.3139\n",
            "Train Epoch [3/4], Batch [165700/260984], Loss: 2.4158\n",
            "Train Epoch [3/4], Batch [165800/260984], Loss: 2.2640\n",
            "Train Epoch [3/4], Batch [165900/260984], Loss: 2.2243\n",
            "Train Epoch [3/4], Batch [166000/260984], Loss: 2.2530\n",
            "Train Epoch [3/4], Batch [166100/260984], Loss: 2.3163\n",
            "Train Epoch [3/4], Batch [166200/260984], Loss: 2.1879\n",
            "Train Epoch [3/4], Batch [166300/260984], Loss: 2.3462\n",
            "Train Epoch [3/4], Batch [166400/260984], Loss: 2.2736\n",
            "Train Epoch [3/4], Batch [166500/260984], Loss: 2.2300\n",
            "Train Epoch [3/4], Batch [166600/260984], Loss: 2.3410\n",
            "Train Epoch [3/4], Batch [166700/260984], Loss: 2.2339\n",
            "Train Epoch [3/4], Batch [166800/260984], Loss: 2.2587\n",
            "Train Epoch [3/4], Batch [166900/260984], Loss: 2.3992\n",
            "Train Epoch [3/4], Batch [167000/260984], Loss: 2.3814\n",
            "Train Epoch [3/4], Batch [167100/260984], Loss: 2.1659\n",
            "Train Epoch [3/4], Batch [167200/260984], Loss: 2.2627\n",
            "Train Epoch [3/4], Batch [167300/260984], Loss: 2.3424\n",
            "Train Epoch [3/4], Batch [167400/260984], Loss: 2.4952\n",
            "Train Epoch [3/4], Batch [167500/260984], Loss: 2.2282\n",
            "Train Epoch [3/4], Batch [167600/260984], Loss: 2.3276\n",
            "Train Epoch [3/4], Batch [167700/260984], Loss: 2.4582\n",
            "Train Epoch [3/4], Batch [167800/260984], Loss: 2.3424\n",
            "Train Epoch [3/4], Batch [167900/260984], Loss: 2.2405\n",
            "Train Epoch [3/4], Batch [168000/260984], Loss: 2.3145\n",
            "Train Epoch [3/4], Batch [168100/260984], Loss: 2.3267\n",
            "Train Epoch [3/4], Batch [168200/260984], Loss: 2.2927\n",
            "Train Epoch [3/4], Batch [168300/260984], Loss: 2.3534\n",
            "Train Epoch [3/4], Batch [168400/260984], Loss: 2.1471\n",
            "Train Epoch [3/4], Batch [168500/260984], Loss: 2.3473\n",
            "Train Epoch [3/4], Batch [168600/260984], Loss: 2.2207\n",
            "Train Epoch [3/4], Batch [168700/260984], Loss: 2.2173\n",
            "Train Epoch [3/4], Batch [168800/260984], Loss: 2.2915\n",
            "Train Epoch [3/4], Batch [168900/260984], Loss: 2.3728\n",
            "Train Epoch [3/4], Batch [169000/260984], Loss: 2.3464\n",
            "Train Epoch [3/4], Batch [169100/260984], Loss: 2.1908\n",
            "Train Epoch [3/4], Batch [169200/260984], Loss: 2.3813\n",
            "Train Epoch [3/4], Batch [169300/260984], Loss: 2.2573\n",
            "Train Epoch [3/4], Batch [169400/260984], Loss: 2.1556\n",
            "Train Epoch [3/4], Batch [169500/260984], Loss: 2.3468\n",
            "Train Epoch [3/4], Batch [169600/260984], Loss: 2.2612\n",
            "Train Epoch [3/4], Batch [169700/260984], Loss: 2.3469\n",
            "Train Epoch [3/4], Batch [169800/260984], Loss: 2.2551\n",
            "Train Epoch [3/4], Batch [169900/260984], Loss: 2.4595\n",
            "Train Epoch [3/4], Batch [170000/260984], Loss: 2.3783\n",
            "Train Epoch [3/4], Batch [170100/260984], Loss: 2.2580\n",
            "Train Epoch [3/4], Batch [170200/260984], Loss: 2.1105\n",
            "Train Epoch [3/4], Batch [170300/260984], Loss: 2.3162\n",
            "Train Epoch [3/4], Batch [170400/260984], Loss: 2.1970\n",
            "Train Epoch [3/4], Batch [170500/260984], Loss: 2.2639\n",
            "Train Epoch [3/4], Batch [170600/260984], Loss: 2.0926\n",
            "Train Epoch [3/4], Batch [170700/260984], Loss: 2.3623\n",
            "Train Epoch [3/4], Batch [170800/260984], Loss: 2.3185\n",
            "Train Epoch [3/4], Batch [170900/260984], Loss: 2.2123\n",
            "Train Epoch [3/4], Batch [171000/260984], Loss: 2.4178\n",
            "Train Epoch [3/4], Batch [171100/260984], Loss: 2.2834\n",
            "Train Epoch [3/4], Batch [171200/260984], Loss: 2.3129\n",
            "Train Epoch [3/4], Batch [171300/260984], Loss: 2.3208\n",
            "Train Epoch [3/4], Batch [171400/260984], Loss: 2.2172\n",
            "Train Epoch [3/4], Batch [171500/260984], Loss: 2.2779\n",
            "Train Epoch [3/4], Batch [171600/260984], Loss: 2.3279\n",
            "Train Epoch [3/4], Batch [171700/260984], Loss: 2.2587\n",
            "Train Epoch [3/4], Batch [171800/260984], Loss: 2.4118\n",
            "Train Epoch [3/4], Batch [171900/260984], Loss: 2.2313\n",
            "Train Epoch [3/4], Batch [172000/260984], Loss: 2.2605\n",
            "Train Epoch [3/4], Batch [172100/260984], Loss: 2.3146\n",
            "Train Epoch [3/4], Batch [172200/260984], Loss: 2.4421\n",
            "Train Epoch [3/4], Batch [172300/260984], Loss: 2.3325\n",
            "Train Epoch [3/4], Batch [172400/260984], Loss: 2.3105\n",
            "Train Epoch [3/4], Batch [172500/260984], Loss: 2.1369\n",
            "Train Epoch [3/4], Batch [172600/260984], Loss: 2.3769\n",
            "Train Epoch [3/4], Batch [172700/260984], Loss: 2.2279\n",
            "Train Epoch [3/4], Batch [172800/260984], Loss: 2.2223\n",
            "Train Epoch [3/4], Batch [172900/260984], Loss: 2.3712\n",
            "Train Epoch [3/4], Batch [173000/260984], Loss: 2.2843\n",
            "Train Epoch [3/4], Batch [173100/260984], Loss: 2.3372\n",
            "Train Epoch [3/4], Batch [173200/260984], Loss: 2.3143\n",
            "Train Epoch [3/4], Batch [173300/260984], Loss: 2.3175\n",
            "Train Epoch [3/4], Batch [173400/260984], Loss: 2.4242\n",
            "Train Epoch [3/4], Batch [173500/260984], Loss: 2.2650\n",
            "Train Epoch [3/4], Batch [173600/260984], Loss: 2.2930\n",
            "Train Epoch [3/4], Batch [173700/260984], Loss: 2.3703\n",
            "Train Epoch [3/4], Batch [173800/260984], Loss: 2.2842\n",
            "Train Epoch [3/4], Batch [173900/260984], Loss: 2.3446\n",
            "Train Epoch [3/4], Batch [174000/260984], Loss: 2.3769\n",
            "Train Epoch [3/4], Batch [174100/260984], Loss: 2.2846\n",
            "Train Epoch [3/4], Batch [174200/260984], Loss: 2.2314\n",
            "Train Epoch [3/4], Batch [174300/260984], Loss: 2.2449\n",
            "Train Epoch [3/4], Batch [174400/260984], Loss: 2.5162\n",
            "Train Epoch [3/4], Batch [174500/260984], Loss: 2.3547\n",
            "Train Epoch [3/4], Batch [174600/260984], Loss: 2.1917\n",
            "Train Epoch [3/4], Batch [174700/260984], Loss: 2.1971\n",
            "Train Epoch [3/4], Batch [174800/260984], Loss: 2.4570\n",
            "Train Epoch [3/4], Batch [174900/260984], Loss: 2.2566\n",
            "Train Epoch [3/4], Batch [175000/260984], Loss: 2.4296\n",
            "Train Epoch [3/4], Batch [175100/260984], Loss: 2.3706\n",
            "Train Epoch [3/4], Batch [175200/260984], Loss: 2.2480\n",
            "Train Epoch [3/4], Batch [175300/260984], Loss: 2.3129\n",
            "Train Epoch [3/4], Batch [175400/260984], Loss: 2.2843\n",
            "Train Epoch [3/4], Batch [175500/260984], Loss: 2.3184\n",
            "Train Epoch [3/4], Batch [175600/260984], Loss: 2.3294\n",
            "Train Epoch [3/4], Batch [175700/260984], Loss: 2.4381\n",
            "Train Epoch [3/4], Batch [175800/260984], Loss: 2.3338\n",
            "Train Epoch [3/4], Batch [175900/260984], Loss: 2.4974\n",
            "Train Epoch [3/4], Batch [176000/260984], Loss: 2.2806\n",
            "Train Epoch [3/4], Batch [176100/260984], Loss: 2.2699\n",
            "Train Epoch [3/4], Batch [176200/260984], Loss: 2.2703\n",
            "Train Epoch [3/4], Batch [176300/260984], Loss: 2.3374\n",
            "Train Epoch [3/4], Batch [176400/260984], Loss: 2.3662\n",
            "Train Epoch [3/4], Batch [176500/260984], Loss: 2.3285\n",
            "Train Epoch [3/4], Batch [176600/260984], Loss: 2.2877\n",
            "Train Epoch [3/4], Batch [176700/260984], Loss: 2.3250\n",
            "Train Epoch [3/4], Batch [176800/260984], Loss: 2.2800\n",
            "Train Epoch [3/4], Batch [176900/260984], Loss: 2.2974\n",
            "Train Epoch [3/4], Batch [177000/260984], Loss: 2.4121\n",
            "Train Epoch [3/4], Batch [177100/260984], Loss: 2.3627\n",
            "Train Epoch [3/4], Batch [177200/260984], Loss: 2.2254\n",
            "Train Epoch [3/4], Batch [177300/260984], Loss: 2.3086\n",
            "Train Epoch [3/4], Batch [177400/260984], Loss: 2.5030\n",
            "Train Epoch [3/4], Batch [177500/260984], Loss: 2.3200\n",
            "Train Epoch [3/4], Batch [177600/260984], Loss: 2.3127\n",
            "Train Epoch [3/4], Batch [177700/260984], Loss: 2.3704\n",
            "Train Epoch [3/4], Batch [177800/260984], Loss: 2.3999\n",
            "Train Epoch [3/4], Batch [177900/260984], Loss: 2.0482\n",
            "Train Epoch [3/4], Batch [178000/260984], Loss: 2.2239\n",
            "Train Epoch [3/4], Batch [178100/260984], Loss: 2.2064\n",
            "Train Epoch [3/4], Batch [178200/260984], Loss: 2.3078\n",
            "Train Epoch [3/4], Batch [178300/260984], Loss: 2.2734\n",
            "Train Epoch [3/4], Batch [178400/260984], Loss: 2.1894\n",
            "Train Epoch [3/4], Batch [178500/260984], Loss: 2.3189\n",
            "Train Epoch [3/4], Batch [178600/260984], Loss: 2.3849\n",
            "Train Epoch [3/4], Batch [178700/260984], Loss: 2.3761\n",
            "Train Epoch [3/4], Batch [178800/260984], Loss: 2.2930\n",
            "Train Epoch [3/4], Batch [178900/260984], Loss: 2.4013\n",
            "Train Epoch [3/4], Batch [179000/260984], Loss: 2.3096\n",
            "Train Epoch [3/4], Batch [179100/260984], Loss: 2.3738\n",
            "Train Epoch [3/4], Batch [179200/260984], Loss: 2.2578\n",
            "Train Epoch [3/4], Batch [179300/260984], Loss: 2.3117\n",
            "Train Epoch [3/4], Batch [179400/260984], Loss: 2.3965\n",
            "Train Epoch [3/4], Batch [179500/260984], Loss: 2.4071\n",
            "Train Epoch [3/4], Batch [179600/260984], Loss: 2.3444\n",
            "Train Epoch [3/4], Batch [179700/260984], Loss: 2.3728\n",
            "Train Epoch [3/4], Batch [179800/260984], Loss: 2.5472\n",
            "Train Epoch [3/4], Batch [179900/260984], Loss: 2.3127\n",
            "Train Epoch [3/4], Batch [180000/260984], Loss: 2.3539\n",
            "Train Epoch [3/4], Batch [180100/260984], Loss: 2.2736\n",
            "Train Epoch [3/4], Batch [180200/260984], Loss: 2.4003\n",
            "Train Epoch [3/4], Batch [180300/260984], Loss: 2.4362\n",
            "Train Epoch [3/4], Batch [180400/260984], Loss: 2.3999\n",
            "Train Epoch [3/4], Batch [180500/260984], Loss: 2.2574\n",
            "Train Epoch [3/4], Batch [180600/260984], Loss: 2.3246\n",
            "Train Epoch [3/4], Batch [180700/260984], Loss: 2.1657\n",
            "Train Epoch [3/4], Batch [180800/260984], Loss: 2.2862\n",
            "Train Epoch [3/4], Batch [180900/260984], Loss: 2.4259\n",
            "Train Epoch [3/4], Batch [181000/260984], Loss: 2.4721\n",
            "Train Epoch [3/4], Batch [181100/260984], Loss: 2.0671\n",
            "Train Epoch [3/4], Batch [181200/260984], Loss: 2.1465\n",
            "Train Epoch [3/4], Batch [181300/260984], Loss: 2.3744\n",
            "Train Epoch [3/4], Batch [181400/260984], Loss: 2.4010\n",
            "Train Epoch [3/4], Batch [181500/260984], Loss: 2.3566\n",
            "Train Epoch [3/4], Batch [181600/260984], Loss: 2.3105\n",
            "Train Epoch [3/4], Batch [181700/260984], Loss: 2.3233\n",
            "Train Epoch [3/4], Batch [181800/260984], Loss: 2.2536\n",
            "Train Epoch [3/4], Batch [181900/260984], Loss: 2.3535\n",
            "Train Epoch [3/4], Batch [182000/260984], Loss: 2.3812\n",
            "Train Epoch [3/4], Batch [182100/260984], Loss: 2.2908\n",
            "Train Epoch [3/4], Batch [182200/260984], Loss: 2.2095\n",
            "Train Epoch [3/4], Batch [182300/260984], Loss: 2.3579\n",
            "Train Epoch [3/4], Batch [182400/260984], Loss: 2.3431\n",
            "Train Epoch [3/4], Batch [182500/260984], Loss: 2.3982\n",
            "Train Epoch [3/4], Batch [182600/260984], Loss: 2.2328\n",
            "Train Epoch [3/4], Batch [182700/260984], Loss: 2.1586\n",
            "Train Epoch [3/4], Batch [182800/260984], Loss: 2.3140\n",
            "Train Epoch [3/4], Batch [182900/260984], Loss: 2.3469\n",
            "Train Epoch [3/4], Batch [183000/260984], Loss: 2.2976\n",
            "Train Epoch [3/4], Batch [183100/260984], Loss: 2.2499\n",
            "Train Epoch [3/4], Batch [183200/260984], Loss: 2.4518\n",
            "Train Epoch [3/4], Batch [183300/260984], Loss: 2.3183\n",
            "Train Epoch [3/4], Batch [183400/260984], Loss: 2.3088\n",
            "Train Epoch [3/4], Batch [183500/260984], Loss: 2.2515\n",
            "Train Epoch [3/4], Batch [183600/260984], Loss: 2.2151\n",
            "Train Epoch [3/4], Batch [183700/260984], Loss: 2.3899\n",
            "Train Epoch [3/4], Batch [183800/260984], Loss: 2.2287\n",
            "Train Epoch [3/4], Batch [183900/260984], Loss: 2.2210\n",
            "Train Epoch [3/4], Batch [184000/260984], Loss: 2.2500\n",
            "Train Epoch [3/4], Batch [184100/260984], Loss: 2.4389\n",
            "Train Epoch [3/4], Batch [184200/260984], Loss: 2.2991\n",
            "Train Epoch [3/4], Batch [184300/260984], Loss: 2.4110\n",
            "Train Epoch [3/4], Batch [184400/260984], Loss: 2.3091\n",
            "Train Epoch [3/4], Batch [184500/260984], Loss: 2.3258\n",
            "Train Epoch [3/4], Batch [184600/260984], Loss: 2.3391\n",
            "Train Epoch [3/4], Batch [184700/260984], Loss: 2.3785\n",
            "Train Epoch [3/4], Batch [184800/260984], Loss: 2.2845\n",
            "Train Epoch [3/4], Batch [184900/260984], Loss: 2.1800\n",
            "Train Epoch [3/4], Batch [185000/260984], Loss: 2.2472\n",
            "Train Epoch [3/4], Batch [185100/260984], Loss: 2.3830\n",
            "Train Epoch [3/4], Batch [185200/260984], Loss: 2.2749\n",
            "Train Epoch [3/4], Batch [185300/260984], Loss: 2.3954\n",
            "Train Epoch [3/4], Batch [185400/260984], Loss: 2.3536\n",
            "Train Epoch [3/4], Batch [185500/260984], Loss: 2.2139\n",
            "Train Epoch [3/4], Batch [185600/260984], Loss: 2.2991\n",
            "Train Epoch [3/4], Batch [185700/260984], Loss: 2.2592\n",
            "Train Epoch [3/4], Batch [185800/260984], Loss: 2.2763\n",
            "Train Epoch [3/4], Batch [185900/260984], Loss: 2.2281\n",
            "Train Epoch [3/4], Batch [186000/260984], Loss: 2.1923\n",
            "Train Epoch [3/4], Batch [186100/260984], Loss: 2.3470\n",
            "Train Epoch [3/4], Batch [186200/260984], Loss: 2.3463\n",
            "Train Epoch [3/4], Batch [186300/260984], Loss: 2.3820\n",
            "Train Epoch [3/4], Batch [186400/260984], Loss: 2.1692\n",
            "Train Epoch [3/4], Batch [186500/260984], Loss: 2.3175\n",
            "Train Epoch [3/4], Batch [186600/260984], Loss: 2.2746\n",
            "Train Epoch [3/4], Batch [186700/260984], Loss: 2.2905\n",
            "Train Epoch [3/4], Batch [186800/260984], Loss: 2.2594\n",
            "Train Epoch [3/4], Batch [186900/260984], Loss: 2.2523\n",
            "Train Epoch [3/4], Batch [187000/260984], Loss: 2.2636\n",
            "Train Epoch [3/4], Batch [187100/260984], Loss: 2.4207\n",
            "Train Epoch [3/4], Batch [187200/260984], Loss: 2.2816\n",
            "Train Epoch [3/4], Batch [187300/260984], Loss: 2.2455\n",
            "Train Epoch [3/4], Batch [187400/260984], Loss: 2.3709\n",
            "Train Epoch [3/4], Batch [187500/260984], Loss: 2.2331\n",
            "Train Epoch [3/4], Batch [187600/260984], Loss: 2.2294\n",
            "Train Epoch [3/4], Batch [187700/260984], Loss: 2.2417\n",
            "Train Epoch [3/4], Batch [187800/260984], Loss: 2.3363\n",
            "Train Epoch [3/4], Batch [187900/260984], Loss: 2.2704\n",
            "Train Epoch [3/4], Batch [188000/260984], Loss: 2.2645\n",
            "Train Epoch [3/4], Batch [188100/260984], Loss: 2.4156\n",
            "Train Epoch [3/4], Batch [188200/260984], Loss: 2.3789\n",
            "Train Epoch [3/4], Batch [188300/260984], Loss: 2.3367\n",
            "Train Epoch [3/4], Batch [188400/260984], Loss: 2.3721\n",
            "Train Epoch [3/4], Batch [188500/260984], Loss: 2.3076\n",
            "Train Epoch [3/4], Batch [188600/260984], Loss: 2.3502\n",
            "Train Epoch [3/4], Batch [188700/260984], Loss: 2.4499\n",
            "Train Epoch [3/4], Batch [188800/260984], Loss: 2.3411\n",
            "Train Epoch [3/4], Batch [188900/260984], Loss: 2.3144\n",
            "Train Epoch [3/4], Batch [189000/260984], Loss: 2.3687\n",
            "Train Epoch [3/4], Batch [189100/260984], Loss: 2.3199\n",
            "Train Epoch [3/4], Batch [189200/260984], Loss: 2.3442\n",
            "Train Epoch [3/4], Batch [189300/260984], Loss: 2.3920\n",
            "Train Epoch [3/4], Batch [189400/260984], Loss: 2.2495\n",
            "Train Epoch [3/4], Batch [189500/260984], Loss: 2.3562\n",
            "Train Epoch [3/4], Batch [189600/260984], Loss: 2.3527\n",
            "Train Epoch [3/4], Batch [189700/260984], Loss: 2.3663\n",
            "Train Epoch [3/4], Batch [189800/260984], Loss: 2.2124\n",
            "Train Epoch [3/4], Batch [189900/260984], Loss: 2.2904\n",
            "Train Epoch [3/4], Batch [190000/260984], Loss: 2.2603\n",
            "Train Epoch [3/4], Batch [190100/260984], Loss: 2.3719\n",
            "Train Epoch [3/4], Batch [190200/260984], Loss: 2.2301\n",
            "Train Epoch [3/4], Batch [190300/260984], Loss: 2.1716\n",
            "Train Epoch [3/4], Batch [190400/260984], Loss: 2.2107\n",
            "Train Epoch [3/4], Batch [190500/260984], Loss: 2.3050\n",
            "Train Epoch [3/4], Batch [190600/260984], Loss: 2.2619\n",
            "Train Epoch [3/4], Batch [190700/260984], Loss: 2.1590\n",
            "Train Epoch [3/4], Batch [190800/260984], Loss: 2.3475\n",
            "Train Epoch [3/4], Batch [190900/260984], Loss: 2.2379\n",
            "Train Epoch [3/4], Batch [191000/260984], Loss: 2.2970\n",
            "Train Epoch [3/4], Batch [191100/260984], Loss: 2.3001\n",
            "Train Epoch [3/4], Batch [191200/260984], Loss: 2.3134\n",
            "Train Epoch [3/4], Batch [191300/260984], Loss: 2.3221\n",
            "Train Epoch [3/4], Batch [191400/260984], Loss: 2.4467\n",
            "Train Epoch [3/4], Batch [191500/260984], Loss: 2.4676\n",
            "Train Epoch [3/4], Batch [191600/260984], Loss: 2.2288\n",
            "Train Epoch [3/4], Batch [191700/260984], Loss: 2.2129\n",
            "Train Epoch [3/4], Batch [191800/260984], Loss: 2.2569\n",
            "Train Epoch [3/4], Batch [191900/260984], Loss: 2.4004\n",
            "Train Epoch [3/4], Batch [192000/260984], Loss: 2.1228\n",
            "Train Epoch [3/4], Batch [192100/260984], Loss: 2.1593\n",
            "Train Epoch [3/4], Batch [192200/260984], Loss: 2.2215\n",
            "Train Epoch [3/4], Batch [192300/260984], Loss: 2.1674\n",
            "Train Epoch [3/4], Batch [192400/260984], Loss: 2.3576\n",
            "Train Epoch [3/4], Batch [192500/260984], Loss: 2.2388\n",
            "Train Epoch [3/4], Batch [192600/260984], Loss: 2.3404\n",
            "Train Epoch [3/4], Batch [192700/260984], Loss: 2.2221\n",
            "Train Epoch [3/4], Batch [192800/260984], Loss: 2.3165\n",
            "Train Epoch [3/4], Batch [192900/260984], Loss: 2.3133\n",
            "Train Epoch [3/4], Batch [193000/260984], Loss: 2.2604\n",
            "Train Epoch [3/4], Batch [193100/260984], Loss: 2.1643\n",
            "Train Epoch [3/4], Batch [193200/260984], Loss: 2.2249\n",
            "Train Epoch [3/4], Batch [193300/260984], Loss: 2.3453\n",
            "Train Epoch [3/4], Batch [193400/260984], Loss: 2.2421\n",
            "Train Epoch [3/4], Batch [193500/260984], Loss: 2.1847\n",
            "Train Epoch [3/4], Batch [193600/260984], Loss: 2.2893\n",
            "Train Epoch [3/4], Batch [193700/260984], Loss: 2.3569\n",
            "Train Epoch [3/4], Batch [193800/260984], Loss: 2.2556\n",
            "Train Epoch [3/4], Batch [193900/260984], Loss: 2.1348\n",
            "Train Epoch [3/4], Batch [194000/260984], Loss: 2.3107\n",
            "Train Epoch [3/4], Batch [194100/260984], Loss: 2.3034\n",
            "Train Epoch [3/4], Batch [194200/260984], Loss: 2.3688\n",
            "Train Epoch [3/4], Batch [194300/260984], Loss: 2.4057\n",
            "Train Epoch [3/4], Batch [194400/260984], Loss: 2.2607\n",
            "Train Epoch [3/4], Batch [194500/260984], Loss: 2.1383\n",
            "Train Epoch [3/4], Batch [194600/260984], Loss: 2.4109\n",
            "Train Epoch [3/4], Batch [194700/260984], Loss: 2.0645\n",
            "Train Epoch [3/4], Batch [194800/260984], Loss: 2.3596\n",
            "Train Epoch [3/4], Batch [194900/260984], Loss: 2.2579\n",
            "Train Epoch [3/4], Batch [195000/260984], Loss: 2.3114\n",
            "Train Epoch [3/4], Batch [195100/260984], Loss: 2.3819\n",
            "Train Epoch [3/4], Batch [195200/260984], Loss: 2.2207\n",
            "Train Epoch [3/4], Batch [195300/260984], Loss: 2.3546\n",
            "Train Epoch [3/4], Batch [195400/260984], Loss: 2.3532\n",
            "Train Epoch [3/4], Batch [195500/260984], Loss: 2.3369\n",
            "Train Epoch [3/4], Batch [195600/260984], Loss: 2.2315\n",
            "Train Epoch [3/4], Batch [195700/260984], Loss: 2.1385\n",
            "Train Epoch [3/4], Batch [195800/260984], Loss: 2.3467\n",
            "Train Epoch [3/4], Batch [195900/260984], Loss: 2.3255\n",
            "Train Epoch [3/4], Batch [196000/260984], Loss: 2.4347\n",
            "Train Epoch [3/4], Batch [196100/260984], Loss: 2.5031\n",
            "Train Epoch [3/4], Batch [196200/260984], Loss: 2.3432\n",
            "Train Epoch [3/4], Batch [196300/260984], Loss: 2.3614\n",
            "Train Epoch [3/4], Batch [196400/260984], Loss: 2.3083\n",
            "Train Epoch [3/4], Batch [196500/260984], Loss: 2.2329\n",
            "Train Epoch [3/4], Batch [196600/260984], Loss: 2.2442\n",
            "Train Epoch [3/4], Batch [196700/260984], Loss: 2.4480\n",
            "Train Epoch [3/4], Batch [196800/260984], Loss: 2.3139\n",
            "Train Epoch [3/4], Batch [196900/260984], Loss: 2.1635\n",
            "Train Epoch [3/4], Batch [197000/260984], Loss: 2.2842\n",
            "Train Epoch [3/4], Batch [197100/260984], Loss: 2.2306\n",
            "Train Epoch [3/4], Batch [197200/260984], Loss: 2.2023\n",
            "Train Epoch [3/4], Batch [197300/260984], Loss: 2.3462\n",
            "Train Epoch [3/4], Batch [197400/260984], Loss: 2.4725\n",
            "Train Epoch [3/4], Batch [197500/260984], Loss: 2.4368\n",
            "Train Epoch [3/4], Batch [197600/260984], Loss: 2.3452\n",
            "Train Epoch [3/4], Batch [197700/260984], Loss: 2.1659\n",
            "Train Epoch [3/4], Batch [197800/260984], Loss: 2.2835\n",
            "Train Epoch [3/4], Batch [197900/260984], Loss: 2.1049\n",
            "Train Epoch [3/4], Batch [198000/260984], Loss: 2.3451\n",
            "Train Epoch [3/4], Batch [198100/260984], Loss: 2.2885\n",
            "Train Epoch [3/4], Batch [198200/260984], Loss: 2.4905\n",
            "Train Epoch [3/4], Batch [198300/260984], Loss: 2.3462\n",
            "Train Epoch [3/4], Batch [198400/260984], Loss: 2.3088\n",
            "Train Epoch [3/4], Batch [198500/260984], Loss: 2.4467\n",
            "Train Epoch [3/4], Batch [198600/260984], Loss: 2.2046\n",
            "Train Epoch [3/4], Batch [198700/260984], Loss: 2.2673\n",
            "Train Epoch [3/4], Batch [198800/260984], Loss: 2.2722\n",
            "Train Epoch [3/4], Batch [198900/260984], Loss: 2.2926\n",
            "Train Epoch [3/4], Batch [199000/260984], Loss: 2.3766\n",
            "Train Epoch [3/4], Batch [199100/260984], Loss: 2.2655\n",
            "Train Epoch [3/4], Batch [199200/260984], Loss: 2.1919\n",
            "Train Epoch [3/4], Batch [199300/260984], Loss: 2.2427\n",
            "Train Epoch [3/4], Batch [199400/260984], Loss: 2.3988\n",
            "Train Epoch [3/4], Batch [199500/260984], Loss: 2.2508\n",
            "Train Epoch [3/4], Batch [199600/260984], Loss: 2.3070\n",
            "Train Epoch [3/4], Batch [199700/260984], Loss: 2.3248\n",
            "Train Epoch [3/4], Batch [199800/260984], Loss: 2.2049\n",
            "Train Epoch [3/4], Batch [199900/260984], Loss: 2.1597\n",
            "Train Epoch [3/4], Batch [200000/260984], Loss: 2.3499\n",
            "Train Epoch [3/4], Batch [200100/260984], Loss: 2.3319\n",
            "Train Epoch [3/4], Batch [200200/260984], Loss: 2.2690\n",
            "Train Epoch [3/4], Batch [200300/260984], Loss: 2.2855\n",
            "Train Epoch [3/4], Batch [200400/260984], Loss: 2.4471\n",
            "Train Epoch [3/4], Batch [200500/260984], Loss: 2.2642\n",
            "Train Epoch [3/4], Batch [200600/260984], Loss: 2.1584\n",
            "Train Epoch [3/4], Batch [200700/260984], Loss: 2.3202\n",
            "Train Epoch [3/4], Batch [200800/260984], Loss: 2.3695\n",
            "Train Epoch [3/4], Batch [200900/260984], Loss: 2.2600\n",
            "Train Epoch [3/4], Batch [201000/260984], Loss: 2.3410\n",
            "Train Epoch [3/4], Batch [201100/260984], Loss: 2.2575\n",
            "Train Epoch [3/4], Batch [201200/260984], Loss: 2.2870\n",
            "Train Epoch [3/4], Batch [201300/260984], Loss: 2.2536\n",
            "Train Epoch [3/4], Batch [201400/260984], Loss: 2.4226\n",
            "Train Epoch [3/4], Batch [201500/260984], Loss: 2.2492\n",
            "Train Epoch [3/4], Batch [201600/260984], Loss: 2.3074\n",
            "Train Epoch [3/4], Batch [201700/260984], Loss: 2.3710\n",
            "Train Epoch [3/4], Batch [201800/260984], Loss: 2.3150\n",
            "Train Epoch [3/4], Batch [201900/260984], Loss: 2.2793\n",
            "Train Epoch [3/4], Batch [202000/260984], Loss: 2.2525\n",
            "Train Epoch [3/4], Batch [202100/260984], Loss: 2.4168\n",
            "Train Epoch [3/4], Batch [202200/260984], Loss: 2.3162\n",
            "Train Epoch [3/4], Batch [202300/260984], Loss: 2.2988\n",
            "Train Epoch [3/4], Batch [202400/260984], Loss: 2.3443\n",
            "Train Epoch [3/4], Batch [202500/260984], Loss: 2.3429\n",
            "Train Epoch [3/4], Batch [202600/260984], Loss: 2.2525\n",
            "Train Epoch [3/4], Batch [202700/260984], Loss: 2.2496\n",
            "Train Epoch [3/4], Batch [202800/260984], Loss: 2.2503\n",
            "Train Epoch [3/4], Batch [202900/260984], Loss: 2.3030\n",
            "Train Epoch [3/4], Batch [203000/260984], Loss: 2.2864\n",
            "Train Epoch [3/4], Batch [203100/260984], Loss: 2.2257\n",
            "Train Epoch [3/4], Batch [203200/260984], Loss: 2.4298\n",
            "Train Epoch [3/4], Batch [203300/260984], Loss: 2.1521\n",
            "Train Epoch [3/4], Batch [203400/260984], Loss: 2.3149\n",
            "Train Epoch [3/4], Batch [203500/260984], Loss: 2.3537\n",
            "Train Epoch [3/4], Batch [203600/260984], Loss: 2.3390\n",
            "Train Epoch [3/4], Batch [203700/260984], Loss: 2.2909\n",
            "Train Epoch [3/4], Batch [203800/260984], Loss: 2.3130\n",
            "Train Epoch [3/4], Batch [203900/260984], Loss: 2.3045\n",
            "Train Epoch [3/4], Batch [204000/260984], Loss: 2.2784\n",
            "Train Epoch [3/4], Batch [204100/260984], Loss: 2.2844\n",
            "Train Epoch [3/4], Batch [204200/260984], Loss: 2.3894\n",
            "Train Epoch [3/4], Batch [204300/260984], Loss: 2.2169\n",
            "Train Epoch [3/4], Batch [204400/260984], Loss: 2.2010\n",
            "Train Epoch [3/4], Batch [204500/260984], Loss: 2.3737\n",
            "Train Epoch [3/4], Batch [204600/260984], Loss: 2.3186\n",
            "Train Epoch [3/4], Batch [204700/260984], Loss: 2.1723\n",
            "Train Epoch [3/4], Batch [204800/260984], Loss: 2.4087\n",
            "Train Epoch [3/4], Batch [204900/260984], Loss: 2.2254\n",
            "Train Epoch [3/4], Batch [205000/260984], Loss: 2.3866\n",
            "Train Epoch [3/4], Batch [205100/260984], Loss: 2.2819\n",
            "Train Epoch [3/4], Batch [205200/260984], Loss: 2.2410\n",
            "Train Epoch [3/4], Batch [205300/260984], Loss: 2.4717\n",
            "Train Epoch [3/4], Batch [205400/260984], Loss: 2.4196\n",
            "Train Epoch [3/4], Batch [205500/260984], Loss: 2.3605\n",
            "Train Epoch [3/4], Batch [205600/260984], Loss: 2.2873\n",
            "Train Epoch [3/4], Batch [205700/260984], Loss: 2.4273\n",
            "Train Epoch [3/4], Batch [205800/260984], Loss: 2.3146\n",
            "Train Epoch [3/4], Batch [205900/260984], Loss: 2.2653\n",
            "Train Epoch [3/4], Batch [206000/260984], Loss: 2.2862\n",
            "Train Epoch [3/4], Batch [206100/260984], Loss: 2.4409\n",
            "Train Epoch [3/4], Batch [206200/260984], Loss: 2.2533\n",
            "Train Epoch [3/4], Batch [206300/260984], Loss: 2.3782\n",
            "Train Epoch [3/4], Batch [206400/260984], Loss: 2.3056\n",
            "Train Epoch [3/4], Batch [206500/260984], Loss: 2.2653\n",
            "Train Epoch [3/4], Batch [206600/260984], Loss: 2.2277\n",
            "Train Epoch [3/4], Batch [206700/260984], Loss: 2.2546\n",
            "Train Epoch [3/4], Batch [206800/260984], Loss: 2.3617\n",
            "Train Epoch [3/4], Batch [206900/260984], Loss: 2.2422\n",
            "Train Epoch [3/4], Batch [207000/260984], Loss: 2.3321\n",
            "Train Epoch [3/4], Batch [207100/260984], Loss: 2.4984\n",
            "Train Epoch [3/4], Batch [207200/260984], Loss: 2.2166\n",
            "Train Epoch [3/4], Batch [207300/260984], Loss: 2.3734\n",
            "Train Epoch [3/4], Batch [207400/260984], Loss: 2.1409\n",
            "Train Epoch [3/4], Batch [207500/260984], Loss: 2.4323\n",
            "Train Epoch [3/4], Batch [207600/260984], Loss: 2.1812\n",
            "Train Epoch [3/4], Batch [207700/260984], Loss: 2.3148\n",
            "Train Epoch [3/4], Batch [207800/260984], Loss: 2.3430\n",
            "Train Epoch [3/4], Batch [207900/260984], Loss: 2.4097\n",
            "Train Epoch [3/4], Batch [208000/260984], Loss: 2.3814\n",
            "Train Epoch [3/4], Batch [208100/260984], Loss: 2.1930\n",
            "Train Epoch [3/4], Batch [208200/260984], Loss: 2.2223\n",
            "Train Epoch [3/4], Batch [208300/260984], Loss: 2.3375\n",
            "Train Epoch [3/4], Batch [208400/260984], Loss: 2.3410\n",
            "Train Epoch [3/4], Batch [208500/260984], Loss: 2.2230\n",
            "Train Epoch [3/4], Batch [208600/260984], Loss: 2.3460\n",
            "Train Epoch [3/4], Batch [208700/260984], Loss: 2.3292\n",
            "Train Epoch [3/4], Batch [208800/260984], Loss: 2.2322\n",
            "Train Epoch [3/4], Batch [208900/260984], Loss: 2.4052\n",
            "Train Epoch [3/4], Batch [209000/260984], Loss: 2.3783\n",
            "Train Epoch [3/4], Batch [209100/260984], Loss: 2.3438\n",
            "Train Epoch [3/4], Batch [209200/260984], Loss: 2.3915\n",
            "Train Epoch [3/4], Batch [209300/260984], Loss: 2.2957\n",
            "Train Epoch [3/4], Batch [209400/260984], Loss: 2.3180\n",
            "Train Epoch [3/4], Batch [209500/260984], Loss: 2.3285\n",
            "Train Epoch [3/4], Batch [209600/260984], Loss: 2.4119\n",
            "Train Epoch [3/4], Batch [209700/260984], Loss: 2.3453\n",
            "Train Epoch [3/4], Batch [209800/260984], Loss: 2.2057\n",
            "Train Epoch [3/4], Batch [209900/260984], Loss: 2.2883\n",
            "Train Epoch [3/4], Batch [210000/260984], Loss: 2.2466\n",
            "Train Epoch [3/4], Batch [210100/260984], Loss: 2.2922\n",
            "Train Epoch [3/4], Batch [210200/260984], Loss: 2.0978\n",
            "Train Epoch [3/4], Batch [210300/260984], Loss: 2.2986\n",
            "Train Epoch [3/4], Batch [210400/260984], Loss: 2.3536\n",
            "Train Epoch [3/4], Batch [210500/260984], Loss: 2.2746\n",
            "Train Epoch [3/4], Batch [210600/260984], Loss: 2.3736\n",
            "Train Epoch [3/4], Batch [210700/260984], Loss: 2.4684\n",
            "Train Epoch [3/4], Batch [210800/260984], Loss: 2.3154\n",
            "Train Epoch [3/4], Batch [210900/260984], Loss: 2.1954\n",
            "Train Epoch [3/4], Batch [211000/260984], Loss: 2.1891\n",
            "Train Epoch [3/4], Batch [211100/260984], Loss: 2.3376\n",
            "Train Epoch [3/4], Batch [211200/260984], Loss: 2.3327\n",
            "Train Epoch [3/4], Batch [211300/260984], Loss: 2.3987\n",
            "Train Epoch [3/4], Batch [211400/260984], Loss: 2.2265\n",
            "Train Epoch [3/4], Batch [211500/260984], Loss: 2.3985\n",
            "Train Epoch [3/4], Batch [211600/260984], Loss: 2.2208\n",
            "Train Epoch [3/4], Batch [211700/260984], Loss: 2.3704\n",
            "Train Epoch [3/4], Batch [211800/260984], Loss: 2.2188\n",
            "Train Epoch [3/4], Batch [211900/260984], Loss: 2.1459\n",
            "Train Epoch [3/4], Batch [212000/260984], Loss: 2.2390\n",
            "Train Epoch [3/4], Batch [212100/260984], Loss: 2.2067\n",
            "Train Epoch [3/4], Batch [212200/260984], Loss: 2.3536\n",
            "Train Epoch [3/4], Batch [212300/260984], Loss: 2.1888\n",
            "Train Epoch [3/4], Batch [212400/260984], Loss: 2.3569\n",
            "Train Epoch [3/4], Batch [212500/260984], Loss: 2.4302\n",
            "Train Epoch [3/4], Batch [212600/260984], Loss: 2.3691\n",
            "Train Epoch [3/4], Batch [212700/260984], Loss: 2.3640\n",
            "Train Epoch [3/4], Batch [212800/260984], Loss: 2.3491\n",
            "Train Epoch [3/4], Batch [212900/260984], Loss: 2.3403\n",
            "Train Epoch [3/4], Batch [213000/260984], Loss: 2.2889\n",
            "Train Epoch [3/4], Batch [213100/260984], Loss: 2.2868\n",
            "Train Epoch [3/4], Batch [213200/260984], Loss: 2.3733\n",
            "Train Epoch [3/4], Batch [213300/260984], Loss: 2.1733\n",
            "Train Epoch [3/4], Batch [213400/260984], Loss: 2.3339\n",
            "Train Epoch [3/4], Batch [213500/260984], Loss: 2.2600\n",
            "Train Epoch [3/4], Batch [213600/260984], Loss: 2.2557\n",
            "Train Epoch [3/4], Batch [213700/260984], Loss: 2.2586\n",
            "Train Epoch [3/4], Batch [213800/260984], Loss: 2.3379\n",
            "Train Epoch [3/4], Batch [213900/260984], Loss: 2.2841\n",
            "Train Epoch [3/4], Batch [214000/260984], Loss: 2.3712\n",
            "Train Epoch [3/4], Batch [214100/260984], Loss: 2.2906\n",
            "Train Epoch [3/4], Batch [214200/260984], Loss: 2.2970\n",
            "Train Epoch [3/4], Batch [214300/260984], Loss: 2.2714\n",
            "Train Epoch [3/4], Batch [214400/260984], Loss: 2.3293\n",
            "Train Epoch [3/4], Batch [214500/260984], Loss: 2.2529\n",
            "Train Epoch [3/4], Batch [214600/260984], Loss: 2.2012\n",
            "Train Epoch [3/4], Batch [214700/260984], Loss: 2.3256\n",
            "Train Epoch [3/4], Batch [214800/260984], Loss: 2.2563\n",
            "Train Epoch [3/4], Batch [214900/260984], Loss: 2.3756\n",
            "Train Epoch [3/4], Batch [215000/260984], Loss: 2.3410\n",
            "Train Epoch [3/4], Batch [215100/260984], Loss: 2.4452\n",
            "Train Epoch [3/4], Batch [215200/260984], Loss: 2.2969\n",
            "Train Epoch [3/4], Batch [215300/260984], Loss: 2.3335\n",
            "Train Epoch [3/4], Batch [215400/260984], Loss: 2.2535\n",
            "Train Epoch [3/4], Batch [215500/260984], Loss: 2.2461\n",
            "Train Epoch [3/4], Batch [215600/260984], Loss: 2.2937\n",
            "Train Epoch [3/4], Batch [215700/260984], Loss: 2.2203\n",
            "Train Epoch [3/4], Batch [215800/260984], Loss: 2.3722\n",
            "Train Epoch [3/4], Batch [215900/260984], Loss: 2.3181\n",
            "Train Epoch [3/4], Batch [216000/260984], Loss: 2.5321\n",
            "Train Epoch [3/4], Batch [216100/260984], Loss: 2.3194\n",
            "Train Epoch [3/4], Batch [216200/260984], Loss: 2.2582\n",
            "Train Epoch [3/4], Batch [216300/260984], Loss: 2.2481\n",
            "Train Epoch [3/4], Batch [216400/260984], Loss: 2.4690\n",
            "Train Epoch [3/4], Batch [216500/260984], Loss: 2.3133\n",
            "Train Epoch [3/4], Batch [216600/260984], Loss: 2.3373\n",
            "Train Epoch [3/4], Batch [216700/260984], Loss: 2.2542\n",
            "Train Epoch [3/4], Batch [216800/260984], Loss: 2.2536\n",
            "Train Epoch [3/4], Batch [216900/260984], Loss: 2.2830\n",
            "Train Epoch [3/4], Batch [217000/260984], Loss: 2.2563\n",
            "Train Epoch [3/4], Batch [217100/260984], Loss: 2.2663\n",
            "Train Epoch [3/4], Batch [217200/260984], Loss: 2.3797\n",
            "Train Epoch [3/4], Batch [217300/260984], Loss: 2.3349\n",
            "Train Epoch [3/4], Batch [217400/260984], Loss: 2.4077\n",
            "Train Epoch [3/4], Batch [217500/260984], Loss: 2.1757\n",
            "Train Epoch [3/4], Batch [217600/260984], Loss: 2.2940\n",
            "Train Epoch [3/4], Batch [217700/260984], Loss: 2.3030\n",
            "Train Epoch [3/4], Batch [217800/260984], Loss: 2.2493\n",
            "Train Epoch [3/4], Batch [217900/260984], Loss: 2.3136\n",
            "Train Epoch [3/4], Batch [218000/260984], Loss: 2.3906\n",
            "Train Epoch [3/4], Batch [218100/260984], Loss: 2.1886\n",
            "Train Epoch [3/4], Batch [218200/260984], Loss: 2.3186\n",
            "Train Epoch [3/4], Batch [218300/260984], Loss: 2.4627\n",
            "Train Epoch [3/4], Batch [218400/260984], Loss: 2.2315\n",
            "Train Epoch [3/4], Batch [218500/260984], Loss: 2.3513\n",
            "Train Epoch [3/4], Batch [218600/260984], Loss: 2.2265\n",
            "Train Epoch [3/4], Batch [218700/260984], Loss: 2.3330\n",
            "Train Epoch [3/4], Batch [218800/260984], Loss: 2.4685\n",
            "Train Epoch [3/4], Batch [218900/260984], Loss: 2.2298\n",
            "Train Epoch [3/4], Batch [219000/260984], Loss: 2.4719\n",
            "Train Epoch [3/4], Batch [219100/260984], Loss: 2.2232\n",
            "Train Epoch [3/4], Batch [219200/260984], Loss: 2.2996\n",
            "Train Epoch [3/4], Batch [219300/260984], Loss: 2.2220\n",
            "Train Epoch [3/4], Batch [219400/260984], Loss: 2.2006\n",
            "Train Epoch [3/4], Batch [219500/260984], Loss: 2.3101\n",
            "Train Epoch [3/4], Batch [219600/260984], Loss: 2.3298\n",
            "Train Epoch [3/4], Batch [219700/260984], Loss: 2.3320\n",
            "Train Epoch [3/4], Batch [219800/260984], Loss: 2.2828\n",
            "Train Epoch [3/4], Batch [219900/260984], Loss: 2.2807\n",
            "Train Epoch [3/4], Batch [220000/260984], Loss: 2.2778\n",
            "Train Epoch [3/4], Batch [220100/260984], Loss: 2.3156\n",
            "Train Epoch [3/4], Batch [220200/260984], Loss: 2.2646\n",
            "Train Epoch [3/4], Batch [220300/260984], Loss: 2.2901\n",
            "Train Epoch [3/4], Batch [220400/260984], Loss: 2.3077\n",
            "Train Epoch [3/4], Batch [220500/260984], Loss: 2.3145\n",
            "Train Epoch [3/4], Batch [220600/260984], Loss: 2.3430\n",
            "Train Epoch [3/4], Batch [220700/260984], Loss: 2.2370\n",
            "Train Epoch [3/4], Batch [220800/260984], Loss: 2.3059\n",
            "Train Epoch [3/4], Batch [220900/260984], Loss: 2.3004\n",
            "Train Epoch [3/4], Batch [221000/260984], Loss: 2.2015\n",
            "Train Epoch [3/4], Batch [221100/260984], Loss: 2.2735\n",
            "Train Epoch [3/4], Batch [221200/260984], Loss: 2.2540\n",
            "Train Epoch [3/4], Batch [221300/260984], Loss: 2.4965\n",
            "Train Epoch [3/4], Batch [221400/260984], Loss: 2.2872\n",
            "Train Epoch [3/4], Batch [221500/260984], Loss: 2.1473\n",
            "Train Epoch [3/4], Batch [221600/260984], Loss: 2.2572\n",
            "Train Epoch [3/4], Batch [221700/260984], Loss: 2.3461\n",
            "Train Epoch [3/4], Batch [221800/260984], Loss: 2.2390\n",
            "Train Epoch [3/4], Batch [221900/260984], Loss: 2.2595\n",
            "Train Epoch [3/4], Batch [222000/260984], Loss: 2.1541\n",
            "Train Epoch [3/4], Batch [222100/260984], Loss: 2.3990\n",
            "Train Epoch [3/4], Batch [222200/260984], Loss: 2.2603\n",
            "Train Epoch [3/4], Batch [222300/260984], Loss: 2.2278\n",
            "Train Epoch [3/4], Batch [222400/260984], Loss: 2.3534\n",
            "Train Epoch [3/4], Batch [222500/260984], Loss: 2.3091\n",
            "Train Epoch [3/4], Batch [222600/260984], Loss: 2.1618\n",
            "Train Epoch [3/4], Batch [222700/260984], Loss: 2.2851\n",
            "Train Epoch [3/4], Batch [222800/260984], Loss: 2.3048\n",
            "Train Epoch [3/4], Batch [222900/260984], Loss: 2.4380\n",
            "Train Epoch [3/4], Batch [223000/260984], Loss: 2.3365\n",
            "Train Epoch [3/4], Batch [223100/260984], Loss: 2.2429\n",
            "Train Epoch [3/4], Batch [223200/260984], Loss: 2.3789\n",
            "Train Epoch [3/4], Batch [223300/260984], Loss: 2.1600\n",
            "Train Epoch [3/4], Batch [223400/260984], Loss: 2.3426\n",
            "Train Epoch [3/4], Batch [223500/260984], Loss: 2.2259\n",
            "Train Epoch [3/4], Batch [223600/260984], Loss: 2.1302\n",
            "Train Epoch [3/4], Batch [223700/260984], Loss: 2.3457\n",
            "Train Epoch [3/4], Batch [223800/260984], Loss: 2.2843\n",
            "Train Epoch [3/4], Batch [223900/260984], Loss: 2.3471\n",
            "Train Epoch [3/4], Batch [224000/260984], Loss: 2.2560\n",
            "Train Epoch [3/4], Batch [224100/260984], Loss: 2.2325\n",
            "Train Epoch [3/4], Batch [224200/260984], Loss: 2.3454\n",
            "Train Epoch [3/4], Batch [224300/260984], Loss: 2.2522\n",
            "Train Epoch [3/4], Batch [224400/260984], Loss: 2.3352\n",
            "Train Epoch [3/4], Batch [224500/260984], Loss: 2.3191\n",
            "Train Epoch [3/4], Batch [224600/260984], Loss: 2.3208\n",
            "Train Epoch [3/4], Batch [224700/260984], Loss: 2.2715\n",
            "Train Epoch [3/4], Batch [224800/260984], Loss: 2.3672\n",
            "Train Epoch [3/4], Batch [224900/260984], Loss: 2.4299\n",
            "Train Epoch [3/4], Batch [225000/260984], Loss: 2.3830\n",
            "Train Epoch [3/4], Batch [225100/260984], Loss: 2.2503\n",
            "Train Epoch [3/4], Batch [225200/260984], Loss: 2.3440\n",
            "Train Epoch [3/4], Batch [225300/260984], Loss: 2.4392\n",
            "Train Epoch [3/4], Batch [225400/260984], Loss: 2.2847\n",
            "Train Epoch [3/4], Batch [225500/260984], Loss: 2.4405\n",
            "Train Epoch [3/4], Batch [225600/260984], Loss: 2.3213\n",
            "Train Epoch [3/4], Batch [225700/260984], Loss: 2.2633\n",
            "Train Epoch [3/4], Batch [225800/260984], Loss: 2.4331\n",
            "Train Epoch [3/4], Batch [225900/260984], Loss: 2.2827\n",
            "Train Epoch [3/4], Batch [226000/260984], Loss: 2.2315\n",
            "Train Epoch [3/4], Batch [226100/260984], Loss: 2.2968\n",
            "Train Epoch [3/4], Batch [226200/260984], Loss: 2.2877\n",
            "Train Epoch [3/4], Batch [226300/260984], Loss: 2.4041\n",
            "Train Epoch [3/4], Batch [226400/260984], Loss: 2.4137\n",
            "Train Epoch [3/4], Batch [226500/260984], Loss: 2.4010\n",
            "Train Epoch [3/4], Batch [226600/260984], Loss: 2.3881\n",
            "Train Epoch [3/4], Batch [226700/260984], Loss: 2.2158\n",
            "Train Epoch [3/4], Batch [226800/260984], Loss: 2.2908\n",
            "Train Epoch [3/4], Batch [226900/260984], Loss: 2.3246\n",
            "Train Epoch [3/4], Batch [227000/260984], Loss: 2.3075\n",
            "Train Epoch [3/4], Batch [227100/260984], Loss: 2.2455\n",
            "Train Epoch [3/4], Batch [227200/260984], Loss: 2.1438\n",
            "Train Epoch [3/4], Batch [227300/260984], Loss: 2.1337\n",
            "Train Epoch [3/4], Batch [227400/260984], Loss: 2.4038\n",
            "Train Epoch [3/4], Batch [227500/260984], Loss: 2.1866\n",
            "Train Epoch [3/4], Batch [227600/260984], Loss: 2.2260\n",
            "Train Epoch [3/4], Batch [227700/260984], Loss: 2.3419\n",
            "Train Epoch [3/4], Batch [227800/260984], Loss: 2.4360\n",
            "Train Epoch [3/4], Batch [227900/260984], Loss: 2.3722\n",
            "Train Epoch [3/4], Batch [228000/260984], Loss: 2.2291\n",
            "Train Epoch [3/4], Batch [228100/260984], Loss: 2.0713\n",
            "Train Epoch [3/4], Batch [228200/260984], Loss: 2.3646\n",
            "Train Epoch [3/4], Batch [228300/260984], Loss: 2.2058\n",
            "Train Epoch [3/4], Batch [228400/260984], Loss: 2.3774\n",
            "Train Epoch [3/4], Batch [228500/260984], Loss: 2.4234\n",
            "Train Epoch [3/4], Batch [228600/260984], Loss: 2.3399\n",
            "Train Epoch [3/4], Batch [228700/260984], Loss: 2.2790\n",
            "Train Epoch [3/4], Batch [228800/260984], Loss: 2.2549\n",
            "Train Epoch [3/4], Batch [228900/260984], Loss: 2.4388\n",
            "Train Epoch [3/4], Batch [229000/260984], Loss: 2.4085\n",
            "Train Epoch [3/4], Batch [229100/260984], Loss: 2.2298\n",
            "Train Epoch [3/4], Batch [229200/260984], Loss: 2.3227\n",
            "Train Epoch [3/4], Batch [229300/260984], Loss: 2.1046\n",
            "Train Epoch [3/4], Batch [229400/260984], Loss: 2.2057\n",
            "Train Epoch [3/4], Batch [229500/260984], Loss: 2.1506\n",
            "Train Epoch [3/4], Batch [229600/260984], Loss: 2.3281\n",
            "Train Epoch [3/4], Batch [229700/260984], Loss: 2.3185\n",
            "Train Epoch [3/4], Batch [229800/260984], Loss: 2.3688\n",
            "Train Epoch [3/4], Batch [229900/260984], Loss: 2.3103\n",
            "Train Epoch [3/4], Batch [230000/260984], Loss: 2.1890\n",
            "Train Epoch [3/4], Batch [230100/260984], Loss: 2.1948\n",
            "Train Epoch [3/4], Batch [230200/260984], Loss: 2.3653\n",
            "Train Epoch [3/4], Batch [230300/260984], Loss: 2.3824\n",
            "Train Epoch [3/4], Batch [230400/260984], Loss: 2.2234\n",
            "Train Epoch [3/4], Batch [230500/260984], Loss: 2.2695\n",
            "Train Epoch [3/4], Batch [230600/260984], Loss: 2.2768\n",
            "Train Epoch [3/4], Batch [230700/260984], Loss: 2.3411\n",
            "Train Epoch [3/4], Batch [230800/260984], Loss: 2.2457\n",
            "Train Epoch [3/4], Batch [230900/260984], Loss: 2.2217\n",
            "Train Epoch [3/4], Batch [231000/260984], Loss: 2.3221\n",
            "Train Epoch [3/4], Batch [231100/260984], Loss: 2.3557\n",
            "Train Epoch [3/4], Batch [231200/260984], Loss: 2.2833\n",
            "Train Epoch [3/4], Batch [231300/260984], Loss: 2.3391\n",
            "Train Epoch [3/4], Batch [231400/260984], Loss: 2.4076\n",
            "Train Epoch [3/4], Batch [231500/260984], Loss: 2.3214\n",
            "Train Epoch [3/4], Batch [231600/260984], Loss: 2.3963\n",
            "Train Epoch [3/4], Batch [231700/260984], Loss: 2.0867\n",
            "Train Epoch [3/4], Batch [231800/260984], Loss: 2.3725\n",
            "Train Epoch [3/4], Batch [231900/260984], Loss: 2.2355\n",
            "Train Epoch [3/4], Batch [232000/260984], Loss: 2.2519\n",
            "Train Epoch [3/4], Batch [232100/260984], Loss: 2.3672\n",
            "Train Epoch [3/4], Batch [232200/260984], Loss: 2.3747\n",
            "Train Epoch [3/4], Batch [232300/260984], Loss: 2.3234\n",
            "Train Epoch [3/4], Batch [232400/260984], Loss: 2.2369\n",
            "Train Epoch [3/4], Batch [232500/260984], Loss: 2.4132\n",
            "Train Epoch [3/4], Batch [232600/260984], Loss: 2.2534\n",
            "Train Epoch [3/4], Batch [232700/260984], Loss: 2.2503\n",
            "Train Epoch [3/4], Batch [232800/260984], Loss: 2.2267\n",
            "Train Epoch [3/4], Batch [232900/260984], Loss: 2.3547\n",
            "Train Epoch [3/4], Batch [233000/260984], Loss: 2.2485\n",
            "Train Epoch [3/4], Batch [233100/260984], Loss: 2.2295\n",
            "Train Epoch [3/4], Batch [233200/260984], Loss: 2.1640\n",
            "Train Epoch [3/4], Batch [233300/260984], Loss: 2.2876\n",
            "Train Epoch [3/4], Batch [233400/260984], Loss: 2.1630\n",
            "Train Epoch [3/4], Batch [233500/260984], Loss: 2.3174\n",
            "Train Epoch [3/4], Batch [233600/260984], Loss: 2.2876\n",
            "Train Epoch [3/4], Batch [233700/260984], Loss: 2.3908\n",
            "Train Epoch [3/4], Batch [233800/260984], Loss: 2.2493\n",
            "Train Epoch [3/4], Batch [233900/260984], Loss: 2.3521\n",
            "Train Epoch [3/4], Batch [234000/260984], Loss: 2.3192\n",
            "Train Epoch [3/4], Batch [234100/260984], Loss: 2.2651\n",
            "Train Epoch [3/4], Batch [234200/260984], Loss: 2.2563\n",
            "Train Epoch [3/4], Batch [234300/260984], Loss: 2.2164\n",
            "Train Epoch [3/4], Batch [234400/260984], Loss: 2.3812\n",
            "Train Epoch [3/4], Batch [234500/260984], Loss: 2.4288\n",
            "Train Epoch [3/4], Batch [234600/260984], Loss: 2.2188\n",
            "Train Epoch [3/4], Batch [234700/260984], Loss: 2.2144\n",
            "Train Epoch [3/4], Batch [234800/260984], Loss: 2.2522\n",
            "Train Epoch [3/4], Batch [234900/260984], Loss: 2.2451\n",
            "Train Epoch [3/4], Batch [235000/260984], Loss: 2.3225\n",
            "Train Epoch [3/4], Batch [235100/260984], Loss: 2.3741\n",
            "Train Epoch [3/4], Batch [235200/260984], Loss: 2.2682\n",
            "Train Epoch [3/4], Batch [235300/260984], Loss: 2.3255\n",
            "Train Epoch [3/4], Batch [235400/260984], Loss: 2.1649\n",
            "Train Epoch [3/4], Batch [235500/260984], Loss: 2.3430\n",
            "Train Epoch [3/4], Batch [235600/260984], Loss: 2.2521\n",
            "Train Epoch [3/4], Batch [235700/260984], Loss: 2.2334\n",
            "Train Epoch [3/4], Batch [235800/260984], Loss: 2.3522\n",
            "Train Epoch [3/4], Batch [235900/260984], Loss: 2.2850\n",
            "Train Epoch [3/4], Batch [236000/260984], Loss: 2.3766\n",
            "Train Epoch [3/4], Batch [236100/260984], Loss: 2.2496\n",
            "Train Epoch [3/4], Batch [236200/260984], Loss: 2.3892\n",
            "Train Epoch [3/4], Batch [236300/260984], Loss: 2.2840\n",
            "Train Epoch [3/4], Batch [236400/260984], Loss: 2.1330\n",
            "Train Epoch [3/4], Batch [236500/260984], Loss: 2.4195\n",
            "Train Epoch [3/4], Batch [236600/260984], Loss: 2.2310\n",
            "Train Epoch [3/4], Batch [236700/260984], Loss: 2.4069\n",
            "Train Epoch [3/4], Batch [236800/260984], Loss: 2.2740\n",
            "Train Epoch [3/4], Batch [236900/260984], Loss: 2.2830\n",
            "Train Epoch [3/4], Batch [237000/260984], Loss: 2.4601\n",
            "Train Epoch [3/4], Batch [237100/260984], Loss: 2.2582\n",
            "Train Epoch [3/4], Batch [237200/260984], Loss: 2.3772\n",
            "Train Epoch [3/4], Batch [237300/260984], Loss: 2.2818\n",
            "Train Epoch [3/4], Batch [237400/260984], Loss: 2.3849\n",
            "Train Epoch [3/4], Batch [237500/260984], Loss: 2.2797\n",
            "Train Epoch [3/4], Batch [237600/260984], Loss: 2.2278\n",
            "Train Epoch [3/4], Batch [237700/260984], Loss: 2.3156\n",
            "Train Epoch [3/4], Batch [237800/260984], Loss: 2.2101\n",
            "Train Epoch [3/4], Batch [237900/260984], Loss: 2.3436\n",
            "Train Epoch [3/4], Batch [238000/260984], Loss: 2.2296\n",
            "Train Epoch [3/4], Batch [238100/260984], Loss: 2.1947\n",
            "Train Epoch [3/4], Batch [238200/260984], Loss: 2.3448\n",
            "Train Epoch [3/4], Batch [238300/260984], Loss: 2.2515\n",
            "Train Epoch [3/4], Batch [238400/260984], Loss: 2.1928\n",
            "Train Epoch [3/4], Batch [238500/260984], Loss: 2.1555\n",
            "Train Epoch [3/4], Batch [238600/260984], Loss: 2.2938\n",
            "Train Epoch [3/4], Batch [238700/260984], Loss: 2.3924\n",
            "Train Epoch [3/4], Batch [238800/260984], Loss: 2.2723\n",
            "Train Epoch [3/4], Batch [238900/260984], Loss: 2.2356\n",
            "Train Epoch [3/4], Batch [239000/260984], Loss: 2.3705\n",
            "Train Epoch [3/4], Batch [239100/260984], Loss: 2.4209\n",
            "Train Epoch [3/4], Batch [239200/260984], Loss: 2.4506\n",
            "Train Epoch [3/4], Batch [239300/260984], Loss: 2.2563\n",
            "Train Epoch [3/4], Batch [239400/260984], Loss: 2.2843\n",
            "Train Epoch [3/4], Batch [239500/260984], Loss: 2.2273\n",
            "Train Epoch [3/4], Batch [239600/260984], Loss: 2.3732\n",
            "Train Epoch [3/4], Batch [239700/260984], Loss: 2.3501\n",
            "Train Epoch [3/4], Batch [239800/260984], Loss: 2.2248\n",
            "Train Epoch [3/4], Batch [239900/260984], Loss: 2.3043\n",
            "Train Epoch [3/4], Batch [240000/260984], Loss: 2.1787\n",
            "Train Epoch [3/4], Batch [240100/260984], Loss: 2.3878\n",
            "Train Epoch [3/4], Batch [240200/260984], Loss: 2.4011\n",
            "Train Epoch [3/4], Batch [240300/260984], Loss: 2.2222\n",
            "Train Epoch [3/4], Batch [240400/260984], Loss: 2.3082\n",
            "Train Epoch [3/4], Batch [240500/260984], Loss: 2.3629\n",
            "Train Epoch [3/4], Batch [240600/260984], Loss: 2.3467\n",
            "Train Epoch [3/4], Batch [240700/260984], Loss: 2.2941\n",
            "Train Epoch [3/4], Batch [240800/260984], Loss: 2.2409\n",
            "Train Epoch [3/4], Batch [240900/260984], Loss: 2.2210\n",
            "Train Epoch [3/4], Batch [241000/260984], Loss: 2.2160\n",
            "Train Epoch [3/4], Batch [241100/260984], Loss: 2.2419\n",
            "Train Epoch [3/4], Batch [241200/260984], Loss: 2.3723\n",
            "Train Epoch [3/4], Batch [241300/260984], Loss: 2.2021\n",
            "Train Epoch [3/4], Batch [241400/260984], Loss: 2.3478\n",
            "Train Epoch [3/4], Batch [241500/260984], Loss: 2.3840\n",
            "Train Epoch [3/4], Batch [241600/260984], Loss: 2.3704\n",
            "Train Epoch [3/4], Batch [241700/260984], Loss: 2.2091\n",
            "Train Epoch [3/4], Batch [241800/260984], Loss: 2.2690\n",
            "Train Epoch [3/4], Batch [241900/260984], Loss: 2.2141\n",
            "Train Epoch [3/4], Batch [242000/260984], Loss: 2.4832\n",
            "Train Epoch [3/4], Batch [242100/260984], Loss: 2.2838\n",
            "Train Epoch [3/4], Batch [242200/260984], Loss: 2.2013\n",
            "Train Epoch [3/4], Batch [242300/260984], Loss: 2.4974\n",
            "Train Epoch [3/4], Batch [242400/260984], Loss: 2.4353\n",
            "Train Epoch [3/4], Batch [242500/260984], Loss: 2.3880\n",
            "Train Epoch [3/4], Batch [242600/260984], Loss: 2.2978\n",
            "Train Epoch [3/4], Batch [242700/260984], Loss: 2.2361\n",
            "Train Epoch [3/4], Batch [242800/260984], Loss: 2.3501\n",
            "Train Epoch [3/4], Batch [242900/260984], Loss: 2.4173\n",
            "Train Epoch [3/4], Batch [243000/260984], Loss: 2.2562\n",
            "Train Epoch [3/4], Batch [243100/260984], Loss: 2.2219\n",
            "Train Epoch [3/4], Batch [243200/260984], Loss: 2.3237\n",
            "Train Epoch [3/4], Batch [243300/260984], Loss: 2.3795\n",
            "Train Epoch [3/4], Batch [243400/260984], Loss: 2.3153\n",
            "Train Epoch [3/4], Batch [243500/260984], Loss: 2.3443\n",
            "Train Epoch [3/4], Batch [243600/260984], Loss: 2.2957\n",
            "Train Epoch [3/4], Batch [243700/260984], Loss: 2.4234\n",
            "Train Epoch [3/4], Batch [243800/260984], Loss: 2.0920\n",
            "Train Epoch [3/4], Batch [243900/260984], Loss: 2.2160\n",
            "Train Epoch [3/4], Batch [244000/260984], Loss: 2.3971\n",
            "Train Epoch [3/4], Batch [244100/260984], Loss: 2.2497\n",
            "Train Epoch [3/4], Batch [244200/260984], Loss: 2.3199\n",
            "Train Epoch [3/4], Batch [244300/260984], Loss: 2.1148\n",
            "Train Epoch [3/4], Batch [244400/260984], Loss: 2.4462\n",
            "Train Epoch [3/4], Batch [244500/260984], Loss: 2.3298\n",
            "Train Epoch [3/4], Batch [244600/260984], Loss: 2.3350\n",
            "Train Epoch [3/4], Batch [244700/260984], Loss: 2.4734\n",
            "Train Epoch [3/4], Batch [244800/260984], Loss: 2.3471\n",
            "Train Epoch [3/4], Batch [244900/260984], Loss: 2.4226\n",
            "Train Epoch [3/4], Batch [245000/260984], Loss: 2.4202\n",
            "Train Epoch [3/4], Batch [245100/260984], Loss: 2.4710\n",
            "Train Epoch [3/4], Batch [245200/260984], Loss: 2.4453\n",
            "Train Epoch [3/4], Batch [245300/260984], Loss: 2.2818\n",
            "Train Epoch [3/4], Batch [245400/260984], Loss: 2.2944\n",
            "Train Epoch [3/4], Batch [245500/260984], Loss: 2.5225\n",
            "Train Epoch [3/4], Batch [245600/260984], Loss: 2.2398\n",
            "Train Epoch [3/4], Batch [245700/260984], Loss: 2.4001\n",
            "Train Epoch [3/4], Batch [245800/260984], Loss: 2.1650\n",
            "Train Epoch [3/4], Batch [245900/260984], Loss: 2.3795\n",
            "Train Epoch [3/4], Batch [246000/260984], Loss: 2.5260\n",
            "Train Epoch [3/4], Batch [246100/260984], Loss: 2.2230\n",
            "Train Epoch [3/4], Batch [246200/260984], Loss: 2.4630\n",
            "Train Epoch [3/4], Batch [246300/260984], Loss: 2.2838\n",
            "Train Epoch [3/4], Batch [246400/260984], Loss: 2.3617\n",
            "Train Epoch [3/4], Batch [246500/260984], Loss: 2.2606\n",
            "Train Epoch [3/4], Batch [246600/260984], Loss: 2.4101\n",
            "Train Epoch [3/4], Batch [246700/260984], Loss: 2.2565\n",
            "Train Epoch [3/4], Batch [246800/260984], Loss: 2.3487\n",
            "Train Epoch [3/4], Batch [246900/260984], Loss: 2.3200\n",
            "Train Epoch [3/4], Batch [247000/260984], Loss: 2.2469\n",
            "Train Epoch [3/4], Batch [247100/260984], Loss: 2.3787\n",
            "Train Epoch [3/4], Batch [247200/260984], Loss: 2.3998\n",
            "Train Epoch [3/4], Batch [247300/260984], Loss: 2.2925\n",
            "Train Epoch [3/4], Batch [247400/260984], Loss: 2.2908\n",
            "Train Epoch [3/4], Batch [247500/260984], Loss: 2.3943\n",
            "Train Epoch [3/4], Batch [247600/260984], Loss: 2.4391\n",
            "Train Epoch [3/4], Batch [247700/260984], Loss: 2.3659\n",
            "Train Epoch [3/4], Batch [247800/260984], Loss: 2.3746\n",
            "Train Epoch [3/4], Batch [247900/260984], Loss: 2.2897\n",
            "Train Epoch [3/4], Batch [248000/260984], Loss: 2.2241\n",
            "Train Epoch [3/4], Batch [248100/260984], Loss: 2.2556\n",
            "Train Epoch [3/4], Batch [248200/260984], Loss: 2.3281\n",
            "Train Epoch [3/4], Batch [248300/260984], Loss: 2.3195\n",
            "Train Epoch [3/4], Batch [248400/260984], Loss: 2.2871\n",
            "Train Epoch [3/4], Batch [248500/260984], Loss: 2.3535\n",
            "Train Epoch [3/4], Batch [248600/260984], Loss: 2.3165\n",
            "Train Epoch [3/4], Batch [248700/260984], Loss: 2.2368\n",
            "Train Epoch [3/4], Batch [248800/260984], Loss: 2.2591\n",
            "Train Epoch [3/4], Batch [248900/260984], Loss: 2.1404\n",
            "Train Epoch [3/4], Batch [249000/260984], Loss: 2.3151\n",
            "Train Epoch [3/4], Batch [249100/260984], Loss: 2.3649\n",
            "Train Epoch [3/4], Batch [249200/260984], Loss: 2.3107\n",
            "Train Epoch [3/4], Batch [249300/260984], Loss: 2.3693\n",
            "Train Epoch [3/4], Batch [249400/260984], Loss: 2.3389\n",
            "Train Epoch [3/4], Batch [249500/260984], Loss: 2.2930\n",
            "Train Epoch [3/4], Batch [249600/260984], Loss: 2.3043\n",
            "Train Epoch [3/4], Batch [249700/260984], Loss: 2.2838\n",
            "Train Epoch [3/4], Batch [249800/260984], Loss: 2.2072\n",
            "Train Epoch [3/4], Batch [249900/260984], Loss: 2.4053\n",
            "Train Epoch [3/4], Batch [250000/260984], Loss: 2.4090\n",
            "Train Epoch [3/4], Batch [250100/260984], Loss: 2.4212\n",
            "Train Epoch [3/4], Batch [250200/260984], Loss: 2.4099\n",
            "Train Epoch [3/4], Batch [250300/260984], Loss: 2.4417\n",
            "Train Epoch [3/4], Batch [250400/260984], Loss: 2.4574\n",
            "Train Epoch [3/4], Batch [250500/260984], Loss: 2.1149\n",
            "Train Epoch [3/4], Batch [250600/260984], Loss: 2.2846\n",
            "Train Epoch [3/4], Batch [250700/260984], Loss: 2.3495\n",
            "Train Epoch [3/4], Batch [250800/260984], Loss: 2.2054\n",
            "Train Epoch [3/4], Batch [250900/260984], Loss: 2.2561\n",
            "Train Epoch [3/4], Batch [251000/260984], Loss: 2.3162\n",
            "Train Epoch [3/4], Batch [251100/260984], Loss: 2.3924\n",
            "Train Epoch [3/4], Batch [251200/260984], Loss: 2.3505\n",
            "Train Epoch [3/4], Batch [251300/260984], Loss: 2.3243\n",
            "Train Epoch [3/4], Batch [251400/260984], Loss: 2.4083\n",
            "Train Epoch [3/4], Batch [251500/260984], Loss: 2.4067\n",
            "Train Epoch [3/4], Batch [251600/260984], Loss: 2.3759\n",
            "Train Epoch [3/4], Batch [251700/260984], Loss: 2.2872\n",
            "Train Epoch [3/4], Batch [251800/260984], Loss: 2.2127\n",
            "Train Epoch [3/4], Batch [251900/260984], Loss: 2.3830\n",
            "Train Epoch [3/4], Batch [252000/260984], Loss: 2.2632\n",
            "Train Epoch [3/4], Batch [252100/260984], Loss: 2.3961\n",
            "Train Epoch [3/4], Batch [252200/260984], Loss: 2.3159\n",
            "Train Epoch [3/4], Batch [252300/260984], Loss: 2.3674\n",
            "Train Epoch [3/4], Batch [252400/260984], Loss: 2.2368\n",
            "Train Epoch [3/4], Batch [252500/260984], Loss: 2.3624\n",
            "Train Epoch [3/4], Batch [252600/260984], Loss: 2.3815\n",
            "Train Epoch [3/4], Batch [252700/260984], Loss: 2.2582\n",
            "Train Epoch [3/4], Batch [252800/260984], Loss: 2.4215\n",
            "Train Epoch [3/4], Batch [252900/260984], Loss: 2.4099\n",
            "Train Epoch [3/4], Batch [253000/260984], Loss: 2.2608\n",
            "Train Epoch [3/4], Batch [253100/260984], Loss: 2.3149\n",
            "Train Epoch [3/4], Batch [253200/260984], Loss: 2.1953\n",
            "Train Epoch [3/4], Batch [253300/260984], Loss: 2.2778\n",
            "Train Epoch [3/4], Batch [253400/260984], Loss: 2.3201\n",
            "Train Epoch [3/4], Batch [253500/260984], Loss: 2.3458\n",
            "Train Epoch [3/4], Batch [253600/260984], Loss: 2.3425\n",
            "Train Epoch [3/4], Batch [253700/260984], Loss: 2.3064\n",
            "Train Epoch [3/4], Batch [253800/260984], Loss: 2.4384\n",
            "Train Epoch [3/4], Batch [253900/260984], Loss: 2.2887\n",
            "Train Epoch [3/4], Batch [254000/260984], Loss: 2.2807\n",
            "Train Epoch [3/4], Batch [254100/260984], Loss: 2.3149\n",
            "Train Epoch [3/4], Batch [254200/260984], Loss: 2.3142\n",
            "Train Epoch [3/4], Batch [254300/260984], Loss: 2.4667\n",
            "Train Epoch [3/4], Batch [254400/260984], Loss: 2.0739\n",
            "Train Epoch [3/4], Batch [254500/260984], Loss: 2.2506\n",
            "Train Epoch [3/4], Batch [254600/260984], Loss: 2.3421\n",
            "Train Epoch [3/4], Batch [254700/260984], Loss: 2.3150\n",
            "Train Epoch [3/4], Batch [254800/260984], Loss: 2.2852\n",
            "Train Epoch [3/4], Batch [254900/260984], Loss: 2.3211\n",
            "Train Epoch [3/4], Batch [255000/260984], Loss: 2.2534\n",
            "Train Epoch [3/4], Batch [255100/260984], Loss: 2.2274\n",
            "Train Epoch [3/4], Batch [255200/260984], Loss: 2.3616\n",
            "Train Epoch [3/4], Batch [255300/260984], Loss: 2.2004\n",
            "Train Epoch [3/4], Batch [255400/260984], Loss: 2.2710\n",
            "Train Epoch [3/4], Batch [255500/260984], Loss: 2.3189\n",
            "Train Epoch [3/4], Batch [255600/260984], Loss: 2.2968\n",
            "Train Epoch [3/4], Batch [255700/260984], Loss: 2.2756\n",
            "Train Epoch [3/4], Batch [255800/260984], Loss: 2.1283\n",
            "Train Epoch [3/4], Batch [255900/260984], Loss: 2.2880\n",
            "Train Epoch [3/4], Batch [256000/260984], Loss: 2.3781\n",
            "Train Epoch [3/4], Batch [256100/260984], Loss: 2.1845\n",
            "Train Epoch [3/4], Batch [256200/260984], Loss: 2.3834\n",
            "Train Epoch [3/4], Batch [256300/260984], Loss: 2.4070\n",
            "Train Epoch [3/4], Batch [256400/260984], Loss: 2.3401\n",
            "Train Epoch [3/4], Batch [256500/260984], Loss: 2.1983\n",
            "Train Epoch [3/4], Batch [256600/260984], Loss: 2.2262\n",
            "Train Epoch [3/4], Batch [256700/260984], Loss: 2.2771\n",
            "Train Epoch [3/4], Batch [256800/260984], Loss: 2.1902\n",
            "Train Epoch [3/4], Batch [256900/260984], Loss: 2.2999\n",
            "Train Epoch [3/4], Batch [257000/260984], Loss: 2.2614\n",
            "Train Epoch [3/4], Batch [257100/260984], Loss: 2.3459\n",
            "Train Epoch [3/4], Batch [257200/260984], Loss: 2.3465\n",
            "Train Epoch [3/4], Batch [257300/260984], Loss: 2.2029\n",
            "Train Epoch [3/4], Batch [257400/260984], Loss: 2.4407\n",
            "Train Epoch [3/4], Batch [257500/260984], Loss: 2.3080\n",
            "Train Epoch [3/4], Batch [257600/260984], Loss: 2.3110\n",
            "Train Epoch [3/4], Batch [257700/260984], Loss: 2.2774\n",
            "Train Epoch [3/4], Batch [257800/260984], Loss: 2.4714\n",
            "Train Epoch [3/4], Batch [257900/260984], Loss: 2.1523\n",
            "Train Epoch [3/4], Batch [258000/260984], Loss: 2.3577\n",
            "Train Epoch [3/4], Batch [258100/260984], Loss: 2.1958\n",
            "Train Epoch [3/4], Batch [258200/260984], Loss: 2.2003\n",
            "Train Epoch [3/4], Batch [258300/260984], Loss: 2.4851\n",
            "Train Epoch [3/4], Batch [258400/260984], Loss: 2.4910\n",
            "Train Epoch [3/4], Batch [258500/260984], Loss: 2.4063\n",
            "Train Epoch [3/4], Batch [258600/260984], Loss: 2.2279\n",
            "Train Epoch [3/4], Batch [258700/260984], Loss: 2.3117\n",
            "Train Epoch [3/4], Batch [258800/260984], Loss: 2.2811\n",
            "Train Epoch [3/4], Batch [258900/260984], Loss: 2.2341\n",
            "Train Epoch [3/4], Batch [259000/260984], Loss: 2.2156\n",
            "Train Epoch [3/4], Batch [259100/260984], Loss: 2.3649\n",
            "Train Epoch [3/4], Batch [259200/260984], Loss: 2.2118\n",
            "Train Epoch [3/4], Batch [259300/260984], Loss: 2.2624\n",
            "Train Epoch [3/4], Batch [259400/260984], Loss: 2.3612\n",
            "Train Epoch [3/4], Batch [259500/260984], Loss: 2.4403\n",
            "Train Epoch [3/4], Batch [259600/260984], Loss: 2.2374\n",
            "Train Epoch [3/4], Batch [259700/260984], Loss: 2.3170\n",
            "Train Epoch [3/4], Batch [259800/260984], Loss: 2.2534\n",
            "Train Epoch [3/4], Batch [259900/260984], Loss: 2.2254\n",
            "Train Epoch [3/4], Batch [260000/260984], Loss: 2.4415\n",
            "Train Epoch [3/4], Batch [260100/260984], Loss: 2.1926\n",
            "Train Epoch [3/4], Batch [260200/260984], Loss: 2.4339\n",
            "Train Epoch [3/4], Batch [260300/260984], Loss: 2.3501\n",
            "Train Epoch [3/4], Batch [260400/260984], Loss: 2.2526\n",
            "Train Epoch [3/4], Batch [260500/260984], Loss: 2.2733\n",
            "Train Epoch [3/4], Batch [260600/260984], Loss: 2.2740\n",
            "Train Epoch [3/4], Batch [260700/260984], Loss: 2.3763\n",
            "Train Epoch [3/4], Batch [260800/260984], Loss: 2.3160\n",
            "Train Epoch [3/4], Batch [260900/260984], Loss: 2.1982\n",
            "Validation Epoch [3/4], Batch [0/13160], Loss: 2.4207\n",
            "Validation Epoch [3/4], Batch [50/13160], Loss: 2.5320\n",
            "Validation Epoch [3/4], Batch [100/13160], Loss: 2.5126\n",
            "Validation Epoch [3/4], Batch [150/13160], Loss: 2.6119\n",
            "Validation Epoch [3/4], Batch [200/13160], Loss: 2.4305\n",
            "Validation Epoch [3/4], Batch [250/13160], Loss: 2.5919\n",
            "Validation Epoch [3/4], Batch [300/13160], Loss: 2.5313\n",
            "Validation Epoch [3/4], Batch [350/13160], Loss: 2.4602\n",
            "Validation Epoch [3/4], Batch [400/13160], Loss: 2.5068\n",
            "Validation Epoch [3/4], Batch [450/13160], Loss: 2.6270\n",
            "Validation Epoch [3/4], Batch [500/13160], Loss: 2.4518\n",
            "Validation Epoch [3/4], Batch [550/13160], Loss: 2.6151\n",
            "Validation Epoch [3/4], Batch [600/13160], Loss: 2.4359\n",
            "Validation Epoch [3/4], Batch [650/13160], Loss: 2.4898\n",
            "Validation Epoch [3/4], Batch [700/13160], Loss: 2.4027\n",
            "Validation Epoch [3/4], Batch [750/13160], Loss: 2.2992\n",
            "Validation Epoch [3/4], Batch [800/13160], Loss: 2.5908\n",
            "Validation Epoch [3/4], Batch [850/13160], Loss: 2.4829\n",
            "Validation Epoch [3/4], Batch [900/13160], Loss: 2.4401\n",
            "Validation Epoch [3/4], Batch [950/13160], Loss: 2.3466\n",
            "Validation Epoch [3/4], Batch [1000/13160], Loss: 2.4332\n",
            "Validation Epoch [3/4], Batch [1050/13160], Loss: 2.5153\n",
            "Validation Epoch [3/4], Batch [1100/13160], Loss: 2.4821\n",
            "Validation Epoch [3/4], Batch [1150/13160], Loss: 2.4599\n",
            "Validation Epoch [3/4], Batch [1200/13160], Loss: 2.5350\n",
            "Validation Epoch [3/4], Batch [1250/13160], Loss: 2.4485\n",
            "Validation Epoch [3/4], Batch [1300/13160], Loss: 2.4804\n",
            "Validation Epoch [3/4], Batch [1350/13160], Loss: 2.5214\n",
            "Validation Epoch [3/4], Batch [1400/13160], Loss: 2.5770\n",
            "Validation Epoch [3/4], Batch [1450/13160], Loss: 2.5017\n",
            "Validation Epoch [3/4], Batch [1500/13160], Loss: 2.5250\n",
            "Validation Epoch [3/4], Batch [1550/13160], Loss: 2.5612\n",
            "Validation Epoch [3/4], Batch [1600/13160], Loss: 2.6128\n",
            "Validation Epoch [3/4], Batch [1650/13160], Loss: 2.4713\n",
            "Validation Epoch [3/4], Batch [1700/13160], Loss: 2.5008\n",
            "Validation Epoch [3/4], Batch [1750/13160], Loss: 2.5118\n",
            "Validation Epoch [3/4], Batch [1800/13160], Loss: 2.4517\n",
            "Validation Epoch [3/4], Batch [1850/13160], Loss: 2.5587\n",
            "Validation Epoch [3/4], Batch [1900/13160], Loss: 2.5774\n",
            "Validation Epoch [3/4], Batch [1950/13160], Loss: 2.4311\n",
            "Validation Epoch [3/4], Batch [2000/13160], Loss: 2.4711\n",
            "Validation Epoch [3/4], Batch [2050/13160], Loss: 2.5060\n",
            "Validation Epoch [3/4], Batch [2100/13160], Loss: 2.6036\n",
            "Validation Epoch [3/4], Batch [2150/13160], Loss: 2.5331\n",
            "Validation Epoch [3/4], Batch [2200/13160], Loss: 2.6074\n",
            "Validation Epoch [3/4], Batch [2250/13160], Loss: 2.4719\n",
            "Validation Epoch [3/4], Batch [2300/13160], Loss: 2.4076\n",
            "Validation Epoch [3/4], Batch [2350/13160], Loss: 2.5469\n",
            "Validation Epoch [3/4], Batch [2400/13160], Loss: 2.4398\n",
            "Validation Epoch [3/4], Batch [2450/13160], Loss: 2.4986\n",
            "Validation Epoch [3/4], Batch [2500/13160], Loss: 2.6345\n",
            "Validation Epoch [3/4], Batch [2550/13160], Loss: 2.5628\n",
            "Validation Epoch [3/4], Batch [2600/13160], Loss: 2.4779\n",
            "Validation Epoch [3/4], Batch [2650/13160], Loss: 2.4447\n",
            "Validation Epoch [3/4], Batch [2700/13160], Loss: 2.3981\n",
            "Validation Epoch [3/4], Batch [2750/13160], Loss: 2.4271\n",
            "Validation Epoch [3/4], Batch [2800/13160], Loss: 2.3481\n",
            "Validation Epoch [3/4], Batch [2850/13160], Loss: 2.4899\n",
            "Validation Epoch [3/4], Batch [2900/13160], Loss: 2.4823\n",
            "Validation Epoch [3/4], Batch [2950/13160], Loss: 2.4982\n",
            "Validation Epoch [3/4], Batch [3000/13160], Loss: 2.3858\n",
            "Validation Epoch [3/4], Batch [3050/13160], Loss: 2.4493\n",
            "Validation Epoch [3/4], Batch [3100/13160], Loss: 2.5010\n",
            "Validation Epoch [3/4], Batch [3150/13160], Loss: 2.5015\n",
            "Validation Epoch [3/4], Batch [3200/13160], Loss: 2.6596\n",
            "Validation Epoch [3/4], Batch [3250/13160], Loss: 2.5026\n",
            "Validation Epoch [3/4], Batch [3300/13160], Loss: 2.5003\n",
            "Validation Epoch [3/4], Batch [3350/13160], Loss: 2.5552\n",
            "Validation Epoch [3/4], Batch [3400/13160], Loss: 2.5043\n",
            "Validation Epoch [3/4], Batch [3450/13160], Loss: 2.3268\n",
            "Validation Epoch [3/4], Batch [3500/13160], Loss: 2.4440\n",
            "Validation Epoch [3/4], Batch [3550/13160], Loss: 2.5298\n",
            "Validation Epoch [3/4], Batch [3600/13160], Loss: 2.5225\n",
            "Validation Epoch [3/4], Batch [3650/13160], Loss: 2.4340\n",
            "Validation Epoch [3/4], Batch [3700/13160], Loss: 2.3424\n",
            "Validation Epoch [3/4], Batch [3750/13160], Loss: 2.4936\n",
            "Validation Epoch [3/4], Batch [3800/13160], Loss: 2.5205\n",
            "Validation Epoch [3/4], Batch [3850/13160], Loss: 2.4750\n",
            "Validation Epoch [3/4], Batch [3900/13160], Loss: 2.4132\n",
            "Validation Epoch [3/4], Batch [3950/13160], Loss: 2.3769\n",
            "Validation Epoch [3/4], Batch [4000/13160], Loss: 2.4135\n",
            "Validation Epoch [3/4], Batch [4050/13160], Loss: 2.2659\n",
            "Validation Epoch [3/4], Batch [4100/13160], Loss: 2.5698\n",
            "Validation Epoch [3/4], Batch [4150/13160], Loss: 2.5020\n",
            "Validation Epoch [3/4], Batch [4200/13160], Loss: 2.4525\n",
            "Validation Epoch [3/4], Batch [4250/13160], Loss: 2.4408\n",
            "Validation Epoch [3/4], Batch [4300/13160], Loss: 2.5355\n",
            "Validation Epoch [3/4], Batch [4350/13160], Loss: 2.5192\n",
            "Validation Epoch [3/4], Batch [4400/13160], Loss: 2.6367\n",
            "Validation Epoch [3/4], Batch [4450/13160], Loss: 2.4505\n",
            "Validation Epoch [3/4], Batch [4500/13160], Loss: 2.5057\n",
            "Validation Epoch [3/4], Batch [4550/13160], Loss: 2.5546\n",
            "Validation Epoch [3/4], Batch [4600/13160], Loss: 2.4415\n",
            "Validation Epoch [3/4], Batch [4650/13160], Loss: 2.3774\n",
            "Validation Epoch [3/4], Batch [4700/13160], Loss: 2.4237\n",
            "Validation Epoch [3/4], Batch [4750/13160], Loss: 2.4119\n",
            "Validation Epoch [3/4], Batch [4800/13160], Loss: 2.5628\n",
            "Validation Epoch [3/4], Batch [4850/13160], Loss: 2.3904\n",
            "Validation Epoch [3/4], Batch [4900/13160], Loss: 2.5901\n",
            "Validation Epoch [3/4], Batch [4950/13160], Loss: 2.5389\n",
            "Validation Epoch [3/4], Batch [5000/13160], Loss: 2.5455\n",
            "Validation Epoch [3/4], Batch [5050/13160], Loss: 2.6231\n",
            "Validation Epoch [3/4], Batch [5100/13160], Loss: 2.4081\n",
            "Validation Epoch [3/4], Batch [5150/13160], Loss: 2.4506\n",
            "Validation Epoch [3/4], Batch [5200/13160], Loss: 2.5315\n",
            "Validation Epoch [3/4], Batch [5250/13160], Loss: 2.5057\n",
            "Validation Epoch [3/4], Batch [5300/13160], Loss: 2.4428\n",
            "Validation Epoch [3/4], Batch [5350/13160], Loss: 2.4390\n",
            "Validation Epoch [3/4], Batch [5400/13160], Loss: 2.3663\n",
            "Validation Epoch [3/4], Batch [5450/13160], Loss: 2.5607\n",
            "Validation Epoch [3/4], Batch [5500/13160], Loss: 2.4641\n",
            "Validation Epoch [3/4], Batch [5550/13160], Loss: 2.4115\n",
            "Validation Epoch [3/4], Batch [5600/13160], Loss: 2.3903\n",
            "Validation Epoch [3/4], Batch [5650/13160], Loss: 2.4782\n",
            "Validation Epoch [3/4], Batch [5700/13160], Loss: 2.5019\n",
            "Validation Epoch [3/4], Batch [5750/13160], Loss: 2.3451\n",
            "Validation Epoch [3/4], Batch [5800/13160], Loss: 2.4518\n",
            "Validation Epoch [3/4], Batch [5850/13160], Loss: 2.4423\n",
            "Validation Epoch [3/4], Batch [5900/13160], Loss: 2.5103\n",
            "Validation Epoch [3/4], Batch [5950/13160], Loss: 2.5017\n",
            "Validation Epoch [3/4], Batch [6000/13160], Loss: 2.2257\n",
            "Validation Epoch [3/4], Batch [6050/13160], Loss: 2.4397\n",
            "Validation Epoch [3/4], Batch [6100/13160], Loss: 2.5749\n",
            "Validation Epoch [3/4], Batch [6150/13160], Loss: 2.3949\n",
            "Validation Epoch [3/4], Batch [6200/13160], Loss: 2.4399\n",
            "Validation Epoch [3/4], Batch [6250/13160], Loss: 2.5186\n",
            "Validation Epoch [3/4], Batch [6300/13160], Loss: 2.3128\n",
            "Validation Epoch [3/4], Batch [6350/13160], Loss: 2.5350\n",
            "Validation Epoch [3/4], Batch [6400/13160], Loss: 2.3381\n",
            "Validation Epoch [3/4], Batch [6450/13160], Loss: 2.4790\n",
            "Validation Epoch [3/4], Batch [6500/13160], Loss: 2.4807\n",
            "Validation Epoch [3/4], Batch [6550/13160], Loss: 2.4880\n",
            "Validation Epoch [3/4], Batch [6600/13160], Loss: 2.4389\n",
            "Validation Epoch [3/4], Batch [6650/13160], Loss: 2.3620\n",
            "Validation Epoch [3/4], Batch [6700/13160], Loss: 2.4932\n",
            "Validation Epoch [3/4], Batch [6750/13160], Loss: 2.4314\n",
            "Validation Epoch [3/4], Batch [6800/13160], Loss: 2.3195\n",
            "Validation Epoch [3/4], Batch [6850/13160], Loss: 2.5240\n",
            "Validation Epoch [3/4], Batch [6900/13160], Loss: 2.4612\n",
            "Validation Epoch [3/4], Batch [6950/13160], Loss: 2.5338\n",
            "Validation Epoch [3/4], Batch [7000/13160], Loss: 2.4885\n",
            "Validation Epoch [3/4], Batch [7050/13160], Loss: 2.4286\n",
            "Validation Epoch [3/4], Batch [7100/13160], Loss: 2.7121\n",
            "Validation Epoch [3/4], Batch [7150/13160], Loss: 2.6084\n",
            "Validation Epoch [3/4], Batch [7200/13160], Loss: 2.4930\n",
            "Validation Epoch [3/4], Batch [7250/13160], Loss: 2.4544\n",
            "Validation Epoch [3/4], Batch [7300/13160], Loss: 2.3544\n",
            "Validation Epoch [3/4], Batch [7350/13160], Loss: 2.5624\n",
            "Validation Epoch [3/4], Batch [7400/13160], Loss: 2.5289\n",
            "Validation Epoch [3/4], Batch [7450/13160], Loss: 2.4260\n",
            "Validation Epoch [3/4], Batch [7500/13160], Loss: 2.4098\n",
            "Validation Epoch [3/4], Batch [7550/13160], Loss: 2.5235\n",
            "Validation Epoch [3/4], Batch [7600/13160], Loss: 2.5929\n",
            "Validation Epoch [3/4], Batch [7650/13160], Loss: 2.4916\n",
            "Validation Epoch [3/4], Batch [7700/13160], Loss: 2.4293\n",
            "Validation Epoch [3/4], Batch [7750/13160], Loss: 2.6155\n",
            "Validation Epoch [3/4], Batch [7800/13160], Loss: 2.4726\n",
            "Validation Epoch [3/4], Batch [7850/13160], Loss: 2.5731\n",
            "Validation Epoch [3/4], Batch [7900/13160], Loss: 2.4790\n",
            "Validation Epoch [3/4], Batch [7950/13160], Loss: 2.3019\n",
            "Validation Epoch [3/4], Batch [8000/13160], Loss: 2.4490\n",
            "Validation Epoch [3/4], Batch [8050/13160], Loss: 2.4323\n",
            "Validation Epoch [3/4], Batch [8100/13160], Loss: 2.4763\n",
            "Validation Epoch [3/4], Batch [8150/13160], Loss: 2.4780\n",
            "Validation Epoch [3/4], Batch [8200/13160], Loss: 2.5877\n",
            "Validation Epoch [3/4], Batch [8250/13160], Loss: 2.4423\n",
            "Validation Epoch [3/4], Batch [8300/13160], Loss: 2.4639\n",
            "Validation Epoch [3/4], Batch [8350/13160], Loss: 2.3476\n",
            "Validation Epoch [3/4], Batch [8400/13160], Loss: 2.5297\n",
            "Validation Epoch [3/4], Batch [8450/13160], Loss: 2.5886\n",
            "Validation Epoch [3/4], Batch [8500/13160], Loss: 2.4121\n",
            "Validation Epoch [3/4], Batch [8550/13160], Loss: 2.5024\n",
            "Validation Epoch [3/4], Batch [8600/13160], Loss: 2.4769\n",
            "Validation Epoch [3/4], Batch [8650/13160], Loss: 2.2974\n",
            "Validation Epoch [3/4], Batch [8700/13160], Loss: 2.4991\n",
            "Validation Epoch [3/4], Batch [8750/13160], Loss: 2.5260\n",
            "Validation Epoch [3/4], Batch [8800/13160], Loss: 2.4400\n",
            "Validation Epoch [3/4], Batch [8850/13160], Loss: 2.3692\n",
            "Validation Epoch [3/4], Batch [8900/13160], Loss: 2.2452\n",
            "Validation Epoch [3/4], Batch [8950/13160], Loss: 2.6280\n",
            "Validation Epoch [3/4], Batch [9000/13160], Loss: 2.5757\n",
            "Validation Epoch [3/4], Batch [9050/13160], Loss: 2.4618\n",
            "Validation Epoch [3/4], Batch [9100/13160], Loss: 2.3941\n",
            "Validation Epoch [3/4], Batch [9150/13160], Loss: 2.4499\n",
            "Validation Epoch [3/4], Batch [9200/13160], Loss: 2.5061\n",
            "Validation Epoch [3/4], Batch [9250/13160], Loss: 2.5459\n",
            "Validation Epoch [3/4], Batch [9300/13160], Loss: 2.4254\n",
            "Validation Epoch [3/4], Batch [9350/13160], Loss: 2.4239\n",
            "Validation Epoch [3/4], Batch [9400/13160], Loss: 2.4808\n",
            "Validation Epoch [3/4], Batch [9450/13160], Loss: 2.5424\n",
            "Validation Epoch [3/4], Batch [9500/13160], Loss: 2.5208\n",
            "Validation Epoch [3/4], Batch [9550/13160], Loss: 2.3912\n",
            "Validation Epoch [3/4], Batch [9600/13160], Loss: 2.4482\n",
            "Validation Epoch [3/4], Batch [9650/13160], Loss: 2.3962\n",
            "Validation Epoch [3/4], Batch [9700/13160], Loss: 2.5144\n",
            "Validation Epoch [3/4], Batch [9750/13160], Loss: 2.3188\n",
            "Validation Epoch [3/4], Batch [9800/13160], Loss: 2.5301\n",
            "Validation Epoch [3/4], Batch [9850/13160], Loss: 2.4485\n",
            "Validation Epoch [3/4], Batch [9900/13160], Loss: 2.4558\n",
            "Validation Epoch [3/4], Batch [9950/13160], Loss: 2.5871\n",
            "Validation Epoch [3/4], Batch [10000/13160], Loss: 2.6232\n",
            "Validation Epoch [3/4], Batch [10050/13160], Loss: 2.5816\n",
            "Validation Epoch [3/4], Batch [10100/13160], Loss: 2.4537\n",
            "Validation Epoch [3/4], Batch [10150/13160], Loss: 2.5261\n",
            "Validation Epoch [3/4], Batch [10200/13160], Loss: 2.5534\n",
            "Validation Epoch [3/4], Batch [10250/13160], Loss: 2.4502\n",
            "Validation Epoch [3/4], Batch [10300/13160], Loss: 2.2758\n",
            "Validation Epoch [3/4], Batch [10350/13160], Loss: 2.3712\n",
            "Validation Epoch [3/4], Batch [10400/13160], Loss: 2.5827\n",
            "Validation Epoch [3/4], Batch [10450/13160], Loss: 2.4998\n",
            "Validation Epoch [3/4], Batch [10500/13160], Loss: 2.4487\n",
            "Validation Epoch [3/4], Batch [10550/13160], Loss: 2.5416\n",
            "Validation Epoch [3/4], Batch [10600/13160], Loss: 2.4389\n",
            "Validation Epoch [3/4], Batch [10650/13160], Loss: 2.4190\n",
            "Validation Epoch [3/4], Batch [10700/13160], Loss: 2.2700\n",
            "Validation Epoch [3/4], Batch [10750/13160], Loss: 2.5165\n",
            "Validation Epoch [3/4], Batch [10800/13160], Loss: 2.3794\n",
            "Validation Epoch [3/4], Batch [10850/13160], Loss: 2.4446\n",
            "Validation Epoch [3/4], Batch [10900/13160], Loss: 2.4517\n",
            "Validation Epoch [3/4], Batch [10950/13160], Loss: 2.4683\n",
            "Validation Epoch [3/4], Batch [11000/13160], Loss: 2.5015\n",
            "Validation Epoch [3/4], Batch [11050/13160], Loss: 2.4049\n",
            "Validation Epoch [3/4], Batch [11100/13160], Loss: 2.3355\n",
            "Validation Epoch [3/4], Batch [11150/13160], Loss: 2.4824\n",
            "Validation Epoch [3/4], Batch [11200/13160], Loss: 2.4774\n",
            "Validation Epoch [3/4], Batch [11250/13160], Loss: 2.3708\n",
            "Validation Epoch [3/4], Batch [11300/13160], Loss: 2.5640\n",
            "Validation Epoch [3/4], Batch [11350/13160], Loss: 2.4785\n",
            "Validation Epoch [3/4], Batch [11400/13160], Loss: 2.5286\n",
            "Validation Epoch [3/4], Batch [11450/13160], Loss: 2.2850\n",
            "Validation Epoch [3/4], Batch [11500/13160], Loss: 2.5123\n",
            "Validation Epoch [3/4], Batch [11550/13160], Loss: 2.5284\n",
            "Validation Epoch [3/4], Batch [11600/13160], Loss: 2.5719\n",
            "Validation Epoch [3/4], Batch [11650/13160], Loss: 2.4615\n",
            "Validation Epoch [3/4], Batch [11700/13160], Loss: 2.4967\n",
            "Validation Epoch [3/4], Batch [11750/13160], Loss: 2.5160\n",
            "Validation Epoch [3/4], Batch [11800/13160], Loss: 2.5233\n",
            "Validation Epoch [3/4], Batch [11850/13160], Loss: 2.4150\n",
            "Validation Epoch [3/4], Batch [11900/13160], Loss: 2.3700\n",
            "Validation Epoch [3/4], Batch [11950/13160], Loss: 2.5366\n",
            "Validation Epoch [3/4], Batch [12000/13160], Loss: 2.3955\n",
            "Validation Epoch [3/4], Batch [12050/13160], Loss: 2.5907\n",
            "Validation Epoch [3/4], Batch [12100/13160], Loss: 2.4844\n",
            "Validation Epoch [3/4], Batch [12150/13160], Loss: 2.4518\n",
            "Validation Epoch [3/4], Batch [12200/13160], Loss: 2.3982\n",
            "Validation Epoch [3/4], Batch [12250/13160], Loss: 2.3376\n",
            "Validation Epoch [3/4], Batch [12300/13160], Loss: 2.5021\n",
            "Validation Epoch [3/4], Batch [12350/13160], Loss: 2.4147\n",
            "Validation Epoch [3/4], Batch [12400/13160], Loss: 2.3770\n",
            "Validation Epoch [3/4], Batch [12450/13160], Loss: 2.4114\n",
            "Validation Epoch [3/4], Batch [12500/13160], Loss: 2.4807\n",
            "Validation Epoch [3/4], Batch [12550/13160], Loss: 2.5575\n",
            "Validation Epoch [3/4], Batch [12600/13160], Loss: 2.4707\n",
            "Validation Epoch [3/4], Batch [12650/13160], Loss: 2.5172\n",
            "Validation Epoch [3/4], Batch [12700/13160], Loss: 2.5731\n",
            "Validation Epoch [3/4], Batch [12750/13160], Loss: 2.4969\n",
            "Validation Epoch [3/4], Batch [12800/13160], Loss: 2.4248\n",
            "Validation Epoch [3/4], Batch [12850/13160], Loss: 2.5948\n",
            "Validation Epoch [3/4], Batch [12900/13160], Loss: 2.2764\n",
            "Validation Epoch [3/4], Batch [12950/13160], Loss: 2.4380\n",
            "Validation Epoch [3/4], Batch [13000/13160], Loss: 2.5218\n",
            "Validation Epoch [3/4], Batch [13050/13160], Loss: 2.4769\n",
            "Validation Epoch [3/4], Batch [13100/13160], Loss: 2.3990\n",
            "Validation Epoch [3/4], Batch [13150/13160], Loss: 2.5569\n",
            "Epoch [3/4], Train Loss: 2.3057, CV Loss: 2.4793\n",
            "Train Epoch [4/4], Batch [0/260984], Loss: 2.2319\n",
            "Train Epoch [4/4], Batch [100/260984], Loss: 2.4410\n",
            "Train Epoch [4/4], Batch [200/260984], Loss: 2.3208\n",
            "Train Epoch [4/4], Batch [300/260984], Loss: 2.4457\n",
            "Train Epoch [4/4], Batch [400/260984], Loss: 2.3093\n",
            "Train Epoch [4/4], Batch [500/260984], Loss: 2.3906\n",
            "Train Epoch [4/4], Batch [600/260984], Loss: 2.2531\n",
            "Train Epoch [4/4], Batch [700/260984], Loss: 2.4024\n",
            "Train Epoch [4/4], Batch [800/260984], Loss: 2.3737\n",
            "Train Epoch [4/4], Batch [900/260984], Loss: 2.4700\n",
            "Train Epoch [4/4], Batch [1000/260984], Loss: 2.2623\n",
            "Train Epoch [4/4], Batch [1100/260984], Loss: 2.3395\n",
            "Train Epoch [4/4], Batch [1200/260984], Loss: 2.2208\n",
            "Train Epoch [4/4], Batch [1300/260984], Loss: 2.1665\n",
            "Train Epoch [4/4], Batch [1400/260984], Loss: 2.3287\n",
            "Train Epoch [4/4], Batch [1500/260984], Loss: 2.3048\n",
            "Train Epoch [4/4], Batch [1600/260984], Loss: 2.1042\n",
            "Train Epoch [4/4], Batch [1700/260984], Loss: 2.3175\n",
            "Train Epoch [4/4], Batch [1800/260984], Loss: 2.2887\n",
            "Train Epoch [4/4], Batch [1900/260984], Loss: 2.3812\n",
            "Train Epoch [4/4], Batch [2000/260984], Loss: 2.3217\n",
            "Train Epoch [4/4], Batch [2100/260984], Loss: 2.3067\n",
            "Train Epoch [4/4], Batch [2200/260984], Loss: 2.4372\n",
            "Train Epoch [4/4], Batch [2300/260984], Loss: 2.2552\n",
            "Train Epoch [4/4], Batch [2400/260984], Loss: 2.3161\n",
            "Train Epoch [4/4], Batch [2500/260984], Loss: 2.2670\n",
            "Train Epoch [4/4], Batch [2600/260984], Loss: 2.1634\n",
            "Train Epoch [4/4], Batch [2700/260984], Loss: 2.3190\n",
            "Train Epoch [4/4], Batch [2800/260984], Loss: 2.3521\n",
            "Train Epoch [4/4], Batch [2900/260984], Loss: 2.3149\n",
            "Train Epoch [4/4], Batch [3000/260984], Loss: 2.3624\n",
            "Train Epoch [4/4], Batch [3100/260984], Loss: 2.2146\n",
            "Train Epoch [4/4], Batch [3200/260984], Loss: 2.3130\n",
            "Train Epoch [4/4], Batch [3300/260984], Loss: 2.2401\n",
            "Train Epoch [4/4], Batch [3400/260984], Loss: 2.2877\n",
            "Train Epoch [4/4], Batch [3500/260984], Loss: 2.1633\n",
            "Train Epoch [4/4], Batch [3600/260984], Loss: 2.3179\n",
            "Train Epoch [4/4], Batch [3700/260984], Loss: 2.3191\n",
            "Train Epoch [4/4], Batch [3800/260984], Loss: 2.3437\n",
            "Train Epoch [4/4], Batch [3900/260984], Loss: 2.2849\n",
            "Train Epoch [4/4], Batch [4000/260984], Loss: 2.2691\n",
            "Train Epoch [4/4], Batch [4100/260984], Loss: 2.1726\n",
            "Train Epoch [4/4], Batch [4200/260984], Loss: 2.2439\n",
            "Train Epoch [4/4], Batch [4300/260984], Loss: 2.3150\n",
            "Train Epoch [4/4], Batch [4400/260984], Loss: 2.2217\n",
            "Train Epoch [4/4], Batch [4500/260984], Loss: 2.2763\n",
            "Train Epoch [4/4], Batch [4600/260984], Loss: 2.1290\n",
            "Train Epoch [4/4], Batch [4700/260984], Loss: 2.3470\n",
            "Train Epoch [4/4], Batch [4800/260984], Loss: 2.2826\n",
            "Train Epoch [4/4], Batch [4900/260984], Loss: 2.3498\n",
            "Train Epoch [4/4], Batch [5000/260984], Loss: 2.4400\n",
            "Train Epoch [4/4], Batch [5100/260984], Loss: 2.3143\n",
            "Train Epoch [4/4], Batch [5200/260984], Loss: 2.3816\n",
            "Train Epoch [4/4], Batch [5300/260984], Loss: 2.3415\n",
            "Train Epoch [4/4], Batch [5400/260984], Loss: 2.1747\n",
            "Train Epoch [4/4], Batch [5500/260984], Loss: 2.3776\n",
            "Train Epoch [4/4], Batch [5600/260984], Loss: 2.3221\n",
            "Train Epoch [4/4], Batch [5700/260984], Loss: 2.2878\n",
            "Train Epoch [4/4], Batch [5800/260984], Loss: 2.1251\n",
            "Train Epoch [4/4], Batch [5900/260984], Loss: 2.3604\n",
            "Train Epoch [4/4], Batch [6000/260984], Loss: 2.3995\n",
            "Train Epoch [4/4], Batch [6100/260984], Loss: 2.1908\n",
            "Train Epoch [4/4], Batch [6200/260984], Loss: 2.2102\n",
            "Train Epoch [4/4], Batch [6300/260984], Loss: 2.3560\n",
            "Train Epoch [4/4], Batch [6400/260984], Loss: 2.2221\n",
            "Train Epoch [4/4], Batch [6500/260984], Loss: 2.3753\n",
            "Train Epoch [4/4], Batch [6600/260984], Loss: 2.1732\n",
            "Train Epoch [4/4], Batch [6700/260984], Loss: 2.2777\n",
            "Train Epoch [4/4], Batch [6800/260984], Loss: 2.3245\n",
            "Train Epoch [4/4], Batch [6900/260984], Loss: 2.3181\n",
            "Train Epoch [4/4], Batch [7000/260984], Loss: 2.4110\n",
            "Train Epoch [4/4], Batch [7100/260984], Loss: 2.3738\n",
            "Train Epoch [4/4], Batch [7200/260984], Loss: 2.2488\n",
            "Train Epoch [4/4], Batch [7300/260984], Loss: 2.2075\n",
            "Train Epoch [4/4], Batch [7400/260984], Loss: 2.2832\n",
            "Train Epoch [4/4], Batch [7500/260984], Loss: 2.3131\n",
            "Train Epoch [4/4], Batch [7600/260984], Loss: 2.2587\n",
            "Train Epoch [4/4], Batch [7700/260984], Loss: 2.2334\n",
            "Train Epoch [4/4], Batch [7800/260984], Loss: 2.2475\n",
            "Train Epoch [4/4], Batch [7900/260984], Loss: 2.2994\n",
            "Train Epoch [4/4], Batch [8000/260984], Loss: 2.2458\n",
            "Train Epoch [4/4], Batch [8100/260984], Loss: 2.3626\n",
            "Train Epoch [4/4], Batch [8200/260984], Loss: 2.3194\n",
            "Train Epoch [4/4], Batch [8300/260984], Loss: 2.2304\n",
            "Train Epoch [4/4], Batch [8400/260984], Loss: 2.3089\n",
            "Train Epoch [4/4], Batch [8500/260984], Loss: 2.3754\n",
            "Train Epoch [4/4], Batch [8600/260984], Loss: 2.2886\n",
            "Train Epoch [4/4], Batch [8700/260984], Loss: 2.1904\n",
            "Train Epoch [4/4], Batch [8800/260984], Loss: 2.4715\n",
            "Train Epoch [4/4], Batch [8900/260984], Loss: 2.4078\n",
            "Train Epoch [4/4], Batch [9000/260984], Loss: 2.2653\n",
            "Train Epoch [4/4], Batch [9100/260984], Loss: 2.3388\n",
            "Train Epoch [4/4], Batch [9200/260984], Loss: 2.1478\n",
            "Train Epoch [4/4], Batch [9300/260984], Loss: 2.2706\n",
            "Train Epoch [4/4], Batch [9400/260984], Loss: 2.3196\n",
            "Train Epoch [4/4], Batch [9500/260984], Loss: 2.2147\n",
            "Train Epoch [4/4], Batch [9600/260984], Loss: 2.3342\n",
            "Train Epoch [4/4], Batch [9700/260984], Loss: 2.2519\n",
            "Train Epoch [4/4], Batch [9800/260984], Loss: 2.3485\n",
            "Train Epoch [4/4], Batch [9900/260984], Loss: 2.3788\n",
            "Train Epoch [4/4], Batch [10000/260984], Loss: 2.4535\n",
            "Train Epoch [4/4], Batch [10100/260984], Loss: 2.2588\n",
            "Train Epoch [4/4], Batch [10200/260984], Loss: 2.3158\n",
            "Train Epoch [4/4], Batch [10300/260984], Loss: 2.2857\n",
            "Train Epoch [4/4], Batch [10400/260984], Loss: 2.3222\n",
            "Train Epoch [4/4], Batch [10500/260984], Loss: 2.2185\n",
            "Train Epoch [4/4], Batch [10600/260984], Loss: 2.3882\n",
            "Train Epoch [4/4], Batch [10700/260984], Loss: 2.4815\n",
            "Train Epoch [4/4], Batch [10800/260984], Loss: 2.3129\n",
            "Train Epoch [4/4], Batch [10900/260984], Loss: 2.1571\n",
            "Train Epoch [4/4], Batch [11000/260984], Loss: 2.3550\n",
            "Train Epoch [4/4], Batch [11100/260984], Loss: 2.3158\n",
            "Train Epoch [4/4], Batch [11200/260984], Loss: 2.3131\n",
            "Train Epoch [4/4], Batch [11300/260984], Loss: 2.2790\n",
            "Train Epoch [4/4], Batch [11400/260984], Loss: 2.1959\n",
            "Train Epoch [4/4], Batch [11500/260984], Loss: 2.1904\n",
            "Train Epoch [4/4], Batch [11600/260984], Loss: 2.1670\n",
            "Train Epoch [4/4], Batch [11700/260984], Loss: 2.3501\n",
            "Train Epoch [4/4], Batch [11800/260984], Loss: 2.4095\n",
            "Train Epoch [4/4], Batch [11900/260984], Loss: 2.3218\n",
            "Train Epoch [4/4], Batch [12000/260984], Loss: 2.3510\n",
            "Train Epoch [4/4], Batch [12100/260984], Loss: 2.3617\n",
            "Train Epoch [4/4], Batch [12200/260984], Loss: 2.3288\n",
            "Train Epoch [4/4], Batch [12300/260984], Loss: 2.3824\n",
            "Train Epoch [4/4], Batch [12400/260984], Loss: 2.2729\n",
            "Train Epoch [4/4], Batch [12500/260984], Loss: 2.1943\n",
            "Train Epoch [4/4], Batch [12600/260984], Loss: 2.3141\n",
            "Train Epoch [4/4], Batch [12700/260984], Loss: 2.2321\n",
            "Train Epoch [4/4], Batch [12800/260984], Loss: 2.2662\n",
            "Train Epoch [4/4], Batch [12900/260984], Loss: 2.3114\n",
            "Train Epoch [4/4], Batch [13000/260984], Loss: 2.2723\n",
            "Train Epoch [4/4], Batch [13100/260984], Loss: 2.1911\n",
            "Train Epoch [4/4], Batch [13200/260984], Loss: 2.3107\n",
            "Train Epoch [4/4], Batch [13300/260984], Loss: 2.2869\n",
            "Train Epoch [4/4], Batch [13400/260984], Loss: 2.2819\n",
            "Train Epoch [4/4], Batch [13500/260984], Loss: 2.2841\n",
            "Train Epoch [4/4], Batch [13600/260984], Loss: 2.2076\n",
            "Train Epoch [4/4], Batch [13700/260984], Loss: 2.3011\n",
            "Train Epoch [4/4], Batch [13800/260984], Loss: 2.2767\n",
            "Train Epoch [4/4], Batch [13900/260984], Loss: 2.3905\n",
            "Train Epoch [4/4], Batch [14000/260984], Loss: 2.2476\n",
            "Train Epoch [4/4], Batch [14100/260984], Loss: 2.0732\n",
            "Train Epoch [4/4], Batch [14200/260984], Loss: 2.2784\n",
            "Train Epoch [4/4], Batch [14300/260984], Loss: 2.3698\n",
            "Train Epoch [4/4], Batch [14400/260984], Loss: 2.1574\n",
            "Train Epoch [4/4], Batch [14500/260984], Loss: 2.4017\n",
            "Train Epoch [4/4], Batch [14600/260984], Loss: 2.3112\n",
            "Train Epoch [4/4], Batch [14700/260984], Loss: 2.3518\n",
            "Train Epoch [4/4], Batch [14800/260984], Loss: 2.2303\n",
            "Train Epoch [4/4], Batch [14900/260984], Loss: 2.3350\n",
            "Train Epoch [4/4], Batch [15000/260984], Loss: 2.2808\n",
            "Train Epoch [4/4], Batch [15100/260984], Loss: 2.3194\n",
            "Train Epoch [4/4], Batch [15200/260984], Loss: 2.1898\n",
            "Train Epoch [4/4], Batch [15300/260984], Loss: 2.3351\n",
            "Train Epoch [4/4], Batch [15400/260984], Loss: 2.2224\n",
            "Train Epoch [4/4], Batch [15500/260984], Loss: 2.3154\n",
            "Train Epoch [4/4], Batch [15600/260984], Loss: 2.2282\n",
            "Train Epoch [4/4], Batch [15700/260984], Loss: 2.2608\n",
            "Train Epoch [4/4], Batch [15800/260984], Loss: 2.3787\n",
            "Train Epoch [4/4], Batch [15900/260984], Loss: 2.3372\n",
            "Train Epoch [4/4], Batch [16000/260984], Loss: 2.2042\n",
            "Train Epoch [4/4], Batch [16100/260984], Loss: 2.1595\n",
            "Train Epoch [4/4], Batch [16200/260984], Loss: 2.3520\n",
            "Train Epoch [4/4], Batch [16300/260984], Loss: 2.3538\n",
            "Train Epoch [4/4], Batch [16400/260984], Loss: 2.0846\n",
            "Train Epoch [4/4], Batch [16500/260984], Loss: 2.2849\n",
            "Train Epoch [4/4], Batch [16600/260984], Loss: 2.4818\n",
            "Train Epoch [4/4], Batch [16700/260984], Loss: 2.3516\n",
            "Train Epoch [4/4], Batch [16800/260984], Loss: 2.2985\n",
            "Train Epoch [4/4], Batch [16900/260984], Loss: 2.2723\n",
            "Train Epoch [4/4], Batch [17000/260984], Loss: 2.1233\n",
            "Train Epoch [4/4], Batch [17100/260984], Loss: 2.2398\n",
            "Train Epoch [4/4], Batch [17200/260984], Loss: 2.2793\n",
            "Train Epoch [4/4], Batch [17300/260984], Loss: 2.4244\n",
            "Train Epoch [4/4], Batch [17400/260984], Loss: 2.3417\n",
            "Train Epoch [4/4], Batch [17500/260984], Loss: 2.1347\n",
            "Train Epoch [4/4], Batch [17600/260984], Loss: 2.2280\n",
            "Train Epoch [4/4], Batch [17700/260984], Loss: 2.2860\n",
            "Train Epoch [4/4], Batch [17800/260984], Loss: 2.2074\n",
            "Train Epoch [4/4], Batch [17900/260984], Loss: 2.2497\n",
            "Train Epoch [4/4], Batch [18000/260984], Loss: 2.2282\n",
            "Train Epoch [4/4], Batch [18100/260984], Loss: 2.2878\n",
            "Train Epoch [4/4], Batch [18200/260984], Loss: 2.2844\n",
            "Train Epoch [4/4], Batch [18300/260984], Loss: 2.2120\n",
            "Train Epoch [4/4], Batch [18400/260984], Loss: 2.3776\n",
            "Train Epoch [4/4], Batch [18500/260984], Loss: 2.2359\n",
            "Train Epoch [4/4], Batch [18600/260984], Loss: 2.2938\n",
            "Train Epoch [4/4], Batch [18700/260984], Loss: 2.2791\n",
            "Train Epoch [4/4], Batch [18800/260984], Loss: 2.4381\n",
            "Train Epoch [4/4], Batch [18900/260984], Loss: 2.2327\n",
            "Train Epoch [4/4], Batch [19000/260984], Loss: 2.3338\n",
            "Train Epoch [4/4], Batch [19100/260984], Loss: 2.3282\n",
            "Train Epoch [4/4], Batch [19200/260984], Loss: 2.2774\n",
            "Train Epoch [4/4], Batch [19300/260984], Loss: 2.3602\n",
            "Train Epoch [4/4], Batch [19400/260984], Loss: 2.4302\n",
            "Train Epoch [4/4], Batch [19500/260984], Loss: 2.2611\n",
            "Train Epoch [4/4], Batch [19600/260984], Loss: 2.2272\n",
            "Train Epoch [4/4], Batch [19700/260984], Loss: 2.2856\n",
            "Train Epoch [4/4], Batch [19800/260984], Loss: 2.2566\n",
            "Train Epoch [4/4], Batch [19900/260984], Loss: 2.2617\n",
            "Train Epoch [4/4], Batch [20000/260984], Loss: 2.4140\n",
            "Train Epoch [4/4], Batch [20100/260984], Loss: 2.4881\n",
            "Train Epoch [4/4], Batch [20200/260984], Loss: 2.3476\n",
            "Train Epoch [4/4], Batch [20300/260984], Loss: 2.3340\n",
            "Train Epoch [4/4], Batch [20400/260984], Loss: 2.3025\n",
            "Train Epoch [4/4], Batch [20500/260984], Loss: 2.4065\n",
            "Train Epoch [4/4], Batch [20600/260984], Loss: 2.2565\n",
            "Train Epoch [4/4], Batch [20700/260984], Loss: 2.2425\n",
            "Train Epoch [4/4], Batch [20800/260984], Loss: 2.1884\n",
            "Train Epoch [4/4], Batch [20900/260984], Loss: 2.2744\n",
            "Train Epoch [4/4], Batch [21000/260984], Loss: 2.3634\n",
            "Train Epoch [4/4], Batch [21100/260984], Loss: 2.3560\n",
            "Train Epoch [4/4], Batch [21200/260984], Loss: 2.4441\n",
            "Train Epoch [4/4], Batch [21300/260984], Loss: 2.4147\n",
            "Train Epoch [4/4], Batch [21400/260984], Loss: 2.3583\n",
            "Train Epoch [4/4], Batch [21500/260984], Loss: 2.1991\n",
            "Train Epoch [4/4], Batch [21600/260984], Loss: 2.1040\n",
            "Train Epoch [4/4], Batch [21700/260984], Loss: 2.1419\n",
            "Train Epoch [4/4], Batch [21800/260984], Loss: 2.2849\n",
            "Train Epoch [4/4], Batch [21900/260984], Loss: 2.3402\n",
            "Train Epoch [4/4], Batch [22000/260984], Loss: 2.2407\n",
            "Train Epoch [4/4], Batch [22100/260984], Loss: 2.2744\n",
            "Train Epoch [4/4], Batch [22200/260984], Loss: 2.3429\n",
            "Train Epoch [4/4], Batch [22300/260984], Loss: 2.4093\n",
            "Train Epoch [4/4], Batch [22400/260984], Loss: 2.3850\n",
            "Train Epoch [4/4], Batch [22500/260984], Loss: 2.2979\n",
            "Train Epoch [4/4], Batch [22600/260984], Loss: 2.3834\n",
            "Train Epoch [4/4], Batch [22700/260984], Loss: 2.2377\n",
            "Train Epoch [4/4], Batch [22800/260984], Loss: 2.3451\n",
            "Train Epoch [4/4], Batch [22900/260984], Loss: 2.2539\n",
            "Train Epoch [4/4], Batch [23000/260984], Loss: 2.0962\n",
            "Train Epoch [4/4], Batch [23100/260984], Loss: 2.2591\n",
            "Train Epoch [4/4], Batch [23200/260984], Loss: 2.3557\n",
            "Train Epoch [4/4], Batch [23300/260984], Loss: 2.1675\n",
            "Train Epoch [4/4], Batch [23400/260984], Loss: 2.4176\n",
            "Train Epoch [4/4], Batch [23500/260984], Loss: 2.2334\n",
            "Train Epoch [4/4], Batch [23600/260984], Loss: 2.2893\n",
            "Train Epoch [4/4], Batch [23700/260984], Loss: 2.2815\n",
            "Train Epoch [4/4], Batch [23800/260984], Loss: 2.1518\n",
            "Train Epoch [4/4], Batch [23900/260984], Loss: 2.2882\n",
            "Train Epoch [4/4], Batch [24000/260984], Loss: 2.5548\n",
            "Train Epoch [4/4], Batch [24100/260984], Loss: 2.3830\n",
            "Train Epoch [4/4], Batch [24200/260984], Loss: 2.3437\n",
            "Train Epoch [4/4], Batch [24300/260984], Loss: 2.3844\n",
            "Train Epoch [4/4], Batch [24400/260984], Loss: 2.3148\n",
            "Train Epoch [4/4], Batch [24500/260984], Loss: 2.4043\n",
            "Train Epoch [4/4], Batch [24600/260984], Loss: 2.4823\n",
            "Train Epoch [4/4], Batch [24700/260984], Loss: 2.2439\n",
            "Train Epoch [4/4], Batch [24800/260984], Loss: 2.3728\n",
            "Train Epoch [4/4], Batch [24900/260984], Loss: 2.4394\n",
            "Train Epoch [4/4], Batch [25000/260984], Loss: 2.3808\n",
            "Train Epoch [4/4], Batch [25100/260984], Loss: 2.2418\n",
            "Train Epoch [4/4], Batch [25200/260984], Loss: 2.3265\n",
            "Train Epoch [4/4], Batch [25300/260984], Loss: 2.1584\n",
            "Train Epoch [4/4], Batch [25400/260984], Loss: 2.2303\n",
            "Train Epoch [4/4], Batch [25500/260984], Loss: 2.3633\n",
            "Train Epoch [4/4], Batch [25600/260984], Loss: 2.3763\n",
            "Train Epoch [4/4], Batch [25700/260984], Loss: 2.2679\n",
            "Train Epoch [4/4], Batch [25800/260984], Loss: 2.1051\n",
            "Train Epoch [4/4], Batch [25900/260984], Loss: 2.3244\n",
            "Train Epoch [4/4], Batch [26000/260984], Loss: 2.3631\n",
            "Train Epoch [4/4], Batch [26100/260984], Loss: 2.3641\n",
            "Train Epoch [4/4], Batch [26200/260984], Loss: 2.3462\n",
            "Train Epoch [4/4], Batch [26300/260984], Loss: 2.3660\n",
            "Train Epoch [4/4], Batch [26400/260984], Loss: 2.2519\n",
            "Train Epoch [4/4], Batch [26500/260984], Loss: 2.2703\n",
            "Train Epoch [4/4], Batch [26600/260984], Loss: 2.2110\n",
            "Train Epoch [4/4], Batch [26700/260984], Loss: 2.3805\n",
            "Train Epoch [4/4], Batch [26800/260984], Loss: 2.4026\n",
            "Train Epoch [4/4], Batch [26900/260984], Loss: 2.3841\n",
            "Train Epoch [4/4], Batch [27000/260984], Loss: 2.4124\n",
            "Train Epoch [4/4], Batch [27100/260984], Loss: 2.2389\n",
            "Train Epoch [4/4], Batch [27200/260984], Loss: 2.1497\n",
            "Train Epoch [4/4], Batch [27300/260984], Loss: 2.3209\n",
            "Train Epoch [4/4], Batch [27400/260984], Loss: 2.1665\n",
            "Train Epoch [4/4], Batch [27500/260984], Loss: 2.2896\n",
            "Train Epoch [4/4], Batch [27600/260984], Loss: 2.2886\n",
            "Train Epoch [4/4], Batch [27700/260984], Loss: 2.2203\n",
            "Train Epoch [4/4], Batch [27800/260984], Loss: 2.2511\n",
            "Train Epoch [4/4], Batch [27900/260984], Loss: 2.3675\n",
            "Train Epoch [4/4], Batch [28000/260984], Loss: 2.2985\n",
            "Train Epoch [4/4], Batch [28100/260984], Loss: 2.2478\n",
            "Train Epoch [4/4], Batch [28200/260984], Loss: 2.3334\n",
            "Train Epoch [4/4], Batch [28300/260984], Loss: 2.2389\n",
            "Train Epoch [4/4], Batch [28400/260984], Loss: 2.3420\n",
            "Train Epoch [4/4], Batch [28500/260984], Loss: 2.3397\n",
            "Train Epoch [4/4], Batch [28600/260984], Loss: 2.1693\n",
            "Train Epoch [4/4], Batch [28700/260984], Loss: 2.3514\n",
            "Train Epoch [4/4], Batch [28800/260984], Loss: 2.2640\n",
            "Train Epoch [4/4], Batch [28900/260984], Loss: 2.3182\n",
            "Train Epoch [4/4], Batch [29000/260984], Loss: 2.4001\n",
            "Train Epoch [4/4], Batch [29100/260984], Loss: 2.2780\n",
            "Train Epoch [4/4], Batch [29200/260984], Loss: 2.2578\n",
            "Train Epoch [4/4], Batch [29300/260984], Loss: 2.2490\n",
            "Train Epoch [4/4], Batch [29400/260984], Loss: 2.5467\n",
            "Train Epoch [4/4], Batch [29500/260984], Loss: 2.3257\n",
            "Train Epoch [4/4], Batch [29600/260984], Loss: 2.3405\n",
            "Train Epoch [4/4], Batch [29700/260984], Loss: 2.4999\n",
            "Train Epoch [4/4], Batch [29800/260984], Loss: 2.4053\n",
            "Train Epoch [4/4], Batch [29900/260984], Loss: 2.3642\n",
            "Train Epoch [4/4], Batch [30000/260984], Loss: 2.2567\n",
            "Train Epoch [4/4], Batch [30100/260984], Loss: 2.4049\n",
            "Train Epoch [4/4], Batch [30200/260984], Loss: 2.3819\n",
            "Train Epoch [4/4], Batch [30300/260984], Loss: 2.3061\n",
            "Train Epoch [4/4], Batch [30400/260984], Loss: 2.2653\n",
            "Train Epoch [4/4], Batch [30500/260984], Loss: 2.3254\n",
            "Train Epoch [4/4], Batch [30600/260984], Loss: 2.3731\n",
            "Train Epoch [4/4], Batch [30700/260984], Loss: 2.2584\n",
            "Train Epoch [4/4], Batch [30800/260984], Loss: 2.3966\n",
            "Train Epoch [4/4], Batch [30900/260984], Loss: 2.3893\n",
            "Train Epoch [4/4], Batch [31000/260984], Loss: 2.2953\n",
            "Train Epoch [4/4], Batch [31100/260984], Loss: 2.1669\n",
            "Train Epoch [4/4], Batch [31200/260984], Loss: 2.2683\n",
            "Train Epoch [4/4], Batch [31300/260984], Loss: 2.2582\n",
            "Train Epoch [4/4], Batch [31400/260984], Loss: 2.3443\n",
            "Train Epoch [4/4], Batch [31500/260984], Loss: 2.3454\n",
            "Train Epoch [4/4], Batch [31600/260984], Loss: 2.2385\n",
            "Train Epoch [4/4], Batch [31700/260984], Loss: 2.2058\n",
            "Train Epoch [4/4], Batch [31800/260984], Loss: 2.2885\n",
            "Train Epoch [4/4], Batch [31900/260984], Loss: 2.1953\n",
            "Train Epoch [4/4], Batch [32000/260984], Loss: 2.2492\n",
            "Train Epoch [4/4], Batch [32100/260984], Loss: 2.3339\n",
            "Train Epoch [4/4], Batch [32200/260984], Loss: 2.1906\n",
            "Train Epoch [4/4], Batch [32300/260984], Loss: 2.2996\n",
            "Train Epoch [4/4], Batch [32400/260984], Loss: 2.2508\n",
            "Train Epoch [4/4], Batch [32500/260984], Loss: 2.3805\n",
            "Train Epoch [4/4], Batch [32600/260984], Loss: 2.3471\n",
            "Train Epoch [4/4], Batch [32700/260984], Loss: 2.2974\n",
            "Train Epoch [4/4], Batch [32800/260984], Loss: 2.3160\n",
            "Train Epoch [4/4], Batch [32900/260984], Loss: 2.2605\n",
            "Train Epoch [4/4], Batch [33000/260984], Loss: 2.2919\n",
            "Train Epoch [4/4], Batch [33100/260984], Loss: 2.3454\n",
            "Train Epoch [4/4], Batch [33200/260984], Loss: 2.2835\n",
            "Train Epoch [4/4], Batch [33300/260984], Loss: 2.3157\n",
            "Train Epoch [4/4], Batch [33400/260984], Loss: 2.2270\n",
            "Train Epoch [4/4], Batch [33500/260984], Loss: 2.3963\n",
            "Train Epoch [4/4], Batch [33600/260984], Loss: 2.3569\n",
            "Train Epoch [4/4], Batch [33700/260984], Loss: 2.2501\n",
            "Train Epoch [4/4], Batch [33800/260984], Loss: 2.4028\n",
            "Train Epoch [4/4], Batch [33900/260984], Loss: 2.5011\n",
            "Train Epoch [4/4], Batch [34000/260984], Loss: 2.3067\n",
            "Train Epoch [4/4], Batch [34100/260984], Loss: 2.3597\n",
            "Train Epoch [4/4], Batch [34200/260984], Loss: 2.2838\n",
            "Train Epoch [4/4], Batch [34300/260984], Loss: 2.2570\n",
            "Train Epoch [4/4], Batch [34400/260984], Loss: 2.2457\n",
            "Train Epoch [4/4], Batch [34500/260984], Loss: 2.4384\n",
            "Train Epoch [4/4], Batch [34600/260984], Loss: 2.3245\n",
            "Train Epoch [4/4], Batch [34700/260984], Loss: 2.2091\n",
            "Train Epoch [4/4], Batch [34800/260984], Loss: 2.1553\n",
            "Train Epoch [4/4], Batch [34900/260984], Loss: 2.3696\n",
            "Train Epoch [4/4], Batch [35000/260984], Loss: 2.1956\n",
            "Train Epoch [4/4], Batch [35100/260984], Loss: 2.2827\n",
            "Train Epoch [4/4], Batch [35200/260984], Loss: 2.4488\n",
            "Train Epoch [4/4], Batch [35300/260984], Loss: 2.4466\n",
            "Train Epoch [4/4], Batch [35400/260984], Loss: 2.1983\n",
            "Train Epoch [4/4], Batch [35500/260984], Loss: 2.2870\n",
            "Train Epoch [4/4], Batch [35600/260984], Loss: 2.2298\n",
            "Train Epoch [4/4], Batch [35700/260984], Loss: 2.2190\n",
            "Train Epoch [4/4], Batch [35800/260984], Loss: 2.3087\n",
            "Train Epoch [4/4], Batch [35900/260984], Loss: 2.3802\n",
            "Train Epoch [4/4], Batch [36000/260984], Loss: 2.3157\n",
            "Train Epoch [4/4], Batch [36100/260984], Loss: 2.2882\n",
            "Train Epoch [4/4], Batch [36200/260984], Loss: 2.2467\n",
            "Train Epoch [4/4], Batch [36300/260984], Loss: 2.4040\n",
            "Train Epoch [4/4], Batch [36400/260984], Loss: 2.4282\n",
            "Train Epoch [4/4], Batch [36500/260984], Loss: 2.2378\n",
            "Train Epoch [4/4], Batch [36600/260984], Loss: 2.3171\n",
            "Train Epoch [4/4], Batch [36700/260984], Loss: 2.3993\n",
            "Train Epoch [4/4], Batch [36800/260984], Loss: 2.2837\n",
            "Train Epoch [4/4], Batch [36900/260984], Loss: 2.2909\n",
            "Train Epoch [4/4], Batch [37000/260984], Loss: 2.2829\n",
            "Train Epoch [4/4], Batch [37100/260984], Loss: 2.3196\n",
            "Train Epoch [4/4], Batch [37200/260984], Loss: 2.3322\n",
            "Train Epoch [4/4], Batch [37300/260984], Loss: 2.3113\n",
            "Train Epoch [4/4], Batch [37400/260984], Loss: 2.3682\n",
            "Train Epoch [4/4], Batch [37500/260984], Loss: 2.2625\n",
            "Train Epoch [4/4], Batch [37600/260984], Loss: 2.3136\n",
            "Train Epoch [4/4], Batch [37700/260984], Loss: 2.2429\n",
            "Train Epoch [4/4], Batch [37800/260984], Loss: 2.3688\n",
            "Train Epoch [4/4], Batch [37900/260984], Loss: 2.4796\n",
            "Train Epoch [4/4], Batch [38000/260984], Loss: 2.3538\n",
            "Train Epoch [4/4], Batch [38100/260984], Loss: 2.2275\n",
            "Train Epoch [4/4], Batch [38200/260984], Loss: 2.3394\n",
            "Train Epoch [4/4], Batch [38300/260984], Loss: 2.3538\n",
            "Train Epoch [4/4], Batch [38400/260984], Loss: 2.1887\n",
            "Train Epoch [4/4], Batch [38500/260984], Loss: 2.3044\n",
            "Train Epoch [4/4], Batch [38600/260984], Loss: 2.3320\n",
            "Train Epoch [4/4], Batch [38700/260984], Loss: 2.4990\n",
            "Train Epoch [4/4], Batch [38800/260984], Loss: 2.2518\n",
            "Train Epoch [4/4], Batch [38900/260984], Loss: 2.2108\n",
            "Train Epoch [4/4], Batch [39000/260984], Loss: 2.3567\n",
            "Train Epoch [4/4], Batch [39100/260984], Loss: 2.3008\n",
            "Train Epoch [4/4], Batch [39200/260984], Loss: 2.3153\n",
            "Train Epoch [4/4], Batch [39300/260984], Loss: 2.3403\n",
            "Train Epoch [4/4], Batch [39400/260984], Loss: 2.3441\n",
            "Train Epoch [4/4], Batch [39500/260984], Loss: 2.1756\n",
            "Train Epoch [4/4], Batch [39600/260984], Loss: 2.4070\n",
            "Train Epoch [4/4], Batch [39700/260984], Loss: 2.2654\n",
            "Train Epoch [4/4], Batch [39800/260984], Loss: 2.3851\n",
            "Train Epoch [4/4], Batch [39900/260984], Loss: 2.2535\n",
            "Train Epoch [4/4], Batch [40000/260984], Loss: 2.3969\n",
            "Train Epoch [4/4], Batch [40100/260984], Loss: 2.2257\n",
            "Train Epoch [4/4], Batch [40200/260984], Loss: 2.2217\n",
            "Train Epoch [4/4], Batch [40300/260984], Loss: 2.2065\n",
            "Train Epoch [4/4], Batch [40400/260984], Loss: 2.3152\n",
            "Train Epoch [4/4], Batch [40500/260984], Loss: 2.1890\n",
            "Train Epoch [4/4], Batch [40600/260984], Loss: 2.3783\n",
            "Train Epoch [4/4], Batch [40700/260984], Loss: 2.4091\n",
            "Train Epoch [4/4], Batch [40800/260984], Loss: 2.4088\n",
            "Train Epoch [4/4], Batch [40900/260984], Loss: 2.4451\n",
            "Train Epoch [4/4], Batch [41000/260984], Loss: 2.4999\n",
            "Train Epoch [4/4], Batch [41100/260984], Loss: 2.3461\n",
            "Train Epoch [4/4], Batch [41200/260984], Loss: 2.3507\n",
            "Train Epoch [4/4], Batch [41300/260984], Loss: 2.2441\n",
            "Train Epoch [4/4], Batch [41400/260984], Loss: 2.2286\n",
            "Train Epoch [4/4], Batch [41500/260984], Loss: 2.3795\n",
            "Train Epoch [4/4], Batch [41600/260984], Loss: 2.2230\n",
            "Train Epoch [4/4], Batch [41700/260984], Loss: 2.3303\n",
            "Train Epoch [4/4], Batch [41800/260984], Loss: 2.2245\n",
            "Train Epoch [4/4], Batch [41900/260984], Loss: 2.3060\n",
            "Train Epoch [4/4], Batch [42000/260984], Loss: 2.3075\n",
            "Train Epoch [4/4], Batch [42100/260984], Loss: 2.2460\n",
            "Train Epoch [4/4], Batch [42200/260984], Loss: 2.2898\n",
            "Train Epoch [4/4], Batch [42300/260984], Loss: 2.2919\n",
            "Train Epoch [4/4], Batch [42400/260984], Loss: 2.3735\n",
            "Train Epoch [4/4], Batch [42500/260984], Loss: 2.2855\n",
            "Train Epoch [4/4], Batch [42600/260984], Loss: 2.3155\n",
            "Train Epoch [4/4], Batch [42700/260984], Loss: 2.1642\n",
            "Train Epoch [4/4], Batch [42800/260984], Loss: 2.3149\n",
            "Train Epoch [4/4], Batch [42900/260984], Loss: 2.2330\n",
            "Train Epoch [4/4], Batch [43000/260984], Loss: 2.3347\n",
            "Train Epoch [4/4], Batch [43100/260984], Loss: 2.2125\n",
            "Train Epoch [4/4], Batch [43200/260984], Loss: 2.1560\n",
            "Train Epoch [4/4], Batch [43300/260984], Loss: 2.2849\n",
            "Train Epoch [4/4], Batch [43400/260984], Loss: 2.2514\n",
            "Train Epoch [4/4], Batch [43500/260984], Loss: 2.3478\n",
            "Train Epoch [4/4], Batch [43600/260984], Loss: 2.3158\n",
            "Train Epoch [4/4], Batch [43700/260984], Loss: 2.1042\n",
            "Train Epoch [4/4], Batch [43800/260984], Loss: 2.2224\n",
            "Train Epoch [4/4], Batch [43900/260984], Loss: 2.2849\n",
            "Train Epoch [4/4], Batch [44000/260984], Loss: 2.2542\n",
            "Train Epoch [4/4], Batch [44100/260984], Loss: 2.3161\n",
            "Train Epoch [4/4], Batch [44200/260984], Loss: 2.3147\n",
            "Train Epoch [4/4], Batch [44300/260984], Loss: 2.2964\n",
            "Train Epoch [4/4], Batch [44400/260984], Loss: 2.3695\n",
            "Train Epoch [4/4], Batch [44500/260984], Loss: 2.3909\n",
            "Train Epoch [4/4], Batch [44600/260984], Loss: 2.2850\n",
            "Train Epoch [4/4], Batch [44700/260984], Loss: 2.3440\n",
            "Train Epoch [4/4], Batch [44800/260984], Loss: 2.1712\n",
            "Train Epoch [4/4], Batch [44900/260984], Loss: 2.3385\n",
            "Train Epoch [4/4], Batch [45000/260984], Loss: 2.4183\n",
            "Train Epoch [4/4], Batch [45100/260984], Loss: 2.2761\n",
            "Train Epoch [4/4], Batch [45200/260984], Loss: 2.3227\n",
            "Train Epoch [4/4], Batch [45300/260984], Loss: 2.3631\n",
            "Train Epoch [4/4], Batch [45400/260984], Loss: 2.2069\n",
            "Train Epoch [4/4], Batch [45500/260984], Loss: 2.2971\n",
            "Train Epoch [4/4], Batch [45600/260984], Loss: 2.2720\n",
            "Train Epoch [4/4], Batch [45700/260984], Loss: 2.1816\n",
            "Train Epoch [4/4], Batch [45800/260984], Loss: 2.4334\n",
            "Train Epoch [4/4], Batch [45900/260984], Loss: 2.3136\n",
            "Train Epoch [4/4], Batch [46000/260984], Loss: 2.2412\n",
            "Train Epoch [4/4], Batch [46100/260984], Loss: 2.2987\n",
            "Train Epoch [4/4], Batch [46200/260984], Loss: 2.2608\n",
            "Train Epoch [4/4], Batch [46300/260984], Loss: 2.2528\n",
            "Train Epoch [4/4], Batch [46400/260984], Loss: 2.4111\n",
            "Train Epoch [4/4], Batch [46500/260984], Loss: 2.1968\n",
            "Train Epoch [4/4], Batch [46600/260984], Loss: 2.3891\n",
            "Train Epoch [4/4], Batch [46700/260984], Loss: 2.2367\n",
            "Train Epoch [4/4], Batch [46800/260984], Loss: 2.2761\n",
            "Train Epoch [4/4], Batch [46900/260984], Loss: 2.2408\n",
            "Train Epoch [4/4], Batch [47000/260984], Loss: 2.2313\n",
            "Train Epoch [4/4], Batch [47100/260984], Loss: 2.3324\n",
            "Train Epoch [4/4], Batch [47200/260984], Loss: 2.2356\n",
            "Train Epoch [4/4], Batch [47300/260984], Loss: 2.2121\n",
            "Train Epoch [4/4], Batch [47400/260984], Loss: 2.5050\n",
            "Train Epoch [4/4], Batch [47500/260984], Loss: 2.4089\n",
            "Train Epoch [4/4], Batch [47600/260984], Loss: 2.3718\n",
            "Train Epoch [4/4], Batch [47700/260984], Loss: 2.1941\n",
            "Train Epoch [4/4], Batch [47800/260984], Loss: 2.4662\n",
            "Train Epoch [4/4], Batch [47900/260984], Loss: 2.2185\n",
            "Train Epoch [4/4], Batch [48000/260984], Loss: 2.2175\n",
            "Train Epoch [4/4], Batch [48100/260984], Loss: 2.3371\n",
            "Train Epoch [4/4], Batch [48200/260984], Loss: 2.2234\n",
            "Train Epoch [4/4], Batch [48300/260984], Loss: 2.3382\n",
            "Train Epoch [4/4], Batch [48400/260984], Loss: 2.2252\n",
            "Train Epoch [4/4], Batch [48500/260984], Loss: 2.3886\n",
            "Train Epoch [4/4], Batch [48600/260984], Loss: 2.3360\n",
            "Train Epoch [4/4], Batch [48700/260984], Loss: 2.3999\n",
            "Train Epoch [4/4], Batch [48800/260984], Loss: 2.3720\n",
            "Train Epoch [4/4], Batch [48900/260984], Loss: 2.3533\n",
            "Train Epoch [4/4], Batch [49000/260984], Loss: 2.3839\n",
            "Train Epoch [4/4], Batch [49100/260984], Loss: 2.3043\n",
            "Train Epoch [4/4], Batch [49200/260984], Loss: 2.2160\n",
            "Train Epoch [4/4], Batch [49300/260984], Loss: 2.3227\n",
            "Train Epoch [4/4], Batch [49400/260984], Loss: 2.3774\n",
            "Train Epoch [4/4], Batch [49500/260984], Loss: 2.2965\n",
            "Train Epoch [4/4], Batch [49600/260984], Loss: 2.3153\n",
            "Train Epoch [4/4], Batch [49700/260984], Loss: 2.2811\n",
            "Train Epoch [4/4], Batch [49800/260984], Loss: 2.1569\n",
            "Train Epoch [4/4], Batch [49900/260984], Loss: 2.2536\n",
            "Train Epoch [4/4], Batch [50000/260984], Loss: 2.3455\n",
            "Train Epoch [4/4], Batch [50100/260984], Loss: 2.3143\n",
            "Train Epoch [4/4], Batch [50200/260984], Loss: 2.1223\n",
            "Train Epoch [4/4], Batch [50300/260984], Loss: 2.5199\n",
            "Train Epoch [4/4], Batch [50400/260984], Loss: 2.5022\n",
            "Train Epoch [4/4], Batch [50500/260984], Loss: 2.4121\n",
            "Train Epoch [4/4], Batch [50600/260984], Loss: 2.2498\n",
            "Train Epoch [4/4], Batch [50700/260984], Loss: 2.3141\n",
            "Train Epoch [4/4], Batch [50800/260984], Loss: 2.3150\n",
            "Train Epoch [4/4], Batch [50900/260984], Loss: 2.3822\n",
            "Train Epoch [4/4], Batch [51000/260984], Loss: 2.3502\n",
            "Train Epoch [4/4], Batch [51100/260984], Loss: 2.1265\n",
            "Train Epoch [4/4], Batch [51200/260984], Loss: 2.2588\n",
            "Train Epoch [4/4], Batch [51300/260984], Loss: 2.2669\n",
            "Train Epoch [4/4], Batch [51400/260984], Loss: 2.3678\n",
            "Train Epoch [4/4], Batch [51500/260984], Loss: 2.4088\n",
            "Train Epoch [4/4], Batch [51600/260984], Loss: 2.2470\n",
            "Train Epoch [4/4], Batch [51700/260984], Loss: 2.3206\n",
            "Train Epoch [4/4], Batch [51800/260984], Loss: 2.2607\n",
            "Train Epoch [4/4], Batch [51900/260984], Loss: 2.3156\n",
            "Train Epoch [4/4], Batch [52000/260984], Loss: 2.2829\n",
            "Train Epoch [4/4], Batch [52100/260984], Loss: 2.1702\n",
            "Train Epoch [4/4], Batch [52200/260984], Loss: 2.2706\n",
            "Train Epoch [4/4], Batch [52300/260984], Loss: 2.3218\n",
            "Train Epoch [4/4], Batch [52400/260984], Loss: 2.4027\n",
            "Train Epoch [4/4], Batch [52500/260984], Loss: 2.3512\n",
            "Train Epoch [4/4], Batch [52600/260984], Loss: 2.5036\n",
            "Train Epoch [4/4], Batch [52700/260984], Loss: 2.3198\n",
            "Train Epoch [4/4], Batch [52800/260984], Loss: 2.1960\n",
            "Train Epoch [4/4], Batch [52900/260984], Loss: 2.2623\n",
            "Train Epoch [4/4], Batch [53000/260984], Loss: 2.2525\n",
            "Train Epoch [4/4], Batch [53100/260984], Loss: 2.3425\n",
            "Train Epoch [4/4], Batch [53200/260984], Loss: 2.2879\n",
            "Train Epoch [4/4], Batch [53300/260984], Loss: 2.4071\n",
            "Train Epoch [4/4], Batch [53400/260984], Loss: 2.2530\n",
            "Train Epoch [4/4], Batch [53500/260984], Loss: 2.3017\n",
            "Train Epoch [4/4], Batch [53600/260984], Loss: 2.3348\n",
            "Train Epoch [4/4], Batch [53700/260984], Loss: 2.1622\n",
            "Train Epoch [4/4], Batch [53800/260984], Loss: 2.2252\n",
            "Train Epoch [4/4], Batch [53900/260984], Loss: 2.2886\n",
            "Train Epoch [4/4], Batch [54000/260984], Loss: 2.1628\n",
            "Train Epoch [4/4], Batch [54100/260984], Loss: 2.3391\n",
            "Train Epoch [4/4], Batch [54200/260984], Loss: 2.2944\n",
            "Train Epoch [4/4], Batch [54300/260984], Loss: 2.4075\n",
            "Train Epoch [4/4], Batch [54400/260984], Loss: 2.3764\n",
            "Train Epoch [4/4], Batch [54500/260984], Loss: 2.3238\n",
            "Train Epoch [4/4], Batch [54600/260984], Loss: 2.1970\n",
            "Train Epoch [4/4], Batch [54700/260984], Loss: 2.2143\n",
            "Train Epoch [4/4], Batch [54800/260984], Loss: 2.1714\n",
            "Train Epoch [4/4], Batch [54900/260984], Loss: 2.4434\n",
            "Train Epoch [4/4], Batch [55000/260984], Loss: 2.3850\n",
            "Train Epoch [4/4], Batch [55100/260984], Loss: 2.3462\n",
            "Train Epoch [4/4], Batch [55200/260984], Loss: 2.3182\n",
            "Train Epoch [4/4], Batch [55300/260984], Loss: 2.2866\n",
            "Train Epoch [4/4], Batch [55400/260984], Loss: 2.3219\n",
            "Train Epoch [4/4], Batch [55500/260984], Loss: 2.2901\n",
            "Train Epoch [4/4], Batch [55600/260984], Loss: 2.3488\n",
            "Train Epoch [4/4], Batch [55700/260984], Loss: 2.3777\n",
            "Train Epoch [4/4], Batch [55800/260984], Loss: 2.2870\n",
            "Train Epoch [4/4], Batch [55900/260984], Loss: 2.2741\n",
            "Train Epoch [4/4], Batch [56000/260984], Loss: 2.4465\n",
            "Train Epoch [4/4], Batch [56100/260984], Loss: 2.2532\n",
            "Train Epoch [4/4], Batch [56200/260984], Loss: 2.4496\n",
            "Train Epoch [4/4], Batch [56300/260984], Loss: 2.3920\n",
            "Train Epoch [4/4], Batch [56400/260984], Loss: 2.3839\n",
            "Train Epoch [4/4], Batch [56500/260984], Loss: 2.4474\n",
            "Train Epoch [4/4], Batch [56600/260984], Loss: 2.4203\n",
            "Train Epoch [4/4], Batch [56700/260984], Loss: 2.3678\n",
            "Train Epoch [4/4], Batch [56800/260984], Loss: 2.2355\n",
            "Train Epoch [4/4], Batch [56900/260984], Loss: 2.3790\n",
            "Train Epoch [4/4], Batch [57000/260984], Loss: 2.4649\n",
            "Train Epoch [4/4], Batch [57100/260984], Loss: 2.2099\n",
            "Train Epoch [4/4], Batch [57200/260984], Loss: 2.4085\n",
            "Train Epoch [4/4], Batch [57300/260984], Loss: 2.2680\n",
            "Train Epoch [4/4], Batch [57400/260984], Loss: 2.2289\n",
            "Train Epoch [4/4], Batch [57500/260984], Loss: 2.1654\n",
            "Train Epoch [4/4], Batch [57600/260984], Loss: 2.4356\n",
            "Train Epoch [4/4], Batch [57700/260984], Loss: 2.3535\n",
            "Train Epoch [4/4], Batch [57800/260984], Loss: 2.3752\n",
            "Train Epoch [4/4], Batch [57900/260984], Loss: 2.2097\n",
            "Train Epoch [4/4], Batch [58000/260984], Loss: 2.4380\n",
            "Train Epoch [4/4], Batch [58100/260984], Loss: 2.1130\n",
            "Train Epoch [4/4], Batch [58200/260984], Loss: 2.3317\n",
            "Train Epoch [4/4], Batch [58300/260984], Loss: 2.2538\n",
            "Train Epoch [4/4], Batch [58400/260984], Loss: 2.2103\n",
            "Train Epoch [4/4], Batch [58500/260984], Loss: 2.3145\n",
            "Train Epoch [4/4], Batch [58600/260984], Loss: 2.2889\n",
            "Train Epoch [4/4], Batch [58700/260984], Loss: 2.2618\n",
            "Train Epoch [4/4], Batch [58800/260984], Loss: 2.1962\n",
            "Train Epoch [4/4], Batch [58900/260984], Loss: 2.3899\n",
            "Train Epoch [4/4], Batch [59000/260984], Loss: 2.1677\n",
            "Train Epoch [4/4], Batch [59100/260984], Loss: 2.3781\n",
            "Train Epoch [4/4], Batch [59200/260984], Loss: 2.2880\n",
            "Train Epoch [4/4], Batch [59300/260984], Loss: 2.3457\n",
            "Train Epoch [4/4], Batch [59400/260984], Loss: 2.2890\n",
            "Train Epoch [4/4], Batch [59500/260984], Loss: 2.3711\n",
            "Train Epoch [4/4], Batch [59600/260984], Loss: 2.2561\n",
            "Train Epoch [4/4], Batch [59700/260984], Loss: 2.3148\n",
            "Train Epoch [4/4], Batch [59800/260984], Loss: 2.2777\n",
            "Train Epoch [4/4], Batch [59900/260984], Loss: 2.3614\n",
            "Train Epoch [4/4], Batch [60000/260984], Loss: 2.4399\n",
            "Train Epoch [4/4], Batch [60100/260984], Loss: 2.3999\n",
            "Train Epoch [4/4], Batch [60200/260984], Loss: 2.3574\n",
            "Train Epoch [4/4], Batch [60300/260984], Loss: 2.1769\n",
            "Train Epoch [4/4], Batch [60400/260984], Loss: 2.3737\n",
            "Train Epoch [4/4], Batch [60500/260984], Loss: 2.3956\n",
            "Train Epoch [4/4], Batch [60600/260984], Loss: 2.3435\n",
            "Train Epoch [4/4], Batch [60700/260984], Loss: 2.1986\n",
            "Train Epoch [4/4], Batch [60800/260984], Loss: 2.3242\n",
            "Train Epoch [4/4], Batch [60900/260984], Loss: 2.3685\n",
            "Train Epoch [4/4], Batch [61000/260984], Loss: 2.4204\n",
            "Train Epoch [4/4], Batch [61100/260984], Loss: 2.2002\n",
            "Train Epoch [4/4], Batch [61200/260984], Loss: 2.3780\n",
            "Train Epoch [4/4], Batch [61300/260984], Loss: 2.2300\n",
            "Train Epoch [4/4], Batch [61400/260984], Loss: 2.2049\n",
            "Train Epoch [4/4], Batch [61500/260984], Loss: 2.2463\n",
            "Train Epoch [4/4], Batch [61600/260984], Loss: 2.2483\n",
            "Train Epoch [4/4], Batch [61700/260984], Loss: 2.3064\n",
            "Train Epoch [4/4], Batch [61800/260984], Loss: 2.3503\n",
            "Train Epoch [4/4], Batch [61900/260984], Loss: 2.1912\n",
            "Train Epoch [4/4], Batch [62000/260984], Loss: 2.3313\n",
            "Train Epoch [4/4], Batch [62100/260984], Loss: 2.3210\n",
            "Train Epoch [4/4], Batch [62200/260984], Loss: 2.3482\n",
            "Train Epoch [4/4], Batch [62300/260984], Loss: 2.2848\n",
            "Train Epoch [4/4], Batch [62400/260984], Loss: 2.2738\n",
            "Train Epoch [4/4], Batch [62500/260984], Loss: 2.3774\n",
            "Train Epoch [4/4], Batch [62600/260984], Loss: 2.4130\n",
            "Train Epoch [4/4], Batch [62700/260984], Loss: 2.2502\n",
            "Train Epoch [4/4], Batch [62800/260984], Loss: 2.2050\n",
            "Train Epoch [4/4], Batch [62900/260984], Loss: 2.3157\n",
            "Train Epoch [4/4], Batch [63000/260984], Loss: 2.3256\n",
            "Train Epoch [4/4], Batch [63100/260984], Loss: 2.2870\n",
            "Train Epoch [4/4], Batch [63200/260984], Loss: 2.3226\n",
            "Train Epoch [4/4], Batch [63300/260984], Loss: 2.3050\n",
            "Train Epoch [4/4], Batch [63400/260984], Loss: 2.4655\n",
            "Train Epoch [4/4], Batch [63500/260984], Loss: 2.3183\n",
            "Train Epoch [4/4], Batch [63600/260984], Loss: 2.2688\n",
            "Train Epoch [4/4], Batch [63700/260984], Loss: 2.3599\n",
            "Train Epoch [4/4], Batch [63800/260984], Loss: 2.2510\n",
            "Train Epoch [4/4], Batch [63900/260984], Loss: 2.3451\n",
            "Train Epoch [4/4], Batch [64000/260984], Loss: 2.3913\n",
            "Train Epoch [4/4], Batch [64100/260984], Loss: 2.2776\n",
            "Train Epoch [4/4], Batch [64200/260984], Loss: 2.4296\n",
            "Train Epoch [4/4], Batch [64300/260984], Loss: 2.3473\n",
            "Train Epoch [4/4], Batch [64400/260984], Loss: 2.2235\n",
            "Train Epoch [4/4], Batch [64500/260984], Loss: 2.2759\n",
            "Train Epoch [4/4], Batch [64600/260984], Loss: 2.1492\n",
            "Train Epoch [4/4], Batch [64700/260984], Loss: 2.1525\n",
            "Train Epoch [4/4], Batch [64800/260984], Loss: 2.1656\n",
            "Train Epoch [4/4], Batch [64900/260984], Loss: 2.3186\n",
            "Train Epoch [4/4], Batch [65000/260984], Loss: 2.3076\n",
            "Train Epoch [4/4], Batch [65100/260984], Loss: 2.4952\n",
            "Train Epoch [4/4], Batch [65200/260984], Loss: 2.4624\n",
            "Train Epoch [4/4], Batch [65300/260984], Loss: 2.3132\n",
            "Train Epoch [4/4], Batch [65400/260984], Loss: 2.2261\n",
            "Train Epoch [4/4], Batch [65500/260984], Loss: 2.3628\n",
            "Train Epoch [4/4], Batch [65600/260984], Loss: 2.4098\n",
            "Train Epoch [4/4], Batch [65700/260984], Loss: 2.1284\n",
            "Train Epoch [4/4], Batch [65800/260984], Loss: 2.2895\n",
            "Train Epoch [4/4], Batch [65900/260984], Loss: 2.2021\n",
            "Train Epoch [4/4], Batch [66000/260984], Loss: 2.1835\n",
            "Train Epoch [4/4], Batch [66100/260984], Loss: 2.2602\n",
            "Train Epoch [4/4], Batch [66200/260984], Loss: 2.2895\n",
            "Train Epoch [4/4], Batch [66300/260984], Loss: 2.3718\n",
            "Train Epoch [4/4], Batch [66400/260984], Loss: 2.0696\n",
            "Train Epoch [4/4], Batch [66500/260984], Loss: 2.3069\n",
            "Train Epoch [4/4], Batch [66600/260984], Loss: 2.3769\n",
            "Train Epoch [4/4], Batch [66700/260984], Loss: 2.2204\n",
            "Train Epoch [4/4], Batch [66800/260984], Loss: 2.3078\n",
            "Train Epoch [4/4], Batch [66900/260984], Loss: 2.2693\n",
            "Train Epoch [4/4], Batch [67000/260984], Loss: 2.2532\n",
            "Train Epoch [4/4], Batch [67100/260984], Loss: 2.2272\n",
            "Train Epoch [4/4], Batch [67200/260984], Loss: 2.3506\n",
            "Train Epoch [4/4], Batch [67300/260984], Loss: 2.1559\n",
            "Train Epoch [4/4], Batch [67400/260984], Loss: 2.4029\n",
            "Train Epoch [4/4], Batch [67500/260984], Loss: 2.3764\n",
            "Train Epoch [4/4], Batch [67600/260984], Loss: 2.2206\n",
            "Train Epoch [4/4], Batch [67700/260984], Loss: 2.2188\n",
            "Train Epoch [4/4], Batch [67800/260984], Loss: 2.3591\n",
            "Train Epoch [4/4], Batch [67900/260984], Loss: 2.2534\n",
            "Train Epoch [4/4], Batch [68000/260984], Loss: 2.3496\n",
            "Train Epoch [4/4], Batch [68100/260984], Loss: 2.4123\n",
            "Train Epoch [4/4], Batch [68200/260984], Loss: 2.2799\n",
            "Train Epoch [4/4], Batch [68300/260984], Loss: 2.3154\n",
            "Train Epoch [4/4], Batch [68400/260984], Loss: 2.3016\n",
            "Train Epoch [4/4], Batch [68500/260984], Loss: 2.2063\n",
            "Train Epoch [4/4], Batch [68600/260984], Loss: 2.3140\n",
            "Train Epoch [4/4], Batch [68700/260984], Loss: 2.2271\n",
            "Train Epoch [4/4], Batch [68800/260984], Loss: 2.2114\n",
            "Train Epoch [4/4], Batch [68900/260984], Loss: 2.4431\n",
            "Train Epoch [4/4], Batch [69000/260984], Loss: 2.3145\n",
            "Train Epoch [4/4], Batch [69100/260984], Loss: 2.3066\n",
            "Train Epoch [4/4], Batch [69200/260984], Loss: 2.3472\n",
            "Train Epoch [4/4], Batch [69300/260984], Loss: 2.3500\n",
            "Train Epoch [4/4], Batch [69400/260984], Loss: 2.3674\n",
            "Train Epoch [4/4], Batch [69500/260984], Loss: 2.3100\n",
            "Train Epoch [4/4], Batch [69600/260984], Loss: 2.3082\n",
            "Train Epoch [4/4], Batch [69700/260984], Loss: 2.2897\n",
            "Train Epoch [4/4], Batch [69800/260984], Loss: 2.3379\n",
            "Train Epoch [4/4], Batch [69900/260984], Loss: 2.2245\n",
            "Train Epoch [4/4], Batch [70000/260984], Loss: 2.2098\n",
            "Train Epoch [4/4], Batch [70100/260984], Loss: 2.1894\n",
            "Train Epoch [4/4], Batch [70200/260984], Loss: 2.2256\n",
            "Train Epoch [4/4], Batch [70300/260984], Loss: 2.2993\n",
            "Train Epoch [4/4], Batch [70400/260984], Loss: 2.3432\n",
            "Train Epoch [4/4], Batch [70500/260984], Loss: 2.2803\n",
            "Train Epoch [4/4], Batch [70600/260984], Loss: 2.4100\n",
            "Train Epoch [4/4], Batch [70700/260984], Loss: 2.1976\n",
            "Train Epoch [4/4], Batch [70800/260984], Loss: 2.3995\n",
            "Train Epoch [4/4], Batch [70900/260984], Loss: 2.2882\n",
            "Train Epoch [4/4], Batch [71000/260984], Loss: 2.2982\n",
            "Train Epoch [4/4], Batch [71100/260984], Loss: 2.3109\n",
            "Train Epoch [4/4], Batch [71200/260984], Loss: 2.3189\n",
            "Train Epoch [4/4], Batch [71300/260984], Loss: 2.1895\n",
            "Train Epoch [4/4], Batch [71400/260984], Loss: 2.2821\n",
            "Train Epoch [4/4], Batch [71500/260984], Loss: 2.3456\n",
            "Train Epoch [4/4], Batch [71600/260984], Loss: 2.3186\n",
            "Train Epoch [4/4], Batch [71700/260984], Loss: 2.2767\n",
            "Train Epoch [4/4], Batch [71800/260984], Loss: 2.3928\n",
            "Train Epoch [4/4], Batch [71900/260984], Loss: 2.2561\n",
            "Train Epoch [4/4], Batch [72000/260984], Loss: 2.1418\n",
            "Train Epoch [4/4], Batch [72100/260984], Loss: 2.2434\n",
            "Train Epoch [4/4], Batch [72200/260984], Loss: 2.3065\n",
            "Train Epoch [4/4], Batch [72300/260984], Loss: 2.2995\n",
            "Train Epoch [4/4], Batch [72400/260984], Loss: 2.2512\n",
            "Train Epoch [4/4], Batch [72500/260984], Loss: 2.2860\n",
            "Train Epoch [4/4], Batch [72600/260984], Loss: 2.2512\n",
            "Train Epoch [4/4], Batch [72700/260984], Loss: 2.1892\n",
            "Train Epoch [4/4], Batch [72800/260984], Loss: 2.3477\n",
            "Train Epoch [4/4], Batch [72900/260984], Loss: 2.3444\n",
            "Train Epoch [4/4], Batch [73000/260984], Loss: 2.2991\n",
            "Train Epoch [4/4], Batch [73100/260984], Loss: 2.3347\n",
            "Train Epoch [4/4], Batch [73200/260984], Loss: 2.4234\n",
            "Train Epoch [4/4], Batch [73300/260984], Loss: 2.2210\n",
            "Train Epoch [4/4], Batch [73400/260984], Loss: 2.3640\n",
            "Train Epoch [4/4], Batch [73500/260984], Loss: 2.3810\n",
            "Train Epoch [4/4], Batch [73600/260984], Loss: 2.3510\n",
            "Train Epoch [4/4], Batch [73700/260984], Loss: 2.2327\n",
            "Train Epoch [4/4], Batch [73800/260984], Loss: 2.1594\n",
            "Train Epoch [4/4], Batch [73900/260984], Loss: 2.3844\n",
            "Train Epoch [4/4], Batch [74000/260984], Loss: 2.2633\n",
            "Train Epoch [4/4], Batch [74100/260984], Loss: 2.1936\n",
            "Train Epoch [4/4], Batch [74200/260984], Loss: 2.2838\n",
            "Train Epoch [4/4], Batch [74300/260984], Loss: 2.4727\n",
            "Train Epoch [4/4], Batch [74400/260984], Loss: 2.4651\n",
            "Train Epoch [4/4], Batch [74500/260984], Loss: 2.3902\n",
            "Train Epoch [4/4], Batch [74600/260984], Loss: 2.1746\n",
            "Train Epoch [4/4], Batch [74700/260984], Loss: 2.2050\n",
            "Train Epoch [4/4], Batch [74800/260984], Loss: 2.4347\n",
            "Train Epoch [4/4], Batch [74900/260984], Loss: 2.2804\n",
            "Train Epoch [4/4], Batch [75000/260984], Loss: 2.3779\n",
            "Train Epoch [4/4], Batch [75100/260984], Loss: 2.3778\n",
            "Train Epoch [4/4], Batch [75200/260984], Loss: 2.3329\n",
            "Train Epoch [4/4], Batch [75300/260984], Loss: 2.3087\n",
            "Train Epoch [4/4], Batch [75400/260984], Loss: 2.3157\n",
            "Train Epoch [4/4], Batch [75500/260984], Loss: 2.3067\n",
            "Train Epoch [4/4], Batch [75600/260984], Loss: 2.3812\n",
            "Train Epoch [4/4], Batch [75700/260984], Loss: 2.2285\n",
            "Train Epoch [4/4], Batch [75800/260984], Loss: 2.4105\n",
            "Train Epoch [4/4], Batch [75900/260984], Loss: 2.1048\n",
            "Train Epoch [4/4], Batch [76000/260984], Loss: 2.1886\n",
            "Train Epoch [4/4], Batch [76100/260984], Loss: 2.1879\n",
            "Train Epoch [4/4], Batch [76200/260984], Loss: 2.2498\n",
            "Train Epoch [4/4], Batch [76300/260984], Loss: 2.2131\n",
            "Train Epoch [4/4], Batch [76400/260984], Loss: 2.3458\n",
            "Train Epoch [4/4], Batch [76500/260984], Loss: 2.3027\n",
            "Train Epoch [4/4], Batch [76600/260984], Loss: 2.2095\n",
            "Train Epoch [4/4], Batch [76700/260984], Loss: 2.4044\n",
            "Train Epoch [4/4], Batch [76800/260984], Loss: 2.3469\n",
            "Train Epoch [4/4], Batch [76900/260984], Loss: 2.2227\n",
            "Train Epoch [4/4], Batch [77000/260984], Loss: 2.4023\n",
            "Train Epoch [4/4], Batch [77100/260984], Loss: 2.2589\n",
            "Train Epoch [4/4], Batch [77200/260984], Loss: 2.3490\n",
            "Train Epoch [4/4], Batch [77300/260984], Loss: 2.2213\n",
            "Train Epoch [4/4], Batch [77400/260984], Loss: 2.2710\n",
            "Train Epoch [4/4], Batch [77500/260984], Loss: 2.3191\n",
            "Train Epoch [4/4], Batch [77600/260984], Loss: 2.2628\n",
            "Train Epoch [4/4], Batch [77700/260984], Loss: 2.3118\n",
            "Train Epoch [4/4], Batch [77800/260984], Loss: 2.4119\n",
            "Train Epoch [4/4], Batch [77900/260984], Loss: 2.3321\n",
            "Train Epoch [4/4], Batch [78000/260984], Loss: 2.4074\n",
            "Train Epoch [4/4], Batch [78100/260984], Loss: 2.4378\n",
            "Train Epoch [4/4], Batch [78200/260984], Loss: 2.3681\n",
            "Train Epoch [4/4], Batch [78300/260984], Loss: 2.4047\n",
            "Train Epoch [4/4], Batch [78400/260984], Loss: 2.3411\n",
            "Train Epoch [4/4], Batch [78500/260984], Loss: 2.2204\n",
            "Train Epoch [4/4], Batch [78600/260984], Loss: 2.2517\n",
            "Train Epoch [4/4], Batch [78700/260984], Loss: 2.3056\n",
            "Train Epoch [4/4], Batch [78800/260984], Loss: 2.5441\n",
            "Train Epoch [4/4], Batch [78900/260984], Loss: 2.3272\n",
            "Train Epoch [4/4], Batch [79000/260984], Loss: 2.4511\n",
            "Train Epoch [4/4], Batch [79100/260984], Loss: 2.3669\n",
            "Train Epoch [4/4], Batch [79200/260984], Loss: 2.2765\n",
            "Train Epoch [4/4], Batch [79300/260984], Loss: 2.3527\n",
            "Train Epoch [4/4], Batch [79400/260984], Loss: 2.4658\n",
            "Train Epoch [4/4], Batch [79500/260984], Loss: 2.1794\n",
            "Train Epoch [4/4], Batch [79600/260984], Loss: 2.1951\n",
            "Train Epoch [4/4], Batch [79700/260984], Loss: 2.1376\n",
            "Train Epoch [4/4], Batch [79800/260984], Loss: 2.2414\n",
            "Train Epoch [4/4], Batch [79900/260984], Loss: 2.3211\n",
            "Train Epoch [4/4], Batch [80000/260984], Loss: 2.4400\n",
            "Train Epoch [4/4], Batch [80100/260984], Loss: 2.4179\n",
            "Train Epoch [4/4], Batch [80200/260984], Loss: 2.2762\n",
            "Train Epoch [4/4], Batch [80300/260984], Loss: 2.3136\n",
            "Train Epoch [4/4], Batch [80400/260984], Loss: 2.2348\n",
            "Train Epoch [4/4], Batch [80500/260984], Loss: 2.2922\n",
            "Train Epoch [4/4], Batch [80600/260984], Loss: 2.2604\n",
            "Train Epoch [4/4], Batch [80700/260984], Loss: 2.1939\n",
            "Train Epoch [4/4], Batch [80800/260984], Loss: 2.3247\n",
            "Train Epoch [4/4], Batch [80900/260984], Loss: 2.4438\n",
            "Train Epoch [4/4], Batch [81000/260984], Loss: 2.3130\n",
            "Train Epoch [4/4], Batch [81100/260984], Loss: 2.3332\n",
            "Train Epoch [4/4], Batch [81200/260984], Loss: 2.3347\n",
            "Train Epoch [4/4], Batch [81300/260984], Loss: 2.1663\n",
            "Train Epoch [4/4], Batch [81400/260984], Loss: 2.3926\n",
            "Train Epoch [4/4], Batch [81500/260984], Loss: 2.0665\n",
            "Train Epoch [4/4], Batch [81600/260984], Loss: 2.3461\n",
            "Train Epoch [4/4], Batch [81700/260984], Loss: 2.4110\n",
            "Train Epoch [4/4], Batch [81800/260984], Loss: 2.4568\n",
            "Train Epoch [4/4], Batch [81900/260984], Loss: 2.3832\n",
            "Train Epoch [4/4], Batch [82000/260984], Loss: 2.2675\n",
            "Train Epoch [4/4], Batch [82100/260984], Loss: 2.2843\n",
            "Train Epoch [4/4], Batch [82200/260984], Loss: 2.2869\n",
            "Train Epoch [4/4], Batch [82300/260984], Loss: 2.5308\n",
            "Train Epoch [4/4], Batch [82400/260984], Loss: 2.4052\n",
            "Train Epoch [4/4], Batch [82500/260984], Loss: 2.1866\n",
            "Train Epoch [4/4], Batch [82600/260984], Loss: 2.3394\n",
            "Train Epoch [4/4], Batch [82700/260984], Loss: 2.3935\n",
            "Train Epoch [4/4], Batch [82800/260984], Loss: 2.4376\n",
            "Train Epoch [4/4], Batch [82900/260984], Loss: 2.3583\n",
            "Train Epoch [4/4], Batch [83000/260984], Loss: 2.4471\n",
            "Train Epoch [4/4], Batch [83100/260984], Loss: 2.4959\n",
            "Train Epoch [4/4], Batch [83200/260984], Loss: 2.3510\n",
            "Train Epoch [4/4], Batch [83300/260984], Loss: 2.3671\n",
            "Train Epoch [4/4], Batch [83400/260984], Loss: 2.1803\n",
            "Train Epoch [4/4], Batch [83500/260984], Loss: 2.4505\n",
            "Train Epoch [4/4], Batch [83600/260984], Loss: 2.2052\n",
            "Train Epoch [4/4], Batch [83700/260984], Loss: 2.2697\n",
            "Train Epoch [4/4], Batch [83800/260984], Loss: 2.3685\n",
            "Train Epoch [4/4], Batch [83900/260984], Loss: 2.1434\n",
            "Train Epoch [4/4], Batch [84000/260984], Loss: 2.3678\n",
            "Train Epoch [4/4], Batch [84100/260984], Loss: 2.4631\n",
            "Train Epoch [4/4], Batch [84200/260984], Loss: 2.3371\n",
            "Train Epoch [4/4], Batch [84300/260984], Loss: 2.1087\n",
            "Train Epoch [4/4], Batch [84400/260984], Loss: 2.3571\n",
            "Train Epoch [4/4], Batch [84500/260984], Loss: 2.4310\n",
            "Train Epoch [4/4], Batch [84600/260984], Loss: 2.1796\n",
            "Train Epoch [4/4], Batch [84700/260984], Loss: 2.2796\n",
            "Train Epoch [4/4], Batch [84800/260984], Loss: 2.2819\n",
            "Train Epoch [4/4], Batch [84900/260984], Loss: 2.3157\n",
            "Train Epoch [4/4], Batch [85000/260984], Loss: 2.2366\n",
            "Train Epoch [4/4], Batch [85100/260984], Loss: 2.2632\n",
            "Train Epoch [4/4], Batch [85200/260984], Loss: 2.1222\n",
            "Train Epoch [4/4], Batch [85300/260984], Loss: 2.3857\n",
            "Train Epoch [4/4], Batch [85400/260984], Loss: 2.3300\n",
            "Train Epoch [4/4], Batch [85500/260984], Loss: 2.3033\n",
            "Train Epoch [4/4], Batch [85600/260984], Loss: 2.1674\n",
            "Train Epoch [4/4], Batch [85700/260984], Loss: 2.2759\n",
            "Train Epoch [4/4], Batch [85800/260984], Loss: 2.1847\n",
            "Train Epoch [4/4], Batch [85900/260984], Loss: 2.3708\n",
            "Train Epoch [4/4], Batch [86000/260984], Loss: 2.2806\n",
            "Train Epoch [4/4], Batch [86100/260984], Loss: 2.2613\n",
            "Train Epoch [4/4], Batch [86200/260984], Loss: 2.2883\n",
            "Train Epoch [4/4], Batch [86300/260984], Loss: 2.4026\n",
            "Train Epoch [4/4], Batch [86400/260984], Loss: 2.3508\n",
            "Train Epoch [4/4], Batch [86500/260984], Loss: 2.3491\n",
            "Train Epoch [4/4], Batch [86600/260984], Loss: 2.3265\n",
            "Train Epoch [4/4], Batch [86700/260984], Loss: 2.2537\n",
            "Train Epoch [4/4], Batch [86800/260984], Loss: 2.1902\n",
            "Train Epoch [4/4], Batch [86900/260984], Loss: 2.4133\n",
            "Train Epoch [4/4], Batch [87000/260984], Loss: 2.2755\n",
            "Train Epoch [4/4], Batch [87100/260984], Loss: 2.3074\n",
            "Train Epoch [4/4], Batch [87200/260984], Loss: 2.2500\n",
            "Train Epoch [4/4], Batch [87300/260984], Loss: 2.0784\n",
            "Train Epoch [4/4], Batch [87400/260984], Loss: 2.4023\n",
            "Train Epoch [4/4], Batch [87500/260984], Loss: 2.3763\n",
            "Train Epoch [4/4], Batch [87600/260984], Loss: 2.3236\n",
            "Train Epoch [4/4], Batch [87700/260984], Loss: 2.1826\n",
            "Train Epoch [4/4], Batch [87800/260984], Loss: 2.3434\n",
            "Train Epoch [4/4], Batch [87900/260984], Loss: 2.2621\n",
            "Train Epoch [4/4], Batch [88000/260984], Loss: 2.4965\n",
            "Train Epoch [4/4], Batch [88100/260984], Loss: 2.3367\n",
            "Train Epoch [4/4], Batch [88200/260984], Loss: 2.1585\n",
            "Train Epoch [4/4], Batch [88300/260984], Loss: 2.2838\n",
            "Train Epoch [4/4], Batch [88400/260984], Loss: 2.4073\n",
            "Train Epoch [4/4], Batch [88500/260984], Loss: 2.4146\n",
            "Train Epoch [4/4], Batch [88600/260984], Loss: 2.3214\n",
            "Train Epoch [4/4], Batch [88700/260984], Loss: 2.2210\n",
            "Train Epoch [4/4], Batch [88800/260984], Loss: 2.4024\n",
            "Train Epoch [4/4], Batch [88900/260984], Loss: 2.4383\n",
            "Train Epoch [4/4], Batch [89000/260984], Loss: 2.3158\n",
            "Train Epoch [4/4], Batch [89100/260984], Loss: 2.3378\n",
            "Train Epoch [4/4], Batch [89200/260984], Loss: 2.4502\n",
            "Train Epoch [4/4], Batch [89300/260984], Loss: 2.2048\n",
            "Train Epoch [4/4], Batch [89400/260984], Loss: 2.2540\n",
            "Train Epoch [4/4], Batch [89500/260984], Loss: 2.3772\n",
            "Train Epoch [4/4], Batch [89600/260984], Loss: 2.2363\n",
            "Train Epoch [4/4], Batch [89700/260984], Loss: 2.1619\n",
            "Train Epoch [4/4], Batch [89800/260984], Loss: 2.0971\n",
            "Train Epoch [4/4], Batch [89900/260984], Loss: 2.3730\n",
            "Train Epoch [4/4], Batch [90000/260984], Loss: 2.3115\n",
            "Train Epoch [4/4], Batch [90100/260984], Loss: 2.3001\n",
            "Train Epoch [4/4], Batch [90200/260984], Loss: 2.4131\n",
            "Train Epoch [4/4], Batch [90300/260984], Loss: 2.2520\n",
            "Train Epoch [4/4], Batch [90400/260984], Loss: 2.1830\n",
            "Train Epoch [4/4], Batch [90500/260984], Loss: 2.3201\n",
            "Train Epoch [4/4], Batch [90600/260984], Loss: 2.4053\n",
            "Train Epoch [4/4], Batch [90700/260984], Loss: 2.2435\n",
            "Train Epoch [4/4], Batch [90800/260984], Loss: 2.1908\n",
            "Train Epoch [4/4], Batch [90900/260984], Loss: 2.2101\n",
            "Train Epoch [4/4], Batch [91000/260984], Loss: 2.5404\n",
            "Train Epoch [4/4], Batch [91100/260984], Loss: 2.3330\n",
            "Train Epoch [4/4], Batch [91200/260984], Loss: 2.4083\n",
            "Train Epoch [4/4], Batch [91300/260984], Loss: 2.3431\n",
            "Train Epoch [4/4], Batch [91400/260984], Loss: 2.3483\n",
            "Train Epoch [4/4], Batch [91500/260984], Loss: 2.3536\n",
            "Train Epoch [4/4], Batch [91600/260984], Loss: 2.3976\n",
            "Train Epoch [4/4], Batch [91700/260984], Loss: 2.3497\n",
            "Train Epoch [4/4], Batch [91800/260984], Loss: 2.2469\n",
            "Train Epoch [4/4], Batch [91900/260984], Loss: 2.1968\n",
            "Train Epoch [4/4], Batch [92000/260984], Loss: 2.3764\n",
            "Train Epoch [4/4], Batch [92100/260984], Loss: 2.3453\n",
            "Train Epoch [4/4], Batch [92200/260984], Loss: 2.3855\n",
            "Train Epoch [4/4], Batch [92300/260984], Loss: 2.2912\n",
            "Train Epoch [4/4], Batch [92400/260984], Loss: 2.4739\n",
            "Train Epoch [4/4], Batch [92500/260984], Loss: 2.2231\n",
            "Train Epoch [4/4], Batch [92600/260984], Loss: 2.1570\n",
            "Train Epoch [4/4], Batch [92700/260984], Loss: 2.2217\n",
            "Train Epoch [4/4], Batch [92800/260984], Loss: 2.5426\n",
            "Train Epoch [4/4], Batch [92900/260984], Loss: 2.2748\n",
            "Train Epoch [4/4], Batch [93000/260984], Loss: 2.2697\n",
            "Train Epoch [4/4], Batch [93100/260984], Loss: 2.3151\n",
            "Train Epoch [4/4], Batch [93200/260984], Loss: 2.4086\n",
            "Train Epoch [4/4], Batch [93300/260984], Loss: 2.2576\n",
            "Train Epoch [4/4], Batch [93400/260984], Loss: 2.3847\n",
            "Train Epoch [4/4], Batch [93500/260984], Loss: 2.1921\n",
            "Train Epoch [4/4], Batch [93600/260984], Loss: 2.2705\n",
            "Train Epoch [4/4], Batch [93700/260984], Loss: 2.2583\n",
            "Train Epoch [4/4], Batch [93800/260984], Loss: 2.2808\n",
            "Train Epoch [4/4], Batch [93900/260984], Loss: 2.1963\n",
            "Train Epoch [4/4], Batch [94000/260984], Loss: 2.1768\n",
            "Train Epoch [4/4], Batch [94100/260984], Loss: 2.3513\n",
            "Train Epoch [4/4], Batch [94200/260984], Loss: 2.2706\n",
            "Train Epoch [4/4], Batch [94300/260984], Loss: 2.3113\n",
            "Train Epoch [4/4], Batch [94400/260984], Loss: 2.1907\n",
            "Train Epoch [4/4], Batch [94500/260984], Loss: 2.3213\n",
            "Train Epoch [4/4], Batch [94600/260984], Loss: 2.3442\n",
            "Train Epoch [4/4], Batch [94700/260984], Loss: 2.3862\n",
            "Train Epoch [4/4], Batch [94800/260984], Loss: 2.4122\n",
            "Train Epoch [4/4], Batch [94900/260984], Loss: 2.3507\n",
            "Train Epoch [4/4], Batch [95000/260984], Loss: 2.2580\n",
            "Train Epoch [4/4], Batch [95100/260984], Loss: 2.3300\n",
            "Train Epoch [4/4], Batch [95200/260984], Loss: 2.2927\n",
            "Train Epoch [4/4], Batch [95300/260984], Loss: 2.4298\n",
            "Train Epoch [4/4], Batch [95400/260984], Loss: 2.3498\n",
            "Train Epoch [4/4], Batch [95500/260984], Loss: 2.2558\n",
            "Train Epoch [4/4], Batch [95600/260984], Loss: 2.2055\n",
            "Train Epoch [4/4], Batch [95700/260984], Loss: 2.2428\n",
            "Train Epoch [4/4], Batch [95800/260984], Loss: 2.3401\n",
            "Train Epoch [4/4], Batch [95900/260984], Loss: 2.2159\n",
            "Train Epoch [4/4], Batch [96000/260984], Loss: 2.2812\n",
            "Train Epoch [4/4], Batch [96100/260984], Loss: 2.3738\n",
            "Train Epoch [4/4], Batch [96200/260984], Loss: 2.2009\n",
            "Train Epoch [4/4], Batch [96300/260984], Loss: 2.2689\n",
            "Train Epoch [4/4], Batch [96400/260984], Loss: 2.2142\n",
            "Train Epoch [4/4], Batch [96500/260984], Loss: 2.4382\n",
            "Train Epoch [4/4], Batch [96600/260984], Loss: 2.2873\n",
            "Train Epoch [4/4], Batch [96700/260984], Loss: 2.2266\n",
            "Train Epoch [4/4], Batch [96800/260984], Loss: 2.5576\n",
            "Train Epoch [4/4], Batch [96900/260984], Loss: 2.2306\n",
            "Train Epoch [4/4], Batch [97000/260984], Loss: 2.2482\n",
            "Train Epoch [4/4], Batch [97100/260984], Loss: 2.2697\n",
            "Train Epoch [4/4], Batch [97200/260984], Loss: 2.1910\n",
            "Train Epoch [4/4], Batch [97300/260984], Loss: 2.2193\n",
            "Train Epoch [4/4], Batch [97400/260984], Loss: 2.2599\n",
            "Train Epoch [4/4], Batch [97500/260984], Loss: 2.4304\n",
            "Train Epoch [4/4], Batch [97600/260984], Loss: 2.2224\n",
            "Train Epoch [4/4], Batch [97700/260984], Loss: 2.1657\n",
            "Train Epoch [4/4], Batch [97800/260984], Loss: 2.3780\n",
            "Train Epoch [4/4], Batch [97900/260984], Loss: 2.2309\n",
            "Train Epoch [4/4], Batch [98000/260984], Loss: 2.4179\n",
            "Train Epoch [4/4], Batch [98100/260984], Loss: 2.2968\n",
            "Train Epoch [4/4], Batch [98200/260984], Loss: 2.1210\n",
            "Train Epoch [4/4], Batch [98300/260984], Loss: 2.3374\n",
            "Train Epoch [4/4], Batch [98400/260984], Loss: 2.2024\n",
            "Train Epoch [4/4], Batch [98500/260984], Loss: 2.3160\n",
            "Train Epoch [4/4], Batch [98600/260984], Loss: 2.2511\n",
            "Train Epoch [4/4], Batch [98700/260984], Loss: 2.2761\n",
            "Train Epoch [4/4], Batch [98800/260984], Loss: 2.2600\n",
            "Train Epoch [4/4], Batch [98900/260984], Loss: 2.1986\n",
            "Train Epoch [4/4], Batch [99000/260984], Loss: 2.3829\n",
            "Train Epoch [4/4], Batch [99100/260984], Loss: 2.3583\n",
            "Train Epoch [4/4], Batch [99200/260984], Loss: 2.4020\n",
            "Train Epoch [4/4], Batch [99300/260984], Loss: 2.0876\n",
            "Train Epoch [4/4], Batch [99400/260984], Loss: 2.2854\n",
            "Train Epoch [4/4], Batch [99500/260984], Loss: 2.1864\n",
            "Train Epoch [4/4], Batch [99600/260984], Loss: 2.3138\n",
            "Train Epoch [4/4], Batch [99700/260984], Loss: 2.3811\n",
            "Train Epoch [4/4], Batch [99800/260984], Loss: 2.3638\n",
            "Train Epoch [4/4], Batch [99900/260984], Loss: 2.2945\n",
            "Train Epoch [4/4], Batch [100000/260984], Loss: 2.3780\n",
            "Train Epoch [4/4], Batch [100100/260984], Loss: 2.3181\n",
            "Train Epoch [4/4], Batch [100200/260984], Loss: 2.2098\n",
            "Train Epoch [4/4], Batch [100300/260984], Loss: 2.2456\n",
            "Train Epoch [4/4], Batch [100400/260984], Loss: 2.2844\n",
            "Train Epoch [4/4], Batch [100500/260984], Loss: 2.2924\n",
            "Train Epoch [4/4], Batch [100600/260984], Loss: 2.4011\n",
            "Train Epoch [4/4], Batch [100700/260984], Loss: 2.4497\n",
            "Train Epoch [4/4], Batch [100800/260984], Loss: 2.2902\n",
            "Train Epoch [4/4], Batch [100900/260984], Loss: 2.3423\n",
            "Train Epoch [4/4], Batch [101000/260984], Loss: 2.3289\n",
            "Train Epoch [4/4], Batch [101100/260984], Loss: 2.3209\n",
            "Train Epoch [4/4], Batch [101200/260984], Loss: 2.3254\n",
            "Train Epoch [4/4], Batch [101300/260984], Loss: 2.3371\n",
            "Train Epoch [4/4], Batch [101400/260984], Loss: 2.2001\n",
            "Train Epoch [4/4], Batch [101500/260984], Loss: 2.4222\n",
            "Train Epoch [4/4], Batch [101600/260984], Loss: 2.4066\n",
            "Train Epoch [4/4], Batch [101700/260984], Loss: 2.2872\n",
            "Train Epoch [4/4], Batch [101800/260984], Loss: 2.3456\n",
            "Train Epoch [4/4], Batch [101900/260984], Loss: 2.2553\n",
            "Train Epoch [4/4], Batch [102000/260984], Loss: 2.4237\n",
            "Train Epoch [4/4], Batch [102100/260984], Loss: 2.3518\n",
            "Train Epoch [4/4], Batch [102200/260984], Loss: 2.1661\n",
            "Train Epoch [4/4], Batch [102300/260984], Loss: 2.2914\n",
            "Train Epoch [4/4], Batch [102400/260984], Loss: 2.2871\n",
            "Train Epoch [4/4], Batch [102500/260984], Loss: 2.2531\n",
            "Train Epoch [4/4], Batch [102600/260984], Loss: 2.2128\n",
            "Train Epoch [4/4], Batch [102700/260984], Loss: 2.2780\n",
            "Train Epoch [4/4], Batch [102800/260984], Loss: 2.1657\n",
            "Train Epoch [4/4], Batch [102900/260984], Loss: 2.2267\n",
            "Train Epoch [4/4], Batch [103000/260984], Loss: 2.2312\n",
            "Train Epoch [4/4], Batch [103100/260984], Loss: 2.4336\n",
            "Train Epoch [4/4], Batch [103200/260984], Loss: 2.3150\n",
            "Train Epoch [4/4], Batch [103300/260984], Loss: 2.3489\n",
            "Train Epoch [4/4], Batch [103400/260984], Loss: 2.2568\n",
            "Train Epoch [4/4], Batch [103500/260984], Loss: 2.4379\n",
            "Train Epoch [4/4], Batch [103600/260984], Loss: 2.3442\n",
            "Train Epoch [4/4], Batch [103700/260984], Loss: 2.4454\n",
            "Train Epoch [4/4], Batch [103800/260984], Loss: 2.3544\n",
            "Train Epoch [4/4], Batch [103900/260984], Loss: 2.3036\n",
            "Train Epoch [4/4], Batch [104000/260984], Loss: 2.2872\n",
            "Train Epoch [4/4], Batch [104100/260984], Loss: 2.1484\n",
            "Train Epoch [4/4], Batch [104200/260984], Loss: 2.3438\n",
            "Train Epoch [4/4], Batch [104300/260984], Loss: 2.2898\n",
            "Train Epoch [4/4], Batch [104400/260984], Loss: 2.1958\n",
            "Train Epoch [4/4], Batch [104500/260984], Loss: 2.2513\n",
            "Train Epoch [4/4], Batch [104600/260984], Loss: 2.1884\n",
            "Train Epoch [4/4], Batch [104700/260984], Loss: 2.3233\n",
            "Train Epoch [4/4], Batch [104800/260984], Loss: 2.2180\n",
            "Train Epoch [4/4], Batch [104900/260984], Loss: 2.1947\n",
            "Train Epoch [4/4], Batch [105000/260984], Loss: 2.3405\n",
            "Train Epoch [4/4], Batch [105100/260984], Loss: 2.2245\n",
            "Train Epoch [4/4], Batch [105200/260984], Loss: 2.3139\n",
            "Train Epoch [4/4], Batch [105300/260984], Loss: 2.2830\n",
            "Train Epoch [4/4], Batch [105400/260984], Loss: 2.3148\n",
            "Train Epoch [4/4], Batch [105500/260984], Loss: 2.4713\n",
            "Train Epoch [4/4], Batch [105600/260984], Loss: 2.3986\n",
            "Train Epoch [4/4], Batch [105700/260984], Loss: 2.4413\n",
            "Train Epoch [4/4], Batch [105800/260984], Loss: 2.3818\n",
            "Train Epoch [4/4], Batch [105900/260984], Loss: 2.3523\n",
            "Train Epoch [4/4], Batch [106000/260984], Loss: 2.3854\n",
            "Train Epoch [4/4], Batch [106100/260984], Loss: 2.3158\n",
            "Train Epoch [4/4], Batch [106200/260984], Loss: 2.4399\n",
            "Train Epoch [4/4], Batch [106300/260984], Loss: 2.3058\n",
            "Train Epoch [4/4], Batch [106400/260984], Loss: 2.3467\n",
            "Train Epoch [4/4], Batch [106500/260984], Loss: 2.4320\n",
            "Train Epoch [4/4], Batch [106600/260984], Loss: 2.4652\n",
            "Train Epoch [4/4], Batch [106700/260984], Loss: 2.1808\n",
            "Train Epoch [4/4], Batch [106800/260984], Loss: 2.2881\n",
            "Train Epoch [4/4], Batch [106900/260984], Loss: 2.3774\n",
            "Train Epoch [4/4], Batch [107000/260984], Loss: 2.3130\n",
            "Train Epoch [4/4], Batch [107100/260984], Loss: 2.2525\n",
            "Train Epoch [4/4], Batch [107200/260984], Loss: 2.3515\n",
            "Train Epoch [4/4], Batch [107300/260984], Loss: 2.1642\n",
            "Train Epoch [4/4], Batch [107400/260984], Loss: 2.4151\n",
            "Train Epoch [4/4], Batch [107500/260984], Loss: 2.2023\n",
            "Train Epoch [4/4], Batch [107600/260984], Loss: 2.3726\n",
            "Train Epoch [4/4], Batch [107700/260984], Loss: 2.2496\n",
            "Train Epoch [4/4], Batch [107800/260984], Loss: 2.2373\n",
            "Train Epoch [4/4], Batch [107900/260984], Loss: 2.4511\n",
            "Train Epoch [4/4], Batch [108000/260984], Loss: 2.4135\n",
            "Train Epoch [4/4], Batch [108100/260984], Loss: 2.4624\n",
            "Train Epoch [4/4], Batch [108200/260984], Loss: 2.4117\n",
            "Train Epoch [4/4], Batch [108300/260984], Loss: 2.3683\n",
            "Train Epoch [4/4], Batch [108400/260984], Loss: 2.5194\n",
            "Train Epoch [4/4], Batch [108500/260984], Loss: 2.3562\n",
            "Train Epoch [4/4], Batch [108600/260984], Loss: 2.3524\n",
            "Train Epoch [4/4], Batch [108700/260984], Loss: 2.4059\n",
            "Train Epoch [4/4], Batch [108800/260984], Loss: 2.4110\n",
            "Train Epoch [4/4], Batch [108900/260984], Loss: 2.1334\n",
            "Train Epoch [4/4], Batch [109000/260984], Loss: 2.3777\n",
            "Train Epoch [4/4], Batch [109100/260984], Loss: 2.3160\n",
            "Train Epoch [4/4], Batch [109200/260984], Loss: 2.2242\n",
            "Train Epoch [4/4], Batch [109300/260984], Loss: 2.2841\n",
            "Train Epoch [4/4], Batch [109400/260984], Loss: 2.1395\n",
            "Train Epoch [4/4], Batch [109500/260984], Loss: 2.1599\n",
            "Train Epoch [4/4], Batch [109600/260984], Loss: 2.4041\n",
            "Train Epoch [4/4], Batch [109700/260984], Loss: 2.2194\n",
            "Train Epoch [4/4], Batch [109800/260984], Loss: 2.3273\n",
            "Train Epoch [4/4], Batch [109900/260984], Loss: 2.3106\n",
            "Train Epoch [4/4], Batch [110000/260984], Loss: 2.3410\n",
            "Train Epoch [4/4], Batch [110100/260984], Loss: 2.2867\n",
            "Train Epoch [4/4], Batch [110200/260984], Loss: 2.2230\n",
            "Train Epoch [4/4], Batch [110300/260984], Loss: 2.3442\n",
            "Train Epoch [4/4], Batch [110400/260984], Loss: 2.2849\n",
            "Train Epoch [4/4], Batch [110500/260984], Loss: 2.3208\n",
            "Train Epoch [4/4], Batch [110600/260984], Loss: 2.2795\n",
            "Train Epoch [4/4], Batch [110700/260984], Loss: 2.2586\n",
            "Train Epoch [4/4], Batch [110800/260984], Loss: 2.2656\n",
            "Train Epoch [4/4], Batch [110900/260984], Loss: 2.4390\n",
            "Train Epoch [4/4], Batch [111000/260984], Loss: 2.3894\n",
            "Train Epoch [4/4], Batch [111100/260984], Loss: 2.5112\n",
            "Train Epoch [4/4], Batch [111200/260984], Loss: 2.3976\n",
            "Train Epoch [4/4], Batch [111300/260984], Loss: 2.3335\n",
            "Train Epoch [4/4], Batch [111400/260984], Loss: 2.4794\n",
            "Train Epoch [4/4], Batch [111500/260984], Loss: 2.5312\n",
            "Train Epoch [4/4], Batch [111600/260984], Loss: 2.3802\n",
            "Train Epoch [4/4], Batch [111700/260984], Loss: 2.3487\n",
            "Train Epoch [4/4], Batch [111800/260984], Loss: 2.2902\n",
            "Train Epoch [4/4], Batch [111900/260984], Loss: 2.4108\n",
            "Train Epoch [4/4], Batch [112000/260984], Loss: 2.3300\n",
            "Train Epoch [4/4], Batch [112100/260984], Loss: 2.2132\n",
            "Train Epoch [4/4], Batch [112200/260984], Loss: 2.2858\n",
            "Train Epoch [4/4], Batch [112300/260984], Loss: 2.0217\n",
            "Train Epoch [4/4], Batch [112400/260984], Loss: 2.1978\n",
            "Train Epoch [4/4], Batch [112500/260984], Loss: 2.2564\n",
            "Train Epoch [4/4], Batch [112600/260984], Loss: 2.2633\n",
            "Train Epoch [4/4], Batch [112700/260984], Loss: 2.2781\n",
            "Train Epoch [4/4], Batch [112800/260984], Loss: 2.3502\n",
            "Train Epoch [4/4], Batch [112900/260984], Loss: 2.1566\n",
            "Train Epoch [4/4], Batch [113000/260984], Loss: 2.4082\n",
            "Train Epoch [4/4], Batch [113100/260984], Loss: 2.3825\n",
            "Train Epoch [4/4], Batch [113200/260984], Loss: 2.1120\n",
            "Train Epoch [4/4], Batch [113300/260984], Loss: 2.3486\n",
            "Train Epoch [4/4], Batch [113400/260984], Loss: 2.3393\n",
            "Train Epoch [4/4], Batch [113500/260984], Loss: 2.2924\n",
            "Train Epoch [4/4], Batch [113600/260984], Loss: 2.3704\n",
            "Train Epoch [4/4], Batch [113700/260984], Loss: 2.3705\n",
            "Train Epoch [4/4], Batch [113800/260984], Loss: 2.2899\n",
            "Train Epoch [4/4], Batch [113900/260984], Loss: 2.4684\n",
            "Train Epoch [4/4], Batch [114000/260984], Loss: 2.2493\n",
            "Train Epoch [4/4], Batch [114100/260984], Loss: 2.3861\n",
            "Train Epoch [4/4], Batch [114200/260984], Loss: 2.3135\n",
            "Train Epoch [4/4], Batch [114300/260984], Loss: 2.2704\n",
            "Train Epoch [4/4], Batch [114400/260984], Loss: 2.4069\n",
            "Train Epoch [4/4], Batch [114500/260984], Loss: 2.5033\n",
            "Train Epoch [4/4], Batch [114600/260984], Loss: 2.5269\n",
            "Train Epoch [4/4], Batch [114700/260984], Loss: 2.1661\n",
            "Train Epoch [4/4], Batch [114800/260984], Loss: 2.2534\n",
            "Train Epoch [4/4], Batch [114900/260984], Loss: 2.2606\n",
            "Train Epoch [4/4], Batch [115000/260984], Loss: 2.3035\n",
            "Train Epoch [4/4], Batch [115100/260984], Loss: 2.4935\n",
            "Train Epoch [4/4], Batch [115200/260984], Loss: 2.3537\n",
            "Train Epoch [4/4], Batch [115300/260984], Loss: 2.3589\n",
            "Train Epoch [4/4], Batch [115400/260984], Loss: 2.3415\n",
            "Train Epoch [4/4], Batch [115500/260984], Loss: 2.3768\n",
            "Train Epoch [4/4], Batch [115600/260984], Loss: 2.2751\n",
            "Train Epoch [4/4], Batch [115700/260984], Loss: 2.4247\n",
            "Train Epoch [4/4], Batch [115800/260984], Loss: 2.2511\n",
            "Train Epoch [4/4], Batch [115900/260984], Loss: 2.2161\n",
            "Train Epoch [4/4], Batch [116000/260984], Loss: 2.4234\n",
            "Train Epoch [4/4], Batch [116100/260984], Loss: 2.2885\n",
            "Train Epoch [4/4], Batch [116200/260984], Loss: 2.1662\n",
            "Train Epoch [4/4], Batch [116300/260984], Loss: 2.1314\n",
            "Train Epoch [4/4], Batch [116400/260984], Loss: 2.3469\n",
            "Train Epoch [4/4], Batch [116500/260984], Loss: 2.2751\n",
            "Train Epoch [4/4], Batch [116600/260984], Loss: 2.2418\n",
            "Train Epoch [4/4], Batch [116700/260984], Loss: 2.4409\n",
            "Train Epoch [4/4], Batch [116800/260984], Loss: 2.2262\n",
            "Train Epoch [4/4], Batch [116900/260984], Loss: 2.2488\n",
            "Train Epoch [4/4], Batch [117000/260984], Loss: 2.3109\n",
            "Train Epoch [4/4], Batch [117100/260984], Loss: 2.3950\n",
            "Train Epoch [4/4], Batch [117200/260984], Loss: 2.3330\n",
            "Train Epoch [4/4], Batch [117300/260984], Loss: 2.3000\n",
            "Train Epoch [4/4], Batch [117400/260984], Loss: 2.4056\n",
            "Train Epoch [4/4], Batch [117500/260984], Loss: 2.1833\n",
            "Train Epoch [4/4], Batch [117600/260984], Loss: 2.2947\n",
            "Train Epoch [4/4], Batch [117700/260984], Loss: 2.2462\n",
            "Train Epoch [4/4], Batch [117800/260984], Loss: 2.2254\n",
            "Train Epoch [4/4], Batch [117900/260984], Loss: 2.2831\n",
            "Train Epoch [4/4], Batch [118000/260984], Loss: 2.3732\n",
            "Train Epoch [4/4], Batch [118100/260984], Loss: 2.2110\n",
            "Train Epoch [4/4], Batch [118200/260984], Loss: 2.1740\n",
            "Train Epoch [4/4], Batch [118300/260984], Loss: 2.4795\n",
            "Train Epoch [4/4], Batch [118400/260984], Loss: 2.2809\n",
            "Train Epoch [4/4], Batch [118500/260984], Loss: 2.2887\n",
            "Train Epoch [4/4], Batch [118600/260984], Loss: 2.2428\n",
            "Train Epoch [4/4], Batch [118700/260984], Loss: 2.3468\n",
            "Train Epoch [4/4], Batch [118800/260984], Loss: 2.4213\n",
            "Train Epoch [4/4], Batch [118900/260984], Loss: 2.2540\n",
            "Train Epoch [4/4], Batch [119000/260984], Loss: 2.3455\n",
            "Train Epoch [4/4], Batch [119100/260984], Loss: 2.4187\n",
            "Train Epoch [4/4], Batch [119200/260984], Loss: 2.1625\n",
            "Train Epoch [4/4], Batch [119300/260984], Loss: 2.4071\n",
            "Train Epoch [4/4], Batch [119400/260984], Loss: 2.1911\n",
            "Train Epoch [4/4], Batch [119500/260984], Loss: 2.3007\n",
            "Train Epoch [4/4], Batch [119600/260984], Loss: 2.3644\n",
            "Train Epoch [4/4], Batch [119700/260984], Loss: 2.2640\n",
            "Train Epoch [4/4], Batch [119800/260984], Loss: 2.1593\n",
            "Train Epoch [4/4], Batch [119900/260984], Loss: 2.0516\n",
            "Train Epoch [4/4], Batch [120000/260984], Loss: 2.2111\n",
            "Train Epoch [4/4], Batch [120100/260984], Loss: 2.3414\n",
            "Train Epoch [4/4], Batch [120200/260984], Loss: 2.2673\n",
            "Train Epoch [4/4], Batch [120300/260984], Loss: 2.4283\n",
            "Train Epoch [4/4], Batch [120400/260984], Loss: 2.2004\n",
            "Train Epoch [4/4], Batch [120500/260984], Loss: 2.3767\n",
            "Train Epoch [4/4], Batch [120600/260984], Loss: 2.1997\n",
            "Train Epoch [4/4], Batch [120700/260984], Loss: 2.2173\n",
            "Train Epoch [4/4], Batch [120800/260984], Loss: 2.2994\n",
            "Train Epoch [4/4], Batch [120900/260984], Loss: 2.3593\n",
            "Train Epoch [4/4], Batch [121000/260984], Loss: 2.3625\n",
            "Train Epoch [4/4], Batch [121100/260984], Loss: 2.3152\n",
            "Train Epoch [4/4], Batch [121200/260984], Loss: 2.2613\n",
            "Train Epoch [4/4], Batch [121300/260984], Loss: 2.2769\n",
            "Train Epoch [4/4], Batch [121400/260984], Loss: 2.3284\n",
            "Train Epoch [4/4], Batch [121500/260984], Loss: 2.3576\n",
            "Train Epoch [4/4], Batch [121600/260984], Loss: 2.3155\n",
            "Train Epoch [4/4], Batch [121700/260984], Loss: 2.3599\n",
            "Train Epoch [4/4], Batch [121800/260984], Loss: 2.0863\n",
            "Train Epoch [4/4], Batch [121900/260984], Loss: 2.2868\n",
            "Train Epoch [4/4], Batch [122000/260984], Loss: 2.2584\n",
            "Train Epoch [4/4], Batch [122100/260984], Loss: 2.2578\n",
            "Train Epoch [4/4], Batch [122200/260984], Loss: 2.1011\n",
            "Train Epoch [4/4], Batch [122300/260984], Loss: 2.2482\n",
            "Train Epoch [4/4], Batch [122400/260984], Loss: 2.5328\n",
            "Train Epoch [4/4], Batch [122500/260984], Loss: 2.3134\n",
            "Train Epoch [4/4], Batch [122600/260984], Loss: 2.3375\n",
            "Train Epoch [4/4], Batch [122700/260984], Loss: 2.2878\n",
            "Train Epoch [4/4], Batch [122800/260984], Loss: 2.2590\n",
            "Train Epoch [4/4], Batch [122900/260984], Loss: 2.3378\n",
            "Train Epoch [4/4], Batch [123000/260984], Loss: 2.3183\n",
            "Train Epoch [4/4], Batch [123100/260984], Loss: 2.3233\n",
            "Train Epoch [4/4], Batch [123200/260984], Loss: 2.3076\n",
            "Train Epoch [4/4], Batch [123300/260984], Loss: 2.3662\n",
            "Train Epoch [4/4], Batch [123400/260984], Loss: 2.2801\n",
            "Train Epoch [4/4], Batch [123500/260984], Loss: 2.2623\n",
            "Train Epoch [4/4], Batch [123600/260984], Loss: 2.3861\n",
            "Train Epoch [4/4], Batch [123700/260984], Loss: 2.2519\n",
            "Train Epoch [4/4], Batch [123800/260984], Loss: 2.1646\n",
            "Train Epoch [4/4], Batch [123900/260984], Loss: 2.4703\n",
            "Train Epoch [4/4], Batch [124000/260984], Loss: 2.3222\n",
            "Train Epoch [4/4], Batch [124100/260984], Loss: 2.2320\n",
            "Train Epoch [4/4], Batch [124200/260984], Loss: 2.4059\n",
            "Train Epoch [4/4], Batch [124300/260984], Loss: 2.2497\n",
            "Train Epoch [4/4], Batch [124400/260984], Loss: 2.2237\n",
            "Train Epoch [4/4], Batch [124500/260984], Loss: 2.2389\n",
            "Train Epoch [4/4], Batch [124600/260984], Loss: 2.3101\n",
            "Train Epoch [4/4], Batch [124700/260984], Loss: 2.4255\n",
            "Train Epoch [4/4], Batch [124800/260984], Loss: 2.2831\n",
            "Train Epoch [4/4], Batch [124900/260984], Loss: 2.1953\n",
            "Train Epoch [4/4], Batch [125000/260984], Loss: 2.2431\n",
            "Train Epoch [4/4], Batch [125100/260984], Loss: 2.4573\n",
            "Train Epoch [4/4], Batch [125200/260984], Loss: 2.3153\n",
            "Train Epoch [4/4], Batch [125300/260984], Loss: 2.1572\n",
            "Train Epoch [4/4], Batch [125400/260984], Loss: 2.3597\n",
            "Train Epoch [4/4], Batch [125500/260984], Loss: 2.2601\n",
            "Train Epoch [4/4], Batch [125600/260984], Loss: 2.4163\n",
            "Train Epoch [4/4], Batch [125700/260984], Loss: 2.3017\n",
            "Train Epoch [4/4], Batch [125800/260984], Loss: 2.4930\n",
            "Train Epoch [4/4], Batch [125900/260984], Loss: 2.3265\n",
            "Train Epoch [4/4], Batch [126000/260984], Loss: 2.3174\n",
            "Train Epoch [4/4], Batch [126100/260984], Loss: 2.3573\n",
            "Train Epoch [4/4], Batch [126200/260984], Loss: 2.3459\n",
            "Train Epoch [4/4], Batch [126300/260984], Loss: 2.3279\n",
            "Train Epoch [4/4], Batch [126400/260984], Loss: 2.2324\n",
            "Train Epoch [4/4], Batch [126500/260984], Loss: 2.2189\n",
            "Train Epoch [4/4], Batch [126600/260984], Loss: 2.3252\n",
            "Train Epoch [4/4], Batch [126700/260984], Loss: 2.3786\n",
            "Train Epoch [4/4], Batch [126800/260984], Loss: 2.3144\n",
            "Train Epoch [4/4], Batch [126900/260984], Loss: 2.2530\n",
            "Train Epoch [4/4], Batch [127000/260984], Loss: 2.2160\n",
            "Train Epoch [4/4], Batch [127100/260984], Loss: 2.3201\n",
            "Train Epoch [4/4], Batch [127200/260984], Loss: 2.3075\n",
            "Train Epoch [4/4], Batch [127300/260984], Loss: 2.4043\n",
            "Train Epoch [4/4], Batch [127400/260984], Loss: 2.4097\n",
            "Train Epoch [4/4], Batch [127500/260984], Loss: 2.2548\n",
            "Train Epoch [4/4], Batch [127600/260984], Loss: 2.3697\n",
            "Train Epoch [4/4], Batch [127700/260984], Loss: 2.4168\n",
            "Train Epoch [4/4], Batch [127800/260984], Loss: 2.2643\n",
            "Train Epoch [4/4], Batch [127900/260984], Loss: 2.2229\n",
            "Train Epoch [4/4], Batch [128000/260984], Loss: 2.1997\n",
            "Train Epoch [4/4], Batch [128100/260984], Loss: 2.3492\n",
            "Train Epoch [4/4], Batch [128200/260984], Loss: 2.1054\n",
            "Train Epoch [4/4], Batch [128300/260984], Loss: 2.3304\n",
            "Train Epoch [4/4], Batch [128400/260984], Loss: 2.3165\n",
            "Train Epoch [4/4], Batch [128500/260984], Loss: 2.2513\n",
            "Train Epoch [4/4], Batch [128600/260984], Loss: 2.2785\n",
            "Train Epoch [4/4], Batch [128700/260984], Loss: 2.3235\n",
            "Train Epoch [4/4], Batch [128800/260984], Loss: 2.3201\n",
            "Train Epoch [4/4], Batch [128900/260984], Loss: 2.1586\n",
            "Train Epoch [4/4], Batch [129000/260984], Loss: 2.4131\n",
            "Train Epoch [4/4], Batch [129100/260984], Loss: 2.1631\n",
            "Train Epoch [4/4], Batch [129200/260984], Loss: 2.3696\n",
            "Train Epoch [4/4], Batch [129300/260984], Loss: 2.3042\n",
            "Train Epoch [4/4], Batch [129400/260984], Loss: 2.3454\n",
            "Train Epoch [4/4], Batch [129500/260984], Loss: 2.3946\n",
            "Train Epoch [4/4], Batch [129600/260984], Loss: 2.2428\n",
            "Train Epoch [4/4], Batch [129700/260984], Loss: 2.2857\n",
            "Train Epoch [4/4], Batch [129800/260984], Loss: 2.3125\n",
            "Train Epoch [4/4], Batch [129900/260984], Loss: 2.2133\n",
            "Train Epoch [4/4], Batch [130000/260984], Loss: 2.2346\n",
            "Train Epoch [4/4], Batch [130100/260984], Loss: 2.1963\n",
            "Train Epoch [4/4], Batch [130200/260984], Loss: 2.2143\n",
            "Train Epoch [4/4], Batch [130300/260984], Loss: 2.3137\n",
            "Train Epoch [4/4], Batch [130400/260984], Loss: 2.3803\n",
            "Train Epoch [4/4], Batch [130500/260984], Loss: 2.2330\n",
            "Train Epoch [4/4], Batch [130600/260984], Loss: 2.4730\n",
            "Train Epoch [4/4], Batch [130700/260984], Loss: 2.2171\n",
            "Train Epoch [4/4], Batch [130800/260984], Loss: 2.2439\n",
            "Train Epoch [4/4], Batch [130900/260984], Loss: 2.2838\n",
            "Train Epoch [4/4], Batch [131000/260984], Loss: 2.3434\n",
            "Train Epoch [4/4], Batch [131100/260984], Loss: 2.2362\n",
            "Train Epoch [4/4], Batch [131200/260984], Loss: 2.1576\n",
            "Train Epoch [4/4], Batch [131300/260984], Loss: 2.3377\n",
            "Train Epoch [4/4], Batch [131400/260984], Loss: 2.1818\n",
            "Train Epoch [4/4], Batch [131500/260984], Loss: 2.2576\n",
            "Train Epoch [4/4], Batch [131600/260984], Loss: 2.2258\n",
            "Train Epoch [4/4], Batch [131700/260984], Loss: 2.3112\n",
            "Train Epoch [4/4], Batch [131800/260984], Loss: 2.2621\n",
            "Train Epoch [4/4], Batch [131900/260984], Loss: 2.1902\n",
            "Train Epoch [4/4], Batch [132000/260984], Loss: 2.4026\n",
            "Train Epoch [4/4], Batch [132100/260984], Loss: 2.2096\n",
            "Train Epoch [4/4], Batch [132200/260984], Loss: 2.2677\n",
            "Train Epoch [4/4], Batch [132300/260984], Loss: 2.3277\n",
            "Train Epoch [4/4], Batch [132400/260984], Loss: 2.1676\n",
            "Train Epoch [4/4], Batch [132500/260984], Loss: 2.3349\n",
            "Train Epoch [4/4], Batch [132600/260984], Loss: 2.2740\n",
            "Train Epoch [4/4], Batch [132700/260984], Loss: 2.2841\n",
            "Train Epoch [4/4], Batch [132800/260984], Loss: 2.1949\n",
            "Train Epoch [4/4], Batch [132900/260984], Loss: 2.3144\n",
            "Train Epoch [4/4], Batch [133000/260984], Loss: 2.1975\n",
            "Train Epoch [4/4], Batch [133100/260984], Loss: 2.3822\n",
            "Train Epoch [4/4], Batch [133200/260984], Loss: 2.2418\n",
            "Train Epoch [4/4], Batch [133300/260984], Loss: 2.3780\n",
            "Train Epoch [4/4], Batch [133400/260984], Loss: 2.3061\n",
            "Train Epoch [4/4], Batch [133500/260984], Loss: 2.2619\n",
            "Train Epoch [4/4], Batch [133600/260984], Loss: 2.3715\n",
            "Train Epoch [4/4], Batch [133700/260984], Loss: 2.3104\n",
            "Train Epoch [4/4], Batch [133800/260984], Loss: 2.2627\n",
            "Train Epoch [4/4], Batch [133900/260984], Loss: 2.3654\n",
            "Train Epoch [4/4], Batch [134000/260984], Loss: 2.2945\n",
            "Train Epoch [4/4], Batch [134100/260984], Loss: 2.4704\n",
            "Train Epoch [4/4], Batch [134200/260984], Loss: 2.2580\n",
            "Train Epoch [4/4], Batch [134300/260984], Loss: 2.4119\n",
            "Train Epoch [4/4], Batch [134400/260984], Loss: 2.2430\n",
            "Train Epoch [4/4], Batch [134500/260984], Loss: 2.1421\n",
            "Train Epoch [4/4], Batch [134600/260984], Loss: 2.2301\n",
            "Train Epoch [4/4], Batch [134700/260984], Loss: 2.3946\n",
            "Train Epoch [4/4], Batch [134800/260984], Loss: 2.2088\n",
            "Train Epoch [4/4], Batch [134900/260984], Loss: 2.3726\n",
            "Train Epoch [4/4], Batch [135000/260984], Loss: 2.2038\n",
            "Train Epoch [4/4], Batch [135100/260984], Loss: 2.0703\n",
            "Train Epoch [4/4], Batch [135200/260984], Loss: 2.2703\n",
            "Train Epoch [4/4], Batch [135300/260984], Loss: 2.3146\n",
            "Train Epoch [4/4], Batch [135400/260984], Loss: 2.3172\n",
            "Train Epoch [4/4], Batch [135500/260984], Loss: 2.5967\n",
            "Train Epoch [4/4], Batch [135600/260984], Loss: 2.4409\n",
            "Train Epoch [4/4], Batch [135700/260984], Loss: 2.1803\n",
            "Train Epoch [4/4], Batch [135800/260984], Loss: 2.2816\n",
            "Train Epoch [4/4], Batch [135900/260984], Loss: 2.2928\n",
            "Train Epoch [4/4], Batch [136000/260984], Loss: 2.3128\n",
            "Train Epoch [4/4], Batch [136100/260984], Loss: 2.3131\n",
            "Train Epoch [4/4], Batch [136200/260984], Loss: 2.3516\n",
            "Train Epoch [4/4], Batch [136300/260984], Loss: 2.1904\n",
            "Train Epoch [4/4], Batch [136400/260984], Loss: 2.4040\n",
            "Train Epoch [4/4], Batch [136500/260984], Loss: 2.2702\n",
            "Train Epoch [4/4], Batch [136600/260984], Loss: 2.3131\n",
            "Train Epoch [4/4], Batch [136700/260984], Loss: 2.3184\n",
            "Train Epoch [4/4], Batch [136800/260984], Loss: 2.3393\n",
            "Train Epoch [4/4], Batch [136900/260984], Loss: 2.3452\n",
            "Train Epoch [4/4], Batch [137000/260984], Loss: 2.6300\n",
            "Train Epoch [4/4], Batch [137100/260984], Loss: 2.2759\n",
            "Train Epoch [4/4], Batch [137200/260984], Loss: 2.2840\n",
            "Train Epoch [4/4], Batch [137300/260984], Loss: 2.4403\n",
            "Train Epoch [4/4], Batch [137400/260984], Loss: 2.4320\n",
            "Train Epoch [4/4], Batch [137500/260984], Loss: 2.2965\n",
            "Train Epoch [4/4], Batch [137600/260984], Loss: 2.1902\n",
            "Train Epoch [4/4], Batch [137700/260984], Loss: 2.3495\n",
            "Train Epoch [4/4], Batch [137800/260984], Loss: 2.1922\n",
            "Train Epoch [4/4], Batch [137900/260984], Loss: 2.3431\n",
            "Train Epoch [4/4], Batch [138000/260984], Loss: 2.2327\n",
            "Train Epoch [4/4], Batch [138100/260984], Loss: 2.2281\n",
            "Train Epoch [4/4], Batch [138200/260984], Loss: 2.3200\n",
            "Train Epoch [4/4], Batch [138300/260984], Loss: 2.3682\n",
            "Train Epoch [4/4], Batch [138400/260984], Loss: 2.3227\n",
            "Train Epoch [4/4], Batch [138500/260984], Loss: 2.1748\n",
            "Train Epoch [4/4], Batch [138600/260984], Loss: 2.3417\n",
            "Train Epoch [4/4], Batch [138700/260984], Loss: 2.5125\n",
            "Train Epoch [4/4], Batch [138800/260984], Loss: 2.2886\n",
            "Train Epoch [4/4], Batch [138900/260984], Loss: 2.4089\n",
            "Train Epoch [4/4], Batch [139000/260984], Loss: 2.4031\n",
            "Train Epoch [4/4], Batch [139100/260984], Loss: 2.3861\n",
            "Train Epoch [4/4], Batch [139200/260984], Loss: 2.0690\n",
            "Train Epoch [4/4], Batch [139300/260984], Loss: 2.3446\n",
            "Train Epoch [4/4], Batch [139400/260984], Loss: 2.2334\n",
            "Train Epoch [4/4], Batch [139500/260984], Loss: 2.4356\n",
            "Train Epoch [4/4], Batch [139600/260984], Loss: 2.2562\n",
            "Train Epoch [4/4], Batch [139700/260984], Loss: 2.3494\n",
            "Train Epoch [4/4], Batch [139800/260984], Loss: 2.4934\n",
            "Train Epoch [4/4], Batch [139900/260984], Loss: 2.3227\n",
            "Train Epoch [4/4], Batch [140000/260984], Loss: 2.3039\n",
            "Train Epoch [4/4], Batch [140100/260984], Loss: 2.2179\n",
            "Train Epoch [4/4], Batch [140200/260984], Loss: 2.3444\n",
            "Train Epoch [4/4], Batch [140300/260984], Loss: 2.1947\n",
            "Train Epoch [4/4], Batch [140400/260984], Loss: 2.2864\n",
            "Train Epoch [4/4], Batch [140500/260984], Loss: 2.3508\n",
            "Train Epoch [4/4], Batch [140600/260984], Loss: 2.2540\n",
            "Train Epoch [4/4], Batch [140700/260984], Loss: 2.3503\n",
            "Train Epoch [4/4], Batch [140800/260984], Loss: 2.2607\n",
            "Train Epoch [4/4], Batch [140900/260984], Loss: 2.3440\n",
            "Train Epoch [4/4], Batch [141000/260984], Loss: 2.3542\n",
            "Train Epoch [4/4], Batch [141100/260984], Loss: 2.1526\n",
            "Train Epoch [4/4], Batch [141200/260984], Loss: 2.3353\n",
            "Train Epoch [4/4], Batch [141300/260984], Loss: 2.2747\n",
            "Train Epoch [4/4], Batch [141400/260984], Loss: 2.2734\n",
            "Train Epoch [4/4], Batch [141500/260984], Loss: 2.4509\n",
            "Train Epoch [4/4], Batch [141600/260984], Loss: 2.1195\n",
            "Train Epoch [4/4], Batch [141700/260984], Loss: 2.2253\n",
            "Train Epoch [4/4], Batch [141800/260984], Loss: 2.1835\n",
            "Train Epoch [4/4], Batch [141900/260984], Loss: 2.3588\n",
            "Train Epoch [4/4], Batch [142000/260984], Loss: 2.3867\n",
            "Train Epoch [4/4], Batch [142100/260984], Loss: 2.2859\n",
            "Train Epoch [4/4], Batch [142200/260984], Loss: 2.2373\n",
            "Train Epoch [4/4], Batch [142300/260984], Loss: 2.2398\n",
            "Train Epoch [4/4], Batch [142400/260984], Loss: 2.3208\n",
            "Train Epoch [4/4], Batch [142500/260984], Loss: 2.2349\n",
            "Train Epoch [4/4], Batch [142600/260984], Loss: 2.4426\n",
            "Train Epoch [4/4], Batch [142700/260984], Loss: 2.3030\n",
            "Train Epoch [4/4], Batch [142800/260984], Loss: 2.3781\n",
            "Train Epoch [4/4], Batch [142900/260984], Loss: 2.2867\n",
            "Train Epoch [4/4], Batch [143000/260984], Loss: 2.3122\n",
            "Train Epoch [4/4], Batch [143100/260984], Loss: 2.3589\n",
            "Train Epoch [4/4], Batch [143200/260984], Loss: 2.4001\n",
            "Train Epoch [4/4], Batch [143300/260984], Loss: 2.2936\n",
            "Train Epoch [4/4], Batch [143400/260984], Loss: 2.1380\n",
            "Train Epoch [4/4], Batch [143500/260984], Loss: 2.3316\n",
            "Train Epoch [4/4], Batch [143600/260984], Loss: 2.2564\n",
            "Train Epoch [4/4], Batch [143700/260984], Loss: 2.3972\n",
            "Train Epoch [4/4], Batch [143800/260984], Loss: 2.4424\n",
            "Train Epoch [4/4], Batch [143900/260984], Loss: 2.3098\n",
            "Train Epoch [4/4], Batch [144000/260984], Loss: 2.3275\n",
            "Train Epoch [4/4], Batch [144100/260984], Loss: 2.4204\n",
            "Train Epoch [4/4], Batch [144200/260984], Loss: 2.4261\n",
            "Train Epoch [4/4], Batch [144300/260984], Loss: 2.2220\n",
            "Train Epoch [4/4], Batch [144400/260984], Loss: 2.2943\n",
            "Train Epoch [4/4], Batch [144500/260984], Loss: 2.2510\n",
            "Train Epoch [4/4], Batch [144600/260984], Loss: 2.3824\n",
            "Train Epoch [4/4], Batch [144700/260984], Loss: 2.3107\n",
            "Train Epoch [4/4], Batch [144800/260984], Loss: 2.2711\n",
            "Train Epoch [4/4], Batch [144900/260984], Loss: 2.3557\n",
            "Train Epoch [4/4], Batch [145000/260984], Loss: 2.2162\n",
            "Train Epoch [4/4], Batch [145100/260984], Loss: 2.1381\n",
            "Train Epoch [4/4], Batch [145200/260984], Loss: 2.2385\n",
            "Train Epoch [4/4], Batch [145300/260984], Loss: 2.3133\n",
            "Train Epoch [4/4], Batch [145400/260984], Loss: 2.3960\n",
            "Train Epoch [4/4], Batch [145500/260984], Loss: 2.3696\n",
            "Train Epoch [4/4], Batch [145600/260984], Loss: 2.3235\n",
            "Train Epoch [4/4], Batch [145700/260984], Loss: 2.2888\n",
            "Train Epoch [4/4], Batch [145800/260984], Loss: 2.3357\n",
            "Train Epoch [4/4], Batch [145900/260984], Loss: 2.3194\n",
            "Train Epoch [4/4], Batch [146000/260984], Loss: 2.3794\n",
            "Train Epoch [4/4], Batch [146100/260984], Loss: 2.2626\n",
            "Train Epoch [4/4], Batch [146200/260984], Loss: 2.2489\n",
            "Train Epoch [4/4], Batch [146300/260984], Loss: 2.2006\n",
            "Train Epoch [4/4], Batch [146400/260984], Loss: 2.3259\n",
            "Train Epoch [4/4], Batch [146500/260984], Loss: 2.2858\n",
            "Train Epoch [4/4], Batch [146600/260984], Loss: 2.4684\n",
            "Train Epoch [4/4], Batch [146700/260984], Loss: 2.3058\n",
            "Train Epoch [4/4], Batch [146800/260984], Loss: 2.3952\n",
            "Train Epoch [4/4], Batch [146900/260984], Loss: 2.3664\n",
            "Train Epoch [4/4], Batch [147000/260984], Loss: 2.3851\n",
            "Train Epoch [4/4], Batch [147100/260984], Loss: 2.4087\n",
            "Train Epoch [4/4], Batch [147200/260984], Loss: 2.3954\n",
            "Train Epoch [4/4], Batch [147300/260984], Loss: 2.3049\n",
            "Train Epoch [4/4], Batch [147400/260984], Loss: 2.2207\n",
            "Train Epoch [4/4], Batch [147500/260984], Loss: 2.4449\n",
            "Train Epoch [4/4], Batch [147600/260984], Loss: 2.4472\n",
            "Train Epoch [4/4], Batch [147700/260984], Loss: 2.2269\n",
            "Train Epoch [4/4], Batch [147800/260984], Loss: 2.3193\n",
            "Train Epoch [4/4], Batch [147900/260984], Loss: 2.1987\n",
            "Train Epoch [4/4], Batch [148000/260984], Loss: 2.2766\n",
            "Train Epoch [4/4], Batch [148100/260984], Loss: 2.2382\n",
            "Train Epoch [4/4], Batch [148200/260984], Loss: 2.1580\n",
            "Train Epoch [4/4], Batch [148300/260984], Loss: 2.5319\n",
            "Train Epoch [4/4], Batch [148400/260984], Loss: 2.1906\n",
            "Train Epoch [4/4], Batch [148500/260984], Loss: 2.3672\n",
            "Train Epoch [4/4], Batch [148600/260984], Loss: 2.2115\n",
            "Train Epoch [4/4], Batch [148700/260984], Loss: 2.2907\n",
            "Train Epoch [4/4], Batch [148800/260984], Loss: 2.2386\n",
            "Train Epoch [4/4], Batch [148900/260984], Loss: 2.3384\n",
            "Train Epoch [4/4], Batch [149000/260984], Loss: 2.3471\n",
            "Train Epoch [4/4], Batch [149100/260984], Loss: 2.2825\n",
            "Train Epoch [4/4], Batch [149200/260984], Loss: 2.4131\n",
            "Train Epoch [4/4], Batch [149300/260984], Loss: 2.2384\n",
            "Train Epoch [4/4], Batch [149400/260984], Loss: 2.2452\n",
            "Train Epoch [4/4], Batch [149500/260984], Loss: 2.2586\n",
            "Train Epoch [4/4], Batch [149600/260984], Loss: 2.4054\n",
            "Train Epoch [4/4], Batch [149700/260984], Loss: 2.3160\n",
            "Train Epoch [4/4], Batch [149800/260984], Loss: 2.2578\n",
            "Train Epoch [4/4], Batch [149900/260984], Loss: 2.3765\n",
            "Train Epoch [4/4], Batch [150000/260984], Loss: 2.2310\n",
            "Train Epoch [4/4], Batch [150100/260984], Loss: 2.2390\n",
            "Train Epoch [4/4], Batch [150200/260984], Loss: 2.2574\n",
            "Train Epoch [4/4], Batch [150300/260984], Loss: 2.3376\n",
            "Train Epoch [4/4], Batch [150400/260984], Loss: 2.1913\n",
            "Train Epoch [4/4], Batch [150500/260984], Loss: 2.3535\n",
            "Train Epoch [4/4], Batch [150600/260984], Loss: 2.2873\n",
            "Train Epoch [4/4], Batch [150700/260984], Loss: 2.2723\n",
            "Train Epoch [4/4], Batch [150800/260984], Loss: 2.2140\n",
            "Train Epoch [4/4], Batch [150900/260984], Loss: 2.3302\n",
            "Train Epoch [4/4], Batch [151000/260984], Loss: 2.2863\n",
            "Train Epoch [4/4], Batch [151100/260984], Loss: 2.2988\n",
            "Train Epoch [4/4], Batch [151200/260984], Loss: 2.2122\n",
            "Train Epoch [4/4], Batch [151300/260984], Loss: 2.3152\n",
            "Train Epoch [4/4], Batch [151400/260984], Loss: 2.3194\n",
            "Train Epoch [4/4], Batch [151500/260984], Loss: 2.2867\n",
            "Train Epoch [4/4], Batch [151600/260984], Loss: 2.1741\n",
            "Train Epoch [4/4], Batch [151700/260984], Loss: 2.3752\n",
            "Train Epoch [4/4], Batch [151800/260984], Loss: 2.1511\n",
            "Train Epoch [4/4], Batch [151900/260984], Loss: 2.3397\n",
            "Train Epoch [4/4], Batch [152000/260984], Loss: 2.3924\n",
            "Train Epoch [4/4], Batch [152100/260984], Loss: 2.3232\n",
            "Train Epoch [4/4], Batch [152200/260984], Loss: 2.2353\n",
            "Train Epoch [4/4], Batch [152300/260984], Loss: 2.3503\n",
            "Train Epoch [4/4], Batch [152400/260984], Loss: 2.2486\n",
            "Train Epoch [4/4], Batch [152500/260984], Loss: 2.3047\n",
            "Train Epoch [4/4], Batch [152600/260984], Loss: 2.3893\n",
            "Train Epoch [4/4], Batch [152700/260984], Loss: 2.3798\n",
            "Train Epoch [4/4], Batch [152800/260984], Loss: 2.3242\n",
            "Train Epoch [4/4], Batch [152900/260984], Loss: 2.3185\n",
            "Train Epoch [4/4], Batch [153000/260984], Loss: 2.3021\n",
            "Train Epoch [4/4], Batch [153100/260984], Loss: 2.1985\n",
            "Train Epoch [4/4], Batch [153200/260984], Loss: 2.1934\n",
            "Train Epoch [4/4], Batch [153300/260984], Loss: 2.2837\n",
            "Train Epoch [4/4], Batch [153400/260984], Loss: 2.3443\n",
            "Train Epoch [4/4], Batch [153500/260984], Loss: 2.4432\n",
            "Train Epoch [4/4], Batch [153600/260984], Loss: 2.2019\n",
            "Train Epoch [4/4], Batch [153700/260984], Loss: 2.2961\n",
            "Train Epoch [4/4], Batch [153800/260984], Loss: 2.3149\n",
            "Train Epoch [4/4], Batch [153900/260984], Loss: 2.1982\n",
            "Train Epoch [4/4], Batch [154000/260984], Loss: 2.1910\n",
            "Train Epoch [4/4], Batch [154100/260984], Loss: 2.3972\n",
            "Train Epoch [4/4], Batch [154200/260984], Loss: 2.4887\n",
            "Train Epoch [4/4], Batch [154300/260984], Loss: 2.2037\n",
            "Train Epoch [4/4], Batch [154400/260984], Loss: 2.1896\n",
            "Train Epoch [4/4], Batch [154500/260984], Loss: 2.2819\n",
            "Train Epoch [4/4], Batch [154600/260984], Loss: 2.3053\n",
            "Train Epoch [4/4], Batch [154700/260984], Loss: 2.4426\n",
            "Train Epoch [4/4], Batch [154800/260984], Loss: 2.4045\n",
            "Train Epoch [4/4], Batch [154900/260984], Loss: 2.3470\n",
            "Train Epoch [4/4], Batch [155000/260984], Loss: 2.2629\n",
            "Train Epoch [4/4], Batch [155100/260984], Loss: 2.4057\n",
            "Train Epoch [4/4], Batch [155200/260984], Loss: 2.2562\n",
            "Train Epoch [4/4], Batch [155300/260984], Loss: 2.3027\n",
            "Train Epoch [4/4], Batch [155400/260984], Loss: 2.2983\n",
            "Train Epoch [4/4], Batch [155500/260984], Loss: 2.2474\n",
            "Train Epoch [4/4], Batch [155600/260984], Loss: 2.1635\n",
            "Train Epoch [4/4], Batch [155700/260984], Loss: 2.2320\n",
            "Train Epoch [4/4], Batch [155800/260984], Loss: 2.1729\n",
            "Train Epoch [4/4], Batch [155900/260984], Loss: 2.2212\n",
            "Train Epoch [4/4], Batch [156000/260984], Loss: 2.2527\n",
            "Train Epoch [4/4], Batch [156100/260984], Loss: 2.2577\n",
            "Train Epoch [4/4], Batch [156200/260984], Loss: 2.4127\n",
            "Train Epoch [4/4], Batch [156300/260984], Loss: 2.1565\n",
            "Train Epoch [4/4], Batch [156400/260984], Loss: 2.4152\n",
            "Train Epoch [4/4], Batch [156500/260984], Loss: 2.1603\n",
            "Train Epoch [4/4], Batch [156600/260984], Loss: 2.3569\n",
            "Train Epoch [4/4], Batch [156700/260984], Loss: 2.3523\n",
            "Train Epoch [4/4], Batch [156800/260984], Loss: 2.3721\n",
            "Train Epoch [4/4], Batch [156900/260984], Loss: 2.4134\n",
            "Train Epoch [4/4], Batch [157000/260984], Loss: 2.3288\n",
            "Train Epoch [4/4], Batch [157100/260984], Loss: 2.2763\n",
            "Train Epoch [4/4], Batch [157200/260984], Loss: 2.3867\n",
            "Train Epoch [4/4], Batch [157300/260984], Loss: 2.3697\n",
            "Train Epoch [4/4], Batch [157400/260984], Loss: 2.3468\n",
            "Train Epoch [4/4], Batch [157500/260984], Loss: 2.4279\n",
            "Train Epoch [4/4], Batch [157600/260984], Loss: 2.3789\n",
            "Train Epoch [4/4], Batch [157700/260984], Loss: 2.0766\n",
            "Train Epoch [4/4], Batch [157800/260984], Loss: 2.2800\n",
            "Train Epoch [4/4], Batch [157900/260984], Loss: 2.3836\n",
            "Train Epoch [4/4], Batch [158000/260984], Loss: 2.2644\n",
            "Train Epoch [4/4], Batch [158100/260984], Loss: 2.3304\n",
            "Train Epoch [4/4], Batch [158200/260984], Loss: 2.3784\n",
            "Train Epoch [4/4], Batch [158300/260984], Loss: 2.3089\n",
            "Train Epoch [4/4], Batch [158400/260984], Loss: 2.2195\n",
            "Train Epoch [4/4], Batch [158500/260984], Loss: 2.2210\n",
            "Train Epoch [4/4], Batch [158600/260984], Loss: 2.4289\n",
            "Train Epoch [4/4], Batch [158700/260984], Loss: 2.3150\n",
            "Train Epoch [4/4], Batch [158800/260984], Loss: 2.2898\n",
            "Train Epoch [4/4], Batch [158900/260984], Loss: 2.2531\n",
            "Train Epoch [4/4], Batch [159000/260984], Loss: 2.3200\n",
            "Train Epoch [4/4], Batch [159100/260984], Loss: 2.3213\n",
            "Train Epoch [4/4], Batch [159200/260984], Loss: 2.3092\n",
            "Train Epoch [4/4], Batch [159300/260984], Loss: 2.2388\n",
            "Train Epoch [4/4], Batch [159400/260984], Loss: 2.2658\n",
            "Train Epoch [4/4], Batch [159500/260984], Loss: 2.1964\n",
            "Train Epoch [4/4], Batch [159600/260984], Loss: 2.2537\n",
            "Train Epoch [4/4], Batch [159700/260984], Loss: 2.3516\n",
            "Train Epoch [4/4], Batch [159800/260984], Loss: 2.2184\n",
            "Train Epoch [4/4], Batch [159900/260984], Loss: 2.2507\n",
            "Train Epoch [4/4], Batch [160000/260984], Loss: 2.3820\n",
            "Train Epoch [4/4], Batch [160100/260984], Loss: 2.2251\n",
            "Train Epoch [4/4], Batch [160200/260984], Loss: 2.2261\n",
            "Train Epoch [4/4], Batch [160300/260984], Loss: 2.2495\n",
            "Train Epoch [4/4], Batch [160400/260984], Loss: 2.4393\n",
            "Train Epoch [4/4], Batch [160500/260984], Loss: 2.2697\n",
            "Train Epoch [4/4], Batch [160600/260984], Loss: 2.3965\n",
            "Train Epoch [4/4], Batch [160700/260984], Loss: 2.2824\n",
            "Train Epoch [4/4], Batch [160800/260984], Loss: 2.2796\n",
            "Train Epoch [4/4], Batch [160900/260984], Loss: 2.2169\n",
            "Train Epoch [4/4], Batch [161000/260984], Loss: 2.2694\n",
            "Train Epoch [4/4], Batch [161100/260984], Loss: 2.4544\n",
            "Train Epoch [4/4], Batch [161200/260984], Loss: 2.4452\n",
            "Train Epoch [4/4], Batch [161300/260984], Loss: 2.3117\n",
            "Train Epoch [4/4], Batch [161400/260984], Loss: 2.3403\n",
            "Train Epoch [4/4], Batch [161500/260984], Loss: 2.2254\n",
            "Train Epoch [4/4], Batch [161600/260984], Loss: 2.2482\n",
            "Train Epoch [4/4], Batch [161700/260984], Loss: 2.2371\n",
            "Train Epoch [4/4], Batch [161800/260984], Loss: 2.2772\n",
            "Train Epoch [4/4], Batch [161900/260984], Loss: 2.2578\n",
            "Train Epoch [4/4], Batch [162000/260984], Loss: 2.4145\n",
            "Train Epoch [4/4], Batch [162100/260984], Loss: 2.2384\n",
            "Train Epoch [4/4], Batch [162200/260984], Loss: 2.2491\n",
            "Train Epoch [4/4], Batch [162300/260984], Loss: 2.3245\n",
            "Train Epoch [4/4], Batch [162400/260984], Loss: 2.3228\n",
            "Train Epoch [4/4], Batch [162500/260984], Loss: 2.3062\n",
            "Train Epoch [4/4], Batch [162600/260984], Loss: 2.4262\n",
            "Train Epoch [4/4], Batch [162700/260984], Loss: 2.2287\n",
            "Train Epoch [4/4], Batch [162800/260984], Loss: 2.2113\n",
            "Train Epoch [4/4], Batch [162900/260984], Loss: 2.3375\n",
            "Train Epoch [4/4], Batch [163000/260984], Loss: 2.3161\n",
            "Train Epoch [4/4], Batch [163100/260984], Loss: 2.2769\n",
            "Train Epoch [4/4], Batch [163200/260984], Loss: 2.2515\n",
            "Train Epoch [4/4], Batch [163300/260984], Loss: 2.3253\n",
            "Train Epoch [4/4], Batch [163400/260984], Loss: 2.2278\n",
            "Train Epoch [4/4], Batch [163500/260984], Loss: 2.2564\n",
            "Train Epoch [4/4], Batch [163600/260984], Loss: 2.3158\n",
            "Train Epoch [4/4], Batch [163700/260984], Loss: 2.3784\n",
            "Train Epoch [4/4], Batch [163800/260984], Loss: 2.2546\n",
            "Train Epoch [4/4], Batch [163900/260984], Loss: 2.2580\n",
            "Train Epoch [4/4], Batch [164000/260984], Loss: 2.3938\n",
            "Train Epoch [4/4], Batch [164100/260984], Loss: 2.3410\n",
            "Train Epoch [4/4], Batch [164200/260984], Loss: 2.2299\n",
            "Train Epoch [4/4], Batch [164300/260984], Loss: 2.3487\n",
            "Train Epoch [4/4], Batch [164400/260984], Loss: 2.5630\n",
            "Train Epoch [4/4], Batch [164500/260984], Loss: 2.2428\n",
            "Train Epoch [4/4], Batch [164600/260984], Loss: 2.4343\n",
            "Train Epoch [4/4], Batch [164700/260984], Loss: 2.3311\n",
            "Train Epoch [4/4], Batch [164800/260984], Loss: 2.3899\n",
            "Train Epoch [4/4], Batch [164900/260984], Loss: 2.3447\n",
            "Train Epoch [4/4], Batch [165000/260984], Loss: 2.3736\n",
            "Train Epoch [4/4], Batch [165100/260984], Loss: 2.3183\n",
            "Train Epoch [4/4], Batch [165200/260984], Loss: 2.3121\n",
            "Train Epoch [4/4], Batch [165300/260984], Loss: 2.4091\n",
            "Train Epoch [4/4], Batch [165400/260984], Loss: 2.4434\n",
            "Train Epoch [4/4], Batch [165500/260984], Loss: 2.3120\n",
            "Train Epoch [4/4], Batch [165600/260984], Loss: 2.4459\n",
            "Train Epoch [4/4], Batch [165700/260984], Loss: 2.2127\n",
            "Train Epoch [4/4], Batch [165800/260984], Loss: 2.1873\n",
            "Train Epoch [4/4], Batch [165900/260984], Loss: 2.2937\n",
            "Train Epoch [4/4], Batch [166000/260984], Loss: 2.3157\n",
            "Train Epoch [4/4], Batch [166100/260984], Loss: 2.3803\n",
            "Train Epoch [4/4], Batch [166200/260984], Loss: 2.4739\n",
            "Train Epoch [4/4], Batch [166300/260984], Loss: 2.2784\n",
            "Train Epoch [4/4], Batch [166400/260984], Loss: 2.4088\n",
            "Train Epoch [4/4], Batch [166500/260984], Loss: 2.3578\n",
            "Train Epoch [4/4], Batch [166600/260984], Loss: 2.2196\n",
            "Train Epoch [4/4], Batch [166700/260984], Loss: 2.3622\n",
            "Train Epoch [4/4], Batch [166800/260984], Loss: 2.1568\n",
            "Train Epoch [4/4], Batch [166900/260984], Loss: 2.2774\n",
            "Train Epoch [4/4], Batch [167000/260984], Loss: 2.2026\n",
            "Train Epoch [4/4], Batch [167100/260984], Loss: 2.3301\n",
            "Train Epoch [4/4], Batch [167200/260984], Loss: 2.3021\n",
            "Train Epoch [4/4], Batch [167300/260984], Loss: 2.2678\n",
            "Train Epoch [4/4], Batch [167400/260984], Loss: 2.2365\n",
            "Train Epoch [4/4], Batch [167500/260984], Loss: 2.2237\n",
            "Train Epoch [4/4], Batch [167600/260984], Loss: 2.2254\n",
            "Train Epoch [4/4], Batch [167700/260984], Loss: 2.4761\n",
            "Train Epoch [4/4], Batch [167800/260984], Loss: 2.3936\n",
            "Train Epoch [4/4], Batch [167900/260984], Loss: 2.2928\n",
            "Train Epoch [4/4], Batch [168000/260984], Loss: 2.3005\n",
            "Train Epoch [4/4], Batch [168100/260984], Loss: 2.1991\n",
            "Train Epoch [4/4], Batch [168200/260984], Loss: 2.2739\n",
            "Train Epoch [4/4], Batch [168300/260984], Loss: 2.3318\n",
            "Train Epoch [4/4], Batch [168400/260984], Loss: 2.3588\n",
            "Train Epoch [4/4], Batch [168500/260984], Loss: 2.2500\n",
            "Train Epoch [4/4], Batch [168600/260984], Loss: 2.4396\n",
            "Train Epoch [4/4], Batch [168700/260984], Loss: 2.2824\n",
            "Train Epoch [4/4], Batch [168800/260984], Loss: 2.4395\n",
            "Train Epoch [4/4], Batch [168900/260984], Loss: 2.2173\n",
            "Train Epoch [4/4], Batch [169000/260984], Loss: 2.2586\n",
            "Train Epoch [4/4], Batch [169100/260984], Loss: 2.3764\n",
            "Train Epoch [4/4], Batch [169200/260984], Loss: 2.1312\n",
            "Train Epoch [4/4], Batch [169300/260984], Loss: 2.3632\n",
            "Train Epoch [4/4], Batch [169400/260984], Loss: 2.3184\n",
            "Train Epoch [4/4], Batch [169500/260984], Loss: 2.3251\n",
            "Train Epoch [4/4], Batch [169600/260984], Loss: 2.2994\n",
            "Train Epoch [4/4], Batch [169700/260984], Loss: 2.4258\n",
            "Train Epoch [4/4], Batch [169800/260984], Loss: 2.4487\n",
            "Train Epoch [4/4], Batch [169900/260984], Loss: 2.3778\n",
            "Train Epoch [4/4], Batch [170000/260984], Loss: 2.3867\n",
            "Train Epoch [4/4], Batch [170100/260984], Loss: 2.2205\n",
            "Train Epoch [4/4], Batch [170200/260984], Loss: 2.2755\n",
            "Train Epoch [4/4], Batch [170300/260984], Loss: 2.3151\n",
            "Train Epoch [4/4], Batch [170400/260984], Loss: 2.3152\n",
            "Train Epoch [4/4], Batch [170500/260984], Loss: 2.1560\n",
            "Train Epoch [4/4], Batch [170600/260984], Loss: 2.3105\n",
            "Train Epoch [4/4], Batch [170700/260984], Loss: 2.1306\n",
            "Train Epoch [4/4], Batch [170800/260984], Loss: 2.2752\n",
            "Train Epoch [4/4], Batch [170900/260984], Loss: 2.2704\n",
            "Train Epoch [4/4], Batch [171000/260984], Loss: 2.3662\n",
            "Train Epoch [4/4], Batch [171100/260984], Loss: 2.1861\n",
            "Train Epoch [4/4], Batch [171200/260984], Loss: 2.2513\n",
            "Train Epoch [4/4], Batch [171300/260984], Loss: 2.2438\n",
            "Train Epoch [4/4], Batch [171400/260984], Loss: 2.2224\n",
            "Train Epoch [4/4], Batch [171500/260984], Loss: 2.3818\n",
            "Train Epoch [4/4], Batch [171600/260984], Loss: 2.2766\n",
            "Train Epoch [4/4], Batch [171700/260984], Loss: 2.3209\n",
            "Train Epoch [4/4], Batch [171800/260984], Loss: 2.3735\n",
            "Train Epoch [4/4], Batch [171900/260984], Loss: 2.2628\n",
            "Train Epoch [4/4], Batch [172000/260984], Loss: 2.3935\n",
            "Train Epoch [4/4], Batch [172100/260984], Loss: 2.2451\n",
            "Train Epoch [4/4], Batch [172200/260984], Loss: 2.2188\n",
            "Train Epoch [4/4], Batch [172300/260984], Loss: 2.2813\n",
            "Train Epoch [4/4], Batch [172400/260984], Loss: 2.2490\n",
            "Train Epoch [4/4], Batch [172500/260984], Loss: 2.4800\n",
            "Train Epoch [4/4], Batch [172600/260984], Loss: 2.3821\n",
            "Train Epoch [4/4], Batch [172700/260984], Loss: 2.3642\n",
            "Train Epoch [4/4], Batch [172800/260984], Loss: 2.3886\n",
            "Train Epoch [4/4], Batch [172900/260984], Loss: 2.2540\n",
            "Train Epoch [4/4], Batch [173000/260984], Loss: 2.3761\n",
            "Train Epoch [4/4], Batch [173100/260984], Loss: 2.2779\n",
            "Train Epoch [4/4], Batch [173200/260984], Loss: 2.1903\n",
            "Train Epoch [4/4], Batch [173300/260984], Loss: 2.2918\n",
            "Train Epoch [4/4], Batch [173400/260984], Loss: 2.1701\n",
            "Train Epoch [4/4], Batch [173500/260984], Loss: 2.3922\n",
            "Train Epoch [4/4], Batch [173600/260984], Loss: 2.2576\n",
            "Train Epoch [4/4], Batch [173700/260984], Loss: 2.3228\n",
            "Train Epoch [4/4], Batch [173800/260984], Loss: 2.2256\n",
            "Train Epoch [4/4], Batch [173900/260984], Loss: 2.3481\n",
            "Train Epoch [4/4], Batch [174000/260984], Loss: 2.3631\n",
            "Train Epoch [4/4], Batch [174100/260984], Loss: 2.4017\n",
            "Train Epoch [4/4], Batch [174200/260984], Loss: 2.3711\n",
            "Train Epoch [4/4], Batch [174300/260984], Loss: 2.2670\n",
            "Train Epoch [4/4], Batch [174400/260984], Loss: 2.2130\n",
            "Train Epoch [4/4], Batch [174500/260984], Loss: 2.4326\n",
            "Train Epoch [4/4], Batch [174600/260984], Loss: 2.2703\n",
            "Train Epoch [4/4], Batch [174700/260984], Loss: 2.3738\n",
            "Train Epoch [4/4], Batch [174800/260984], Loss: 2.3086\n",
            "Train Epoch [4/4], Batch [174900/260984], Loss: 2.3199\n",
            "Train Epoch [4/4], Batch [175000/260984], Loss: 2.1586\n",
            "Train Epoch [4/4], Batch [175100/260984], Loss: 2.4219\n",
            "Train Epoch [4/4], Batch [175200/260984], Loss: 2.3955\n",
            "Train Epoch [4/4], Batch [175300/260984], Loss: 2.3573\n",
            "Train Epoch [4/4], Batch [175400/260984], Loss: 2.3122\n",
            "Train Epoch [4/4], Batch [175500/260984], Loss: 2.2513\n",
            "Train Epoch [4/4], Batch [175600/260984], Loss: 2.3198\n",
            "Train Epoch [4/4], Batch [175700/260984], Loss: 2.3004\n",
            "Train Epoch [4/4], Batch [175800/260984], Loss: 2.4460\n",
            "Train Epoch [4/4], Batch [175900/260984], Loss: 2.4127\n",
            "Train Epoch [4/4], Batch [176000/260984], Loss: 2.4730\n",
            "Train Epoch [4/4], Batch [176100/260984], Loss: 2.2254\n",
            "Train Epoch [4/4], Batch [176200/260984], Loss: 2.3402\n",
            "Train Epoch [4/4], Batch [176300/260984], Loss: 2.2678\n",
            "Train Epoch [4/4], Batch [176400/260984], Loss: 2.2316\n",
            "Train Epoch [4/4], Batch [176500/260984], Loss: 2.3124\n",
            "Train Epoch [4/4], Batch [176600/260984], Loss: 2.3633\n",
            "Train Epoch [4/4], Batch [176700/260984], Loss: 2.3012\n",
            "Train Epoch [4/4], Batch [176800/260984], Loss: 2.3243\n",
            "Train Epoch [4/4], Batch [176900/260984], Loss: 2.3807\n",
            "Train Epoch [4/4], Batch [177000/260984], Loss: 2.1215\n",
            "Train Epoch [4/4], Batch [177100/260984], Loss: 2.2209\n",
            "Train Epoch [4/4], Batch [177200/260984], Loss: 2.2451\n",
            "Train Epoch [4/4], Batch [177300/260984], Loss: 2.2298\n",
            "Train Epoch [4/4], Batch [177400/260984], Loss: 2.2605\n",
            "Train Epoch [4/4], Batch [177500/260984], Loss: 2.3146\n",
            "Train Epoch [4/4], Batch [177600/260984], Loss: 2.4394\n",
            "Train Epoch [4/4], Batch [177700/260984], Loss: 2.3734\n",
            "Train Epoch [4/4], Batch [177800/260984], Loss: 2.3450\n",
            "Train Epoch [4/4], Batch [177900/260984], Loss: 2.4215\n",
            "Train Epoch [4/4], Batch [178000/260984], Loss: 2.2545\n",
            "Train Epoch [4/4], Batch [178100/260984], Loss: 2.1768\n",
            "Train Epoch [4/4], Batch [178200/260984], Loss: 2.2288\n",
            "Train Epoch [4/4], Batch [178300/260984], Loss: 2.2770\n",
            "Train Epoch [4/4], Batch [178400/260984], Loss: 2.2520\n",
            "Train Epoch [4/4], Batch [178500/260984], Loss: 2.1856\n",
            "Train Epoch [4/4], Batch [178600/260984], Loss: 2.0940\n",
            "Train Epoch [4/4], Batch [178700/260984], Loss: 2.4208\n",
            "Train Epoch [4/4], Batch [178800/260984], Loss: 2.1973\n",
            "Train Epoch [4/4], Batch [178900/260984], Loss: 2.2524\n",
            "Train Epoch [4/4], Batch [179000/260984], Loss: 2.3178\n",
            "Train Epoch [4/4], Batch [179100/260984], Loss: 2.3238\n",
            "Train Epoch [4/4], Batch [179200/260984], Loss: 2.1591\n",
            "Train Epoch [4/4], Batch [179300/260984], Loss: 2.3775\n",
            "Train Epoch [4/4], Batch [179400/260984], Loss: 2.3477\n",
            "Train Epoch [4/4], Batch [179500/260984], Loss: 2.3878\n",
            "Train Epoch [4/4], Batch [179600/260984], Loss: 2.2819\n",
            "Train Epoch [4/4], Batch [179700/260984], Loss: 2.2406\n",
            "Train Epoch [4/4], Batch [179800/260984], Loss: 2.3214\n",
            "Train Epoch [4/4], Batch [179900/260984], Loss: 2.3144\n",
            "Train Epoch [4/4], Batch [180000/260984], Loss: 2.1924\n",
            "Train Epoch [4/4], Batch [180100/260984], Loss: 2.2240\n",
            "Train Epoch [4/4], Batch [180200/260984], Loss: 2.4061\n",
            "Train Epoch [4/4], Batch [180300/260984], Loss: 2.3913\n",
            "Train Epoch [4/4], Batch [180400/260984], Loss: 2.3066\n",
            "Train Epoch [4/4], Batch [180500/260984], Loss: 2.4443\n",
            "Train Epoch [4/4], Batch [180600/260984], Loss: 2.3402\n",
            "Train Epoch [4/4], Batch [180700/260984], Loss: 2.3309\n",
            "Train Epoch [4/4], Batch [180800/260984], Loss: 2.5155\n",
            "Train Epoch [4/4], Batch [180900/260984], Loss: 2.2516\n",
            "Train Epoch [4/4], Batch [181000/260984], Loss: 2.3515\n",
            "Train Epoch [4/4], Batch [181100/260984], Loss: 2.2296\n",
            "Train Epoch [4/4], Batch [181200/260984], Loss: 2.2702\n",
            "Train Epoch [4/4], Batch [181300/260984], Loss: 2.4452\n",
            "Train Epoch [4/4], Batch [181400/260984], Loss: 2.3540\n",
            "Train Epoch [4/4], Batch [181500/260984], Loss: 2.3073\n",
            "Train Epoch [4/4], Batch [181600/260984], Loss: 2.3071\n",
            "Train Epoch [4/4], Batch [181700/260984], Loss: 2.4131\n",
            "Train Epoch [4/4], Batch [181800/260984], Loss: 2.1235\n",
            "Train Epoch [4/4], Batch [181900/260984], Loss: 2.3875\n",
            "Train Epoch [4/4], Batch [182000/260984], Loss: 2.3451\n",
            "Train Epoch [4/4], Batch [182100/260984], Loss: 2.1163\n",
            "Train Epoch [4/4], Batch [182200/260984], Loss: 2.2845\n",
            "Train Epoch [4/4], Batch [182300/260984], Loss: 2.1736\n",
            "Train Epoch [4/4], Batch [182400/260984], Loss: 2.3455\n",
            "Train Epoch [4/4], Batch [182500/260984], Loss: 2.2072\n",
            "Train Epoch [4/4], Batch [182600/260984], Loss: 2.3183\n",
            "Train Epoch [4/4], Batch [182700/260984], Loss: 2.3673\n",
            "Train Epoch [4/4], Batch [182800/260984], Loss: 2.0688\n",
            "Train Epoch [4/4], Batch [182900/260984], Loss: 2.4023\n",
            "Train Epoch [4/4], Batch [183000/260984], Loss: 2.4089\n",
            "Train Epoch [4/4], Batch [183100/260984], Loss: 2.2817\n",
            "Train Epoch [4/4], Batch [183200/260984], Loss: 2.3196\n",
            "Train Epoch [4/4], Batch [183300/260984], Loss: 2.4440\n",
            "Train Epoch [4/4], Batch [183400/260984], Loss: 2.3863\n",
            "Train Epoch [4/4], Batch [183500/260984], Loss: 2.2530\n",
            "Train Epoch [4/4], Batch [183600/260984], Loss: 2.2270\n",
            "Train Epoch [4/4], Batch [183700/260984], Loss: 2.3051\n",
            "Train Epoch [4/4], Batch [183800/260984], Loss: 2.4550\n",
            "Train Epoch [4/4], Batch [183900/260984], Loss: 2.4440\n",
            "Train Epoch [4/4], Batch [184000/260984], Loss: 2.3718\n",
            "Train Epoch [4/4], Batch [184100/260984], Loss: 2.2432\n",
            "Train Epoch [4/4], Batch [184200/260984], Loss: 2.3672\n",
            "Train Epoch [4/4], Batch [184300/260984], Loss: 2.3082\n",
            "Train Epoch [4/4], Batch [184400/260984], Loss: 2.3633\n",
            "Train Epoch [4/4], Batch [184500/260984], Loss: 2.4011\n",
            "Train Epoch [4/4], Batch [184600/260984], Loss: 2.4717\n",
            "Train Epoch [4/4], Batch [184700/260984], Loss: 2.3198\n",
            "Train Epoch [4/4], Batch [184800/260984], Loss: 2.3690\n",
            "Train Epoch [4/4], Batch [184900/260984], Loss: 2.2845\n",
            "Train Epoch [4/4], Batch [185000/260984], Loss: 2.1579\n",
            "Train Epoch [4/4], Batch [185100/260984], Loss: 2.3700\n",
            "Train Epoch [4/4], Batch [185200/260984], Loss: 2.2087\n",
            "Train Epoch [4/4], Batch [185300/260984], Loss: 2.2909\n",
            "Train Epoch [4/4], Batch [185400/260984], Loss: 2.2928\n",
            "Train Epoch [4/4], Batch [185500/260984], Loss: 2.2267\n",
            "Train Epoch [4/4], Batch [185600/260984], Loss: 2.4035\n",
            "Train Epoch [4/4], Batch [185700/260984], Loss: 2.2614\n",
            "Train Epoch [4/4], Batch [185800/260984], Loss: 2.3667\n",
            "Train Epoch [4/4], Batch [185900/260984], Loss: 2.4379\n",
            "Train Epoch [4/4], Batch [186000/260984], Loss: 2.3564\n",
            "Train Epoch [4/4], Batch [186100/260984], Loss: 2.2013\n",
            "Train Epoch [4/4], Batch [186200/260984], Loss: 2.2334\n",
            "Train Epoch [4/4], Batch [186300/260984], Loss: 2.2407\n",
            "Train Epoch [4/4], Batch [186400/260984], Loss: 2.2776\n",
            "Train Epoch [4/4], Batch [186500/260984], Loss: 2.3762\n",
            "Train Epoch [4/4], Batch [186600/260984], Loss: 2.3701\n",
            "Train Epoch [4/4], Batch [186700/260984], Loss: 2.2578\n",
            "Train Epoch [4/4], Batch [186800/260984], Loss: 2.2762\n",
            "Train Epoch [4/4], Batch [186900/260984], Loss: 2.2173\n",
            "Train Epoch [4/4], Batch [187000/260984], Loss: 2.2820\n",
            "Train Epoch [4/4], Batch [187100/260984], Loss: 2.2908\n",
            "Train Epoch [4/4], Batch [187200/260984], Loss: 2.2795\n",
            "Train Epoch [4/4], Batch [187300/260984], Loss: 2.2241\n",
            "Train Epoch [4/4], Batch [187400/260984], Loss: 2.4709\n",
            "Train Epoch [4/4], Batch [187500/260984], Loss: 2.2176\n",
            "Train Epoch [4/4], Batch [187600/260984], Loss: 2.2971\n",
            "Train Epoch [4/4], Batch [187700/260984], Loss: 2.4194\n",
            "Train Epoch [4/4], Batch [187800/260984], Loss: 2.4393\n",
            "Train Epoch [4/4], Batch [187900/260984], Loss: 2.3441\n",
            "Train Epoch [4/4], Batch [188000/260984], Loss: 2.3207\n",
            "Train Epoch [4/4], Batch [188100/260984], Loss: 2.2998\n",
            "Train Epoch [4/4], Batch [188200/260984], Loss: 2.2269\n",
            "Train Epoch [4/4], Batch [188300/260984], Loss: 2.3129\n",
            "Train Epoch [4/4], Batch [188400/260984], Loss: 2.3156\n",
            "Train Epoch [4/4], Batch [188500/260984], Loss: 2.1879\n",
            "Train Epoch [4/4], Batch [188600/260984], Loss: 2.1973\n",
            "Train Epoch [4/4], Batch [188700/260984], Loss: 2.3652\n",
            "Train Epoch [4/4], Batch [188800/260984], Loss: 2.2840\n",
            "Train Epoch [4/4], Batch [188900/260984], Loss: 2.3165\n",
            "Train Epoch [4/4], Batch [189000/260984], Loss: 2.3150\n",
            "Train Epoch [4/4], Batch [189100/260984], Loss: 2.4248\n",
            "Train Epoch [4/4], Batch [189200/260984], Loss: 2.4288\n",
            "Train Epoch [4/4], Batch [189300/260984], Loss: 2.2594\n",
            "Train Epoch [4/4], Batch [189400/260984], Loss: 2.1986\n",
            "Train Epoch [4/4], Batch [189500/260984], Loss: 2.2872\n",
            "Train Epoch [4/4], Batch [189600/260984], Loss: 2.3329\n",
            "Train Epoch [4/4], Batch [189700/260984], Loss: 2.3554\n",
            "Train Epoch [4/4], Batch [189800/260984], Loss: 2.4138\n",
            "Train Epoch [4/4], Batch [189900/260984], Loss: 2.3501\n",
            "Train Epoch [4/4], Batch [190000/260984], Loss: 2.3760\n",
            "Train Epoch [4/4], Batch [190100/260984], Loss: 2.2431\n",
            "Train Epoch [4/4], Batch [190200/260984], Loss: 2.3964\n",
            "Train Epoch [4/4], Batch [190300/260984], Loss: 2.2445\n",
            "Train Epoch [4/4], Batch [190400/260984], Loss: 2.0881\n",
            "Train Epoch [4/4], Batch [190500/260984], Loss: 2.2816\n",
            "Train Epoch [4/4], Batch [190600/260984], Loss: 2.2880\n",
            "Train Epoch [4/4], Batch [190700/260984], Loss: 2.2817\n",
            "Train Epoch [4/4], Batch [190800/260984], Loss: 2.3782\n",
            "Train Epoch [4/4], Batch [190900/260984], Loss: 2.2873\n",
            "Train Epoch [4/4], Batch [191000/260984], Loss: 2.1966\n",
            "Train Epoch [4/4], Batch [191100/260984], Loss: 2.1900\n",
            "Train Epoch [4/4], Batch [191200/260984], Loss: 2.3378\n",
            "Train Epoch [4/4], Batch [191300/260984], Loss: 2.3535\n",
            "Train Epoch [4/4], Batch [191400/260984], Loss: 2.1638\n",
            "Train Epoch [4/4], Batch [191500/260984], Loss: 2.2433\n",
            "Train Epoch [4/4], Batch [191600/260984], Loss: 2.3150\n",
            "Train Epoch [4/4], Batch [191700/260984], Loss: 2.2760\n",
            "Train Epoch [4/4], Batch [191800/260984], Loss: 2.2652\n",
            "Train Epoch [4/4], Batch [191900/260984], Loss: 2.2009\n",
            "Train Epoch [4/4], Batch [192000/260984], Loss: 2.4234\n",
            "Train Epoch [4/4], Batch [192100/260984], Loss: 2.1806\n",
            "Train Epoch [4/4], Batch [192200/260984], Loss: 2.2820\n",
            "Train Epoch [4/4], Batch [192300/260984], Loss: 2.1378\n",
            "Train Epoch [4/4], Batch [192400/260984], Loss: 2.2437\n",
            "Train Epoch [4/4], Batch [192500/260984], Loss: 2.2618\n",
            "Train Epoch [4/4], Batch [192600/260984], Loss: 2.3255\n",
            "Train Epoch [4/4], Batch [192700/260984], Loss: 2.3885\n",
            "Train Epoch [4/4], Batch [192800/260984], Loss: 2.5256\n",
            "Train Epoch [4/4], Batch [192900/260984], Loss: 2.2489\n",
            "Train Epoch [4/4], Batch [193000/260984], Loss: 2.3806\n",
            "Train Epoch [4/4], Batch [193100/260984], Loss: 2.1538\n",
            "Train Epoch [4/4], Batch [193200/260984], Loss: 2.2186\n",
            "Train Epoch [4/4], Batch [193300/260984], Loss: 2.3447\n",
            "Train Epoch [4/4], Batch [193400/260984], Loss: 2.4069\n",
            "Train Epoch [4/4], Batch [193500/260984], Loss: 2.5011\n",
            "Train Epoch [4/4], Batch [193600/260984], Loss: 2.2351\n",
            "Train Epoch [4/4], Batch [193700/260984], Loss: 2.2550\n",
            "Train Epoch [4/4], Batch [193800/260984], Loss: 2.2801\n",
            "Train Epoch [4/4], Batch [193900/260984], Loss: 2.2493\n",
            "Train Epoch [4/4], Batch [194000/260984], Loss: 2.2914\n",
            "Train Epoch [4/4], Batch [194100/260984], Loss: 2.2613\n",
            "Train Epoch [4/4], Batch [194200/260984], Loss: 2.3315\n",
            "Train Epoch [4/4], Batch [194300/260984], Loss: 2.3133\n",
            "Train Epoch [4/4], Batch [194400/260984], Loss: 2.3289\n",
            "Train Epoch [4/4], Batch [194500/260984], Loss: 2.2535\n",
            "Train Epoch [4/4], Batch [194600/260984], Loss: 2.3164\n",
            "Train Epoch [4/4], Batch [194700/260984], Loss: 2.4136\n",
            "Train Epoch [4/4], Batch [194800/260984], Loss: 2.2131\n",
            "Train Epoch [4/4], Batch [194900/260984], Loss: 2.2833\n",
            "Train Epoch [4/4], Batch [195000/260984], Loss: 2.4879\n",
            "Train Epoch [4/4], Batch [195100/260984], Loss: 2.2675\n",
            "Train Epoch [4/4], Batch [195200/260984], Loss: 2.3373\n",
            "Train Epoch [4/4], Batch [195300/260984], Loss: 2.3060\n",
            "Train Epoch [4/4], Batch [195400/260984], Loss: 2.2866\n",
            "Train Epoch [4/4], Batch [195500/260984], Loss: 2.1854\n",
            "Train Epoch [4/4], Batch [195600/260984], Loss: 2.2136\n",
            "Train Epoch [4/4], Batch [195700/260984], Loss: 2.3370\n",
            "Train Epoch [4/4], Batch [195800/260984], Loss: 2.3150\n",
            "Train Epoch [4/4], Batch [195900/260984], Loss: 2.2888\n",
            "Train Epoch [4/4], Batch [196000/260984], Loss: 2.2836\n",
            "Train Epoch [4/4], Batch [196100/260984], Loss: 2.4214\n",
            "Train Epoch [4/4], Batch [196200/260984], Loss: 2.2492\n",
            "Train Epoch [4/4], Batch [196300/260984], Loss: 2.3287\n",
            "Train Epoch [4/4], Batch [196400/260984], Loss: 2.3804\n",
            "Train Epoch [4/4], Batch [196500/260984], Loss: 2.1595\n",
            "Train Epoch [4/4], Batch [196600/260984], Loss: 2.3750\n",
            "Train Epoch [4/4], Batch [196700/260984], Loss: 2.1672\n",
            "Train Epoch [4/4], Batch [196800/260984], Loss: 2.3864\n",
            "Train Epoch [4/4], Batch [196900/260984], Loss: 2.1195\n",
            "Train Epoch [4/4], Batch [197000/260984], Loss: 2.4665\n",
            "Train Epoch [4/4], Batch [197100/260984], Loss: 2.2678\n",
            "Train Epoch [4/4], Batch [197200/260984], Loss: 2.3553\n",
            "Train Epoch [4/4], Batch [197300/260984], Loss: 2.2658\n",
            "Train Epoch [4/4], Batch [197400/260984], Loss: 2.2332\n",
            "Train Epoch [4/4], Batch [197500/260984], Loss: 2.2583\n",
            "Train Epoch [4/4], Batch [197600/260984], Loss: 2.2255\n",
            "Train Epoch [4/4], Batch [197700/260984], Loss: 2.2383\n",
            "Train Epoch [4/4], Batch [197800/260984], Loss: 2.4091\n",
            "Train Epoch [4/4], Batch [197900/260984], Loss: 2.3463\n",
            "Train Epoch [4/4], Batch [198000/260984], Loss: 2.2598\n",
            "Train Epoch [4/4], Batch [198100/260984], Loss: 2.4044\n",
            "Train Epoch [4/4], Batch [198200/260984], Loss: 2.4260\n",
            "Train Epoch [4/4], Batch [198300/260984], Loss: 2.2205\n",
            "Train Epoch [4/4], Batch [198400/260984], Loss: 2.1132\n",
            "Train Epoch [4/4], Batch [198500/260984], Loss: 2.1790\n",
            "Train Epoch [4/4], Batch [198600/260984], Loss: 2.2168\n",
            "Train Epoch [4/4], Batch [198700/260984], Loss: 2.4006\n",
            "Train Epoch [4/4], Batch [198800/260984], Loss: 2.2900\n",
            "Train Epoch [4/4], Batch [198900/260984], Loss: 2.2446\n",
            "Train Epoch [4/4], Batch [199000/260984], Loss: 2.2746\n",
            "Train Epoch [4/4], Batch [199100/260984], Loss: 2.2201\n",
            "Train Epoch [4/4], Batch [199200/260984], Loss: 2.2196\n",
            "Train Epoch [4/4], Batch [199300/260984], Loss: 2.3690\n",
            "Train Epoch [4/4], Batch [199400/260984], Loss: 2.3324\n",
            "Train Epoch [4/4], Batch [199500/260984], Loss: 2.3621\n",
            "Train Epoch [4/4], Batch [199600/260984], Loss: 2.2824\n",
            "Train Epoch [4/4], Batch [199700/260984], Loss: 2.3506\n",
            "Train Epoch [4/4], Batch [199800/260984], Loss: 2.3232\n",
            "Train Epoch [4/4], Batch [199900/260984], Loss: 2.4674\n",
            "Train Epoch [4/4], Batch [200000/260984], Loss: 2.5064\n",
            "Train Epoch [4/4], Batch [200100/260984], Loss: 2.3772\n",
            "Train Epoch [4/4], Batch [200200/260984], Loss: 2.3916\n",
            "Train Epoch [4/4], Batch [200300/260984], Loss: 2.2611\n",
            "Train Epoch [4/4], Batch [200400/260984], Loss: 2.3693\n",
            "Train Epoch [4/4], Batch [200500/260984], Loss: 2.3711\n",
            "Train Epoch [4/4], Batch [200600/260984], Loss: 2.3335\n",
            "Train Epoch [4/4], Batch [200700/260984], Loss: 2.2435\n",
            "Train Epoch [4/4], Batch [200800/260984], Loss: 2.1164\n",
            "Train Epoch [4/4], Batch [200900/260984], Loss: 2.2533\n",
            "Train Epoch [4/4], Batch [201000/260984], Loss: 2.1580\n",
            "Train Epoch [4/4], Batch [201100/260984], Loss: 2.3895\n",
            "Train Epoch [4/4], Batch [201200/260984], Loss: 2.3363\n",
            "Train Epoch [4/4], Batch [201300/260984], Loss: 2.5356\n",
            "Train Epoch [4/4], Batch [201400/260984], Loss: 2.3453\n",
            "Train Epoch [4/4], Batch [201500/260984], Loss: 2.4102\n",
            "Train Epoch [4/4], Batch [201600/260984], Loss: 2.2556\n",
            "Train Epoch [4/4], Batch [201700/260984], Loss: 2.0346\n",
            "Train Epoch [4/4], Batch [201800/260984], Loss: 2.5306\n",
            "Train Epoch [4/4], Batch [201900/260984], Loss: 2.2888\n",
            "Train Epoch [4/4], Batch [202000/260984], Loss: 2.2599\n",
            "Train Epoch [4/4], Batch [202100/260984], Loss: 2.3407\n",
            "Train Epoch [4/4], Batch [202200/260984], Loss: 2.2108\n",
            "Train Epoch [4/4], Batch [202300/260984], Loss: 2.3456\n",
            "Train Epoch [4/4], Batch [202400/260984], Loss: 2.1954\n",
            "Train Epoch [4/4], Batch [202500/260984], Loss: 2.2188\n",
            "Train Epoch [4/4], Batch [202600/260984], Loss: 2.3241\n",
            "Train Epoch [4/4], Batch [202700/260984], Loss: 2.4700\n",
            "Train Epoch [4/4], Batch [202800/260984], Loss: 2.2945\n",
            "Train Epoch [4/4], Batch [202900/260984], Loss: 2.3454\n",
            "Train Epoch [4/4], Batch [203000/260984], Loss: 2.3837\n",
            "Train Epoch [4/4], Batch [203100/260984], Loss: 2.4170\n",
            "Train Epoch [4/4], Batch [203200/260984], Loss: 2.3043\n",
            "Train Epoch [4/4], Batch [203300/260984], Loss: 2.3308\n",
            "Train Epoch [4/4], Batch [203400/260984], Loss: 2.5032\n",
            "Train Epoch [4/4], Batch [203500/260984], Loss: 2.2215\n",
            "Train Epoch [4/4], Batch [203600/260984], Loss: 2.3512\n",
            "Train Epoch [4/4], Batch [203700/260984], Loss: 2.3609\n",
            "Train Epoch [4/4], Batch [203800/260984], Loss: 2.2256\n",
            "Train Epoch [4/4], Batch [203900/260984], Loss: 2.3245\n",
            "Train Epoch [4/4], Batch [204000/260984], Loss: 2.3111\n",
            "Train Epoch [4/4], Batch [204100/260984], Loss: 2.4432\n",
            "Train Epoch [4/4], Batch [204200/260984], Loss: 2.3635\n",
            "Train Epoch [4/4], Batch [204300/260984], Loss: 2.3402\n",
            "Train Epoch [4/4], Batch [204400/260984], Loss: 2.3088\n",
            "Train Epoch [4/4], Batch [204500/260984], Loss: 2.3504\n",
            "Train Epoch [4/4], Batch [204600/260984], Loss: 2.3343\n",
            "Train Epoch [4/4], Batch [204700/260984], Loss: 2.3535\n",
            "Train Epoch [4/4], Batch [204800/260984], Loss: 2.2688\n",
            "Train Epoch [4/4], Batch [204900/260984], Loss: 2.3149\n",
            "Train Epoch [4/4], Batch [205000/260984], Loss: 2.2128\n",
            "Train Epoch [4/4], Batch [205100/260984], Loss: 2.2723\n",
            "Train Epoch [4/4], Batch [205200/260984], Loss: 2.2899\n",
            "Train Epoch [4/4], Batch [205300/260984], Loss: 2.2537\n",
            "Train Epoch [4/4], Batch [205400/260984], Loss: 2.2228\n",
            "Train Epoch [4/4], Batch [205500/260984], Loss: 2.4574\n",
            "Train Epoch [4/4], Batch [205600/260984], Loss: 2.1591\n",
            "Train Epoch [4/4], Batch [205700/260984], Loss: 2.2279\n",
            "Train Epoch [4/4], Batch [205800/260984], Loss: 2.2689\n",
            "Train Epoch [4/4], Batch [205900/260984], Loss: 2.2330\n",
            "Train Epoch [4/4], Batch [206000/260984], Loss: 2.4597\n",
            "Train Epoch [4/4], Batch [206100/260984], Loss: 2.2946\n",
            "Train Epoch [4/4], Batch [206200/260984], Loss: 2.2727\n",
            "Train Epoch [4/4], Batch [206300/260984], Loss: 2.3785\n",
            "Train Epoch [4/4], Batch [206400/260984], Loss: 2.3256\n",
            "Train Epoch [4/4], Batch [206500/260984], Loss: 2.3141\n",
            "Train Epoch [4/4], Batch [206600/260984], Loss: 2.3982\n",
            "Train Epoch [4/4], Batch [206700/260984], Loss: 2.3203\n",
            "Train Epoch [4/4], Batch [206800/260984], Loss: 2.2166\n",
            "Train Epoch [4/4], Batch [206900/260984], Loss: 2.2930\n",
            "Train Epoch [4/4], Batch [207000/260984], Loss: 2.3592\n",
            "Train Epoch [4/4], Batch [207100/260984], Loss: 2.3701\n",
            "Train Epoch [4/4], Batch [207200/260984], Loss: 2.3461\n",
            "Train Epoch [4/4], Batch [207300/260984], Loss: 2.3171\n",
            "Train Epoch [4/4], Batch [207400/260984], Loss: 2.2247\n",
            "Train Epoch [4/4], Batch [207500/260984], Loss: 2.4095\n",
            "Train Epoch [4/4], Batch [207600/260984], Loss: 2.4538\n",
            "Train Epoch [4/4], Batch [207700/260984], Loss: 2.2531\n",
            "Train Epoch [4/4], Batch [207800/260984], Loss: 2.2231\n",
            "Train Epoch [4/4], Batch [207900/260984], Loss: 2.1906\n",
            "Train Epoch [4/4], Batch [208000/260984], Loss: 2.2219\n",
            "Train Epoch [4/4], Batch [208100/260984], Loss: 2.2984\n",
            "Train Epoch [4/4], Batch [208200/260984], Loss: 2.3976\n",
            "Train Epoch [4/4], Batch [208300/260984], Loss: 2.2241\n",
            "Train Epoch [4/4], Batch [208400/260984], Loss: 2.3945\n",
            "Train Epoch [4/4], Batch [208500/260984], Loss: 2.3202\n",
            "Train Epoch [4/4], Batch [208600/260984], Loss: 2.3721\n",
            "Train Epoch [4/4], Batch [208700/260984], Loss: 2.3778\n",
            "Train Epoch [4/4], Batch [208800/260984], Loss: 2.1408\n",
            "Train Epoch [4/4], Batch [208900/260984], Loss: 2.2248\n",
            "Train Epoch [4/4], Batch [209000/260984], Loss: 2.2274\n",
            "Train Epoch [4/4], Batch [209100/260984], Loss: 2.2940\n",
            "Train Epoch [4/4], Batch [209200/260984], Loss: 2.1642\n",
            "Train Epoch [4/4], Batch [209300/260984], Loss: 2.3145\n",
            "Train Epoch [4/4], Batch [209400/260984], Loss: 2.3812\n",
            "Train Epoch [4/4], Batch [209500/260984], Loss: 2.2772\n",
            "Train Epoch [4/4], Batch [209600/260984], Loss: 2.3057\n",
            "Train Epoch [4/4], Batch [209700/260984], Loss: 2.2728\n",
            "Train Epoch [4/4], Batch [209800/260984], Loss: 2.2750\n",
            "Train Epoch [4/4], Batch [209900/260984], Loss: 2.1109\n",
            "Train Epoch [4/4], Batch [210000/260984], Loss: 2.3461\n",
            "Train Epoch [4/4], Batch [210100/260984], Loss: 2.3266\n",
            "Train Epoch [4/4], Batch [210200/260984], Loss: 2.3535\n",
            "Train Epoch [4/4], Batch [210300/260984], Loss: 2.1852\n",
            "Train Epoch [4/4], Batch [210400/260984], Loss: 2.2608\n",
            "Train Epoch [4/4], Batch [210500/260984], Loss: 2.3275\n",
            "Train Epoch [4/4], Batch [210600/260984], Loss: 2.2345\n",
            "Train Epoch [4/4], Batch [210700/260984], Loss: 2.2535\n",
            "Train Epoch [4/4], Batch [210800/260984], Loss: 2.3420\n",
            "Train Epoch [4/4], Batch [210900/260984], Loss: 2.3027\n",
            "Train Epoch [4/4], Batch [211000/260984], Loss: 2.2799\n",
            "Train Epoch [4/4], Batch [211100/260984], Loss: 2.3786\n",
            "Train Epoch [4/4], Batch [211200/260984], Loss: 2.2787\n",
            "Train Epoch [4/4], Batch [211300/260984], Loss: 2.3810\n",
            "Train Epoch [4/4], Batch [211400/260984], Loss: 2.4118\n",
            "Train Epoch [4/4], Batch [211500/260984], Loss: 2.2601\n",
            "Train Epoch [4/4], Batch [211600/260984], Loss: 2.3346\n",
            "Train Epoch [4/4], Batch [211700/260984], Loss: 2.3045\n",
            "Train Epoch [4/4], Batch [211800/260984], Loss: 2.3644\n",
            "Train Epoch [4/4], Batch [211900/260984], Loss: 2.2023\n",
            "Train Epoch [4/4], Batch [212000/260984], Loss: 2.4028\n",
            "Train Epoch [4/4], Batch [212100/260984], Loss: 2.3504\n",
            "Train Epoch [4/4], Batch [212200/260984], Loss: 2.3444\n",
            "Train Epoch [4/4], Batch [212300/260984], Loss: 2.4541\n",
            "Train Epoch [4/4], Batch [212400/260984], Loss: 2.4085\n",
            "Train Epoch [4/4], Batch [212500/260984], Loss: 2.2169\n",
            "Train Epoch [4/4], Batch [212600/260984], Loss: 2.4549\n",
            "Train Epoch [4/4], Batch [212700/260984], Loss: 2.3749\n",
            "Train Epoch [4/4], Batch [212800/260984], Loss: 2.4109\n",
            "Train Epoch [4/4], Batch [212900/260984], Loss: 2.1978\n",
            "Train Epoch [4/4], Batch [213000/260984], Loss: 2.4661\n",
            "Train Epoch [4/4], Batch [213100/260984], Loss: 2.3227\n",
            "Train Epoch [4/4], Batch [213200/260984], Loss: 2.3006\n",
            "Train Epoch [4/4], Batch [213300/260984], Loss: 2.4653\n",
            "Train Epoch [4/4], Batch [213400/260984], Loss: 2.1109\n",
            "Train Epoch [4/4], Batch [213500/260984], Loss: 2.5026\n",
            "Train Epoch [4/4], Batch [213600/260984], Loss: 2.2251\n",
            "Train Epoch [4/4], Batch [213700/260984], Loss: 2.3129\n",
            "Train Epoch [4/4], Batch [213800/260984], Loss: 2.2511\n",
            "Train Epoch [4/4], Batch [213900/260984], Loss: 2.2971\n",
            "Train Epoch [4/4], Batch [214000/260984], Loss: 2.4378\n",
            "Train Epoch [4/4], Batch [214100/260984], Loss: 2.4294\n",
            "Train Epoch [4/4], Batch [214200/260984], Loss: 2.2568\n",
            "Train Epoch [4/4], Batch [214300/260984], Loss: 2.3465\n",
            "Train Epoch [4/4], Batch [214400/260984], Loss: 2.2885\n",
            "Train Epoch [4/4], Batch [214500/260984], Loss: 2.3374\n",
            "Train Epoch [4/4], Batch [214600/260984], Loss: 2.3525\n",
            "Train Epoch [4/4], Batch [214700/260984], Loss: 2.2686\n",
            "Train Epoch [4/4], Batch [214800/260984], Loss: 2.3629\n",
            "Train Epoch [4/4], Batch [214900/260984], Loss: 2.2818\n",
            "Train Epoch [4/4], Batch [215000/260984], Loss: 2.1947\n",
            "Train Epoch [4/4], Batch [215100/260984], Loss: 2.3876\n",
            "Train Epoch [4/4], Batch [215200/260984], Loss: 2.3340\n",
            "Train Epoch [4/4], Batch [215300/260984], Loss: 2.3159\n",
            "Train Epoch [4/4], Batch [215400/260984], Loss: 2.4068\n",
            "Train Epoch [4/4], Batch [215500/260984], Loss: 2.4781\n",
            "Train Epoch [4/4], Batch [215600/260984], Loss: 2.2336\n",
            "Train Epoch [4/4], Batch [215700/260984], Loss: 2.3497\n",
            "Train Epoch [4/4], Batch [215800/260984], Loss: 2.3156\n",
            "Train Epoch [4/4], Batch [215900/260984], Loss: 2.2462\n",
            "Train Epoch [4/4], Batch [216000/260984], Loss: 2.1585\n",
            "Train Epoch [4/4], Batch [216100/260984], Loss: 2.2218\n",
            "Train Epoch [4/4], Batch [216200/260984], Loss: 2.3225\n",
            "Train Epoch [4/4], Batch [216300/260984], Loss: 2.3153\n",
            "Train Epoch [4/4], Batch [216400/260984], Loss: 2.2785\n",
            "Train Epoch [4/4], Batch [216500/260984], Loss: 2.4160\n",
            "Train Epoch [4/4], Batch [216600/260984], Loss: 2.2883\n",
            "Train Epoch [4/4], Batch [216700/260984], Loss: 2.2423\n",
            "Train Epoch [4/4], Batch [216800/260984], Loss: 2.3211\n",
            "Train Epoch [4/4], Batch [216900/260984], Loss: 2.3769\n",
            "Train Epoch [4/4], Batch [217000/260984], Loss: 2.2535\n",
            "Train Epoch [4/4], Batch [217100/260984], Loss: 2.4135\n",
            "Train Epoch [4/4], Batch [217200/260984], Loss: 2.1978\n",
            "Train Epoch [4/4], Batch [217300/260984], Loss: 2.2964\n",
            "Train Epoch [4/4], Batch [217400/260984], Loss: 2.4225\n",
            "Train Epoch [4/4], Batch [217500/260984], Loss: 2.3678\n",
            "Train Epoch [4/4], Batch [217600/260984], Loss: 2.1764\n",
            "Train Epoch [4/4], Batch [217700/260984], Loss: 2.3626\n",
            "Train Epoch [4/4], Batch [217800/260984], Loss: 2.2210\n",
            "Train Epoch [4/4], Batch [217900/260984], Loss: 2.1673\n",
            "Train Epoch [4/4], Batch [218000/260984], Loss: 2.3491\n",
            "Train Epoch [4/4], Batch [218100/260984], Loss: 2.3458\n",
            "Train Epoch [4/4], Batch [218200/260984], Loss: 2.3715\n",
            "Train Epoch [4/4], Batch [218300/260984], Loss: 2.3783\n",
            "Train Epoch [4/4], Batch [218400/260984], Loss: 2.2848\n",
            "Train Epoch [4/4], Batch [218500/260984], Loss: 2.3456\n",
            "Train Epoch [4/4], Batch [218600/260984], Loss: 2.2675\n",
            "Train Epoch [4/4], Batch [218700/260984], Loss: 2.4705\n",
            "Train Epoch [4/4], Batch [218800/260984], Loss: 2.2593\n",
            "Train Epoch [4/4], Batch [218900/260984], Loss: 2.4287\n",
            "Train Epoch [4/4], Batch [219000/260984], Loss: 2.4368\n",
            "Train Epoch [4/4], Batch [219100/260984], Loss: 2.3912\n",
            "Train Epoch [4/4], Batch [219200/260984], Loss: 2.2330\n",
            "Train Epoch [4/4], Batch [219300/260984], Loss: 2.2752\n",
            "Train Epoch [4/4], Batch [219400/260984], Loss: 2.3254\n",
            "Train Epoch [4/4], Batch [219500/260984], Loss: 2.4447\n",
            "Train Epoch [4/4], Batch [219600/260984], Loss: 2.2391\n",
            "Train Epoch [4/4], Batch [219700/260984], Loss: 2.2268\n",
            "Train Epoch [4/4], Batch [219800/260984], Loss: 2.4288\n",
            "Train Epoch [4/4], Batch [219900/260984], Loss: 2.2807\n",
            "Train Epoch [4/4], Batch [220000/260984], Loss: 2.3676\n",
            "Train Epoch [4/4], Batch [220100/260984], Loss: 2.3806\n",
            "Train Epoch [4/4], Batch [220200/260984], Loss: 2.2355\n",
            "Train Epoch [4/4], Batch [220300/260984], Loss: 2.2135\n",
            "Train Epoch [4/4], Batch [220400/260984], Loss: 2.1831\n",
            "Train Epoch [4/4], Batch [220500/260984], Loss: 2.3539\n",
            "Train Epoch [4/4], Batch [220600/260984], Loss: 2.3777\n",
            "Train Epoch [4/4], Batch [220700/260984], Loss: 2.1617\n",
            "Train Epoch [4/4], Batch [220800/260984], Loss: 2.3577\n",
            "Train Epoch [4/4], Batch [220900/260984], Loss: 2.3425\n",
            "Train Epoch [4/4], Batch [221000/260984], Loss: 2.2592\n",
            "Train Epoch [4/4], Batch [221100/260984], Loss: 2.2573\n",
            "Train Epoch [4/4], Batch [221200/260984], Loss: 2.2251\n",
            "Train Epoch [4/4], Batch [221300/260984], Loss: 2.2217\n",
            "Train Epoch [4/4], Batch [221400/260984], Loss: 2.2560\n",
            "Train Epoch [4/4], Batch [221500/260984], Loss: 2.1591\n",
            "Train Epoch [4/4], Batch [221600/260984], Loss: 2.4693\n",
            "Train Epoch [4/4], Batch [221700/260984], Loss: 2.2002\n",
            "Train Epoch [4/4], Batch [221800/260984], Loss: 2.2386\n",
            "Train Epoch [4/4], Batch [221900/260984], Loss: 2.3507\n",
            "Train Epoch [4/4], Batch [222000/260984], Loss: 2.3201\n",
            "Train Epoch [4/4], Batch [222100/260984], Loss: 2.2797\n",
            "Train Epoch [4/4], Batch [222200/260984], Loss: 2.3746\n",
            "Train Epoch [4/4], Batch [222300/260984], Loss: 2.2296\n",
            "Train Epoch [4/4], Batch [222400/260984], Loss: 2.2559\n",
            "Train Epoch [4/4], Batch [222500/260984], Loss: 2.3184\n",
            "Train Epoch [4/4], Batch [222600/260984], Loss: 2.2976\n",
            "Train Epoch [4/4], Batch [222700/260984], Loss: 2.2998\n",
            "Train Epoch [4/4], Batch [222800/260984], Loss: 2.3494\n",
            "Train Epoch [4/4], Batch [222900/260984], Loss: 2.2867\n",
            "Train Epoch [4/4], Batch [223000/260984], Loss: 2.2704\n",
            "Train Epoch [4/4], Batch [223100/260984], Loss: 2.2555\n",
            "Train Epoch [4/4], Batch [223200/260984], Loss: 2.3191\n",
            "Train Epoch [4/4], Batch [223300/260984], Loss: 2.2893\n",
            "Train Epoch [4/4], Batch [223400/260984], Loss: 2.3215\n",
            "Train Epoch [4/4], Batch [223500/260984], Loss: 2.2529\n",
            "Train Epoch [4/4], Batch [223600/260984], Loss: 2.4119\n",
            "Train Epoch [4/4], Batch [223700/260984], Loss: 2.2845\n",
            "Train Epoch [4/4], Batch [223800/260984], Loss: 2.1942\n",
            "Train Epoch [4/4], Batch [223900/260984], Loss: 2.3117\n",
            "Train Epoch [4/4], Batch [224000/260984], Loss: 2.2867\n",
            "Train Epoch [4/4], Batch [224100/260984], Loss: 2.4077\n",
            "Train Epoch [4/4], Batch [224200/260984], Loss: 2.3542\n",
            "Train Epoch [4/4], Batch [224300/260984], Loss: 2.2094\n",
            "Train Epoch [4/4], Batch [224400/260984], Loss: 2.2840\n",
            "Train Epoch [4/4], Batch [224500/260984], Loss: 2.3165\n",
            "Train Epoch [4/4], Batch [224600/260984], Loss: 2.4953\n",
            "Train Epoch [4/4], Batch [224700/260984], Loss: 2.2918\n",
            "Train Epoch [4/4], Batch [224800/260984], Loss: 2.2862\n",
            "Train Epoch [4/4], Batch [224900/260984], Loss: 2.2206\n",
            "Train Epoch [4/4], Batch [225000/260984], Loss: 2.3922\n",
            "Train Epoch [4/4], Batch [225100/260984], Loss: 2.2643\n",
            "Train Epoch [4/4], Batch [225200/260984], Loss: 2.2744\n",
            "Train Epoch [4/4], Batch [225300/260984], Loss: 2.3792\n",
            "Train Epoch [4/4], Batch [225400/260984], Loss: 2.2901\n",
            "Train Epoch [4/4], Batch [225500/260984], Loss: 2.3066\n",
            "Train Epoch [4/4], Batch [225600/260984], Loss: 2.2243\n",
            "Train Epoch [4/4], Batch [225700/260984], Loss: 2.2431\n",
            "Train Epoch [4/4], Batch [225800/260984], Loss: 2.3201\n",
            "Train Epoch [4/4], Batch [225900/260984], Loss: 2.4263\n",
            "Train Epoch [4/4], Batch [226000/260984], Loss: 2.2163\n",
            "Train Epoch [4/4], Batch [226100/260984], Loss: 2.4219\n",
            "Train Epoch [4/4], Batch [226200/260984], Loss: 2.3282\n",
            "Train Epoch [4/4], Batch [226300/260984], Loss: 2.3541\n",
            "Train Epoch [4/4], Batch [226400/260984], Loss: 2.2535\n",
            "Train Epoch [4/4], Batch [226500/260984], Loss: 2.2253\n",
            "Train Epoch [4/4], Batch [226600/260984], Loss: 2.1986\n",
            "Train Epoch [4/4], Batch [226700/260984], Loss: 2.3789\n",
            "Train Epoch [4/4], Batch [226800/260984], Loss: 2.2409\n",
            "Train Epoch [4/4], Batch [226900/260984], Loss: 2.1450\n",
            "Train Epoch [4/4], Batch [227000/260984], Loss: 2.1803\n",
            "Train Epoch [4/4], Batch [227100/260984], Loss: 2.3112\n",
            "Train Epoch [4/4], Batch [227200/260984], Loss: 2.1464\n",
            "Train Epoch [4/4], Batch [227300/260984], Loss: 2.3790\n",
            "Train Epoch [4/4], Batch [227400/260984], Loss: 2.3052\n",
            "Train Epoch [4/4], Batch [227500/260984], Loss: 2.2357\n",
            "Train Epoch [4/4], Batch [227600/260984], Loss: 2.3460\n",
            "Train Epoch [4/4], Batch [227700/260984], Loss: 2.4014\n",
            "Train Epoch [4/4], Batch [227800/260984], Loss: 2.2188\n",
            "Train Epoch [4/4], Batch [227900/260984], Loss: 2.4077\n",
            "Train Epoch [4/4], Batch [228000/260984], Loss: 2.3767\n",
            "Train Epoch [4/4], Batch [228100/260984], Loss: 2.1582\n",
            "Train Epoch [4/4], Batch [228200/260984], Loss: 2.2862\n",
            "Train Epoch [4/4], Batch [228300/260984], Loss: 2.2276\n",
            "Train Epoch [4/4], Batch [228400/260984], Loss: 2.2802\n",
            "Train Epoch [4/4], Batch [228500/260984], Loss: 2.3138\n",
            "Train Epoch [4/4], Batch [228600/260984], Loss: 2.3391\n",
            "Train Epoch [4/4], Batch [228700/260984], Loss: 2.3132\n",
            "Train Epoch [4/4], Batch [228800/260984], Loss: 2.3928\n",
            "Train Epoch [4/4], Batch [228900/260984], Loss: 2.3478\n",
            "Train Epoch [4/4], Batch [229000/260984], Loss: 2.2513\n",
            "Train Epoch [4/4], Batch [229100/260984], Loss: 2.1872\n",
            "Train Epoch [4/4], Batch [229200/260984], Loss: 2.2849\n",
            "Train Epoch [4/4], Batch [229300/260984], Loss: 2.3967\n",
            "Train Epoch [4/4], Batch [229400/260984], Loss: 2.3365\n",
            "Train Epoch [4/4], Batch [229500/260984], Loss: 2.2600\n",
            "Train Epoch [4/4], Batch [229600/260984], Loss: 2.2245\n",
            "Train Epoch [4/4], Batch [229700/260984], Loss: 2.3591\n",
            "Train Epoch [4/4], Batch [229800/260984], Loss: 2.3782\n",
            "Train Epoch [4/4], Batch [229900/260984], Loss: 2.3305\n",
            "Train Epoch [4/4], Batch [230000/260984], Loss: 2.3400\n",
            "Train Epoch [4/4], Batch [230100/260984], Loss: 2.3464\n",
            "Train Epoch [4/4], Batch [230200/260984], Loss: 2.3212\n",
            "Train Epoch [4/4], Batch [230300/260984], Loss: 2.3738\n",
            "Train Epoch [4/4], Batch [230400/260984], Loss: 2.3297\n",
            "Train Epoch [4/4], Batch [230500/260984], Loss: 2.3243\n",
            "Train Epoch [4/4], Batch [230600/260984], Loss: 2.3408\n",
            "Train Epoch [4/4], Batch [230700/260984], Loss: 2.2422\n",
            "Train Epoch [4/4], Batch [230800/260984], Loss: 2.2842\n",
            "Train Epoch [4/4], Batch [230900/260984], Loss: 2.3488\n",
            "Train Epoch [4/4], Batch [231000/260984], Loss: 2.2219\n",
            "Train Epoch [4/4], Batch [231100/260984], Loss: 2.2338\n",
            "Train Epoch [4/4], Batch [231200/260984], Loss: 2.3081\n",
            "Train Epoch [4/4], Batch [231300/260984], Loss: 2.2923\n",
            "Train Epoch [4/4], Batch [231400/260984], Loss: 2.3453\n",
            "Train Epoch [4/4], Batch [231500/260984], Loss: 2.4022\n",
            "Train Epoch [4/4], Batch [231600/260984], Loss: 2.4243\n",
            "Train Epoch [4/4], Batch [231700/260984], Loss: 2.3514\n",
            "Train Epoch [4/4], Batch [231800/260984], Loss: 2.5245\n",
            "Train Epoch [4/4], Batch [231900/260984], Loss: 2.3458\n",
            "Train Epoch [4/4], Batch [232000/260984], Loss: 2.3963\n",
            "Train Epoch [4/4], Batch [232100/260984], Loss: 2.1340\n",
            "Train Epoch [4/4], Batch [232200/260984], Loss: 2.3971\n",
            "Train Epoch [4/4], Batch [232300/260984], Loss: 2.3198\n",
            "Train Epoch [4/4], Batch [232400/260984], Loss: 2.3814\n",
            "Train Epoch [4/4], Batch [232500/260984], Loss: 2.2888\n",
            "Train Epoch [4/4], Batch [232600/260984], Loss: 2.3352\n",
            "Train Epoch [4/4], Batch [232700/260984], Loss: 2.4690\n",
            "Train Epoch [4/4], Batch [232800/260984], Loss: 2.3142\n",
            "Train Epoch [4/4], Batch [232900/260984], Loss: 2.2270\n",
            "Train Epoch [4/4], Batch [233000/260984], Loss: 2.1947\n",
            "Train Epoch [4/4], Batch [233100/260984], Loss: 2.3519\n",
            "Train Epoch [4/4], Batch [233200/260984], Loss: 2.3640\n",
            "Train Epoch [4/4], Batch [233300/260984], Loss: 2.4194\n",
            "Train Epoch [4/4], Batch [233400/260984], Loss: 2.1850\n",
            "Train Epoch [4/4], Batch [233500/260984], Loss: 2.3078\n",
            "Train Epoch [4/4], Batch [233600/260984], Loss: 2.4157\n",
            "Train Epoch [4/4], Batch [233700/260984], Loss: 2.3559\n",
            "Train Epoch [4/4], Batch [233800/260984], Loss: 2.2820\n",
            "Train Epoch [4/4], Batch [233900/260984], Loss: 2.5617\n",
            "Train Epoch [4/4], Batch [234000/260984], Loss: 2.2457\n",
            "Train Epoch [4/4], Batch [234100/260984], Loss: 2.3817\n",
            "Train Epoch [4/4], Batch [234200/260984], Loss: 2.4383\n",
            "Train Epoch [4/4], Batch [234300/260984], Loss: 2.2808\n",
            "Train Epoch [4/4], Batch [234400/260984], Loss: 2.4753\n",
            "Train Epoch [4/4], Batch [234500/260984], Loss: 2.2897\n",
            "Train Epoch [4/4], Batch [234600/260984], Loss: 2.3780\n",
            "Train Epoch [4/4], Batch [234700/260984], Loss: 2.1592\n",
            "Train Epoch [4/4], Batch [234800/260984], Loss: 2.3975\n",
            "Train Epoch [4/4], Batch [234900/260984], Loss: 2.3180\n",
            "Train Epoch [4/4], Batch [235000/260984], Loss: 2.2842\n",
            "Train Epoch [4/4], Batch [235100/260984], Loss: 2.4543\n",
            "Train Epoch [4/4], Batch [235200/260984], Loss: 2.2902\n",
            "Train Epoch [4/4], Batch [235300/260984], Loss: 2.3165\n",
            "Train Epoch [4/4], Batch [235400/260984], Loss: 2.2779\n",
            "Train Epoch [4/4], Batch [235500/260984], Loss: 2.4347\n",
            "Train Epoch [4/4], Batch [235600/260984], Loss: 2.2921\n",
            "Train Epoch [4/4], Batch [235700/260984], Loss: 2.1830\n",
            "Train Epoch [4/4], Batch [235800/260984], Loss: 2.3308\n",
            "Train Epoch [4/4], Batch [235900/260984], Loss: 2.2830\n",
            "Train Epoch [4/4], Batch [236000/260984], Loss: 2.3012\n",
            "Train Epoch [4/4], Batch [236100/260984], Loss: 2.2507\n",
            "Train Epoch [4/4], Batch [236200/260984], Loss: 2.1276\n",
            "Train Epoch [4/4], Batch [236300/260984], Loss: 2.4499\n",
            "Train Epoch [4/4], Batch [236400/260984], Loss: 2.2615\n",
            "Train Epoch [4/4], Batch [236500/260984], Loss: 2.3375\n",
            "Train Epoch [4/4], Batch [236600/260984], Loss: 2.3766\n",
            "Train Epoch [4/4], Batch [236700/260984], Loss: 2.1822\n",
            "Train Epoch [4/4], Batch [236800/260984], Loss: 2.3882\n",
            "Train Epoch [4/4], Batch [236900/260984], Loss: 2.0648\n",
            "Train Epoch [4/4], Batch [237000/260984], Loss: 2.2129\n",
            "Train Epoch [4/4], Batch [237100/260984], Loss: 2.2788\n",
            "Train Epoch [4/4], Batch [237200/260984], Loss: 2.3157\n",
            "Train Epoch [4/4], Batch [237300/260984], Loss: 2.2916\n",
            "Train Epoch [4/4], Batch [237400/260984], Loss: 2.2814\n",
            "Train Epoch [4/4], Batch [237500/260984], Loss: 2.3779\n",
            "Train Epoch [4/4], Batch [237600/260984], Loss: 2.3849\n",
            "Train Epoch [4/4], Batch [237700/260984], Loss: 2.2497\n",
            "Train Epoch [4/4], Batch [237800/260984], Loss: 2.1960\n",
            "Train Epoch [4/4], Batch [237900/260984], Loss: 2.5240\n",
            "Train Epoch [4/4], Batch [238000/260984], Loss: 2.2562\n",
            "Train Epoch [4/4], Batch [238100/260984], Loss: 2.2736\n",
            "Train Epoch [4/4], Batch [238200/260984], Loss: 2.1946\n",
            "Train Epoch [4/4], Batch [238300/260984], Loss: 2.2043\n",
            "Train Epoch [4/4], Batch [238400/260984], Loss: 2.3717\n",
            "Train Epoch [4/4], Batch [238500/260984], Loss: 2.3266\n",
            "Train Epoch [4/4], Batch [238600/260984], Loss: 2.2967\n",
            "Train Epoch [4/4], Batch [238700/260984], Loss: 2.3553\n",
            "Train Epoch [4/4], Batch [238800/260984], Loss: 2.3221\n",
            "Train Epoch [4/4], Batch [238900/260984], Loss: 2.4463\n",
            "Train Epoch [4/4], Batch [239000/260984], Loss: 2.4712\n",
            "Train Epoch [4/4], Batch [239100/260984], Loss: 2.3851\n",
            "Train Epoch [4/4], Batch [239200/260984], Loss: 2.4322\n",
            "Train Epoch [4/4], Batch [239300/260984], Loss: 2.2748\n",
            "Train Epoch [4/4], Batch [239400/260984], Loss: 2.3472\n",
            "Train Epoch [4/4], Batch [239500/260984], Loss: 2.1060\n",
            "Train Epoch [4/4], Batch [239600/260984], Loss: 2.3244\n",
            "Train Epoch [4/4], Batch [239700/260984], Loss: 2.4254\n",
            "Train Epoch [4/4], Batch [239800/260984], Loss: 2.3266\n",
            "Train Epoch [4/4], Batch [239900/260984], Loss: 2.3178\n",
            "Train Epoch [4/4], Batch [240000/260984], Loss: 2.2571\n",
            "Train Epoch [4/4], Batch [240100/260984], Loss: 2.2492\n",
            "Train Epoch [4/4], Batch [240200/260984], Loss: 2.3983\n",
            "Train Epoch [4/4], Batch [240300/260984], Loss: 2.3461\n",
            "Train Epoch [4/4], Batch [240400/260984], Loss: 2.3168\n",
            "Train Epoch [4/4], Batch [240500/260984], Loss: 2.2161\n",
            "Train Epoch [4/4], Batch [240600/260984], Loss: 2.1656\n",
            "Train Epoch [4/4], Batch [240700/260984], Loss: 2.3224\n",
            "Train Epoch [4/4], Batch [240800/260984], Loss: 2.4439\n",
            "Train Epoch [4/4], Batch [240900/260984], Loss: 2.3383\n",
            "Train Epoch [4/4], Batch [241000/260984], Loss: 2.1574\n",
            "Train Epoch [4/4], Batch [241100/260984], Loss: 2.2529\n",
            "Train Epoch [4/4], Batch [241200/260984], Loss: 2.2893\n",
            "Train Epoch [4/4], Batch [241300/260984], Loss: 2.5199\n",
            "Train Epoch [4/4], Batch [241400/260984], Loss: 2.2906\n",
            "Train Epoch [4/4], Batch [241500/260984], Loss: 2.2447\n",
            "Train Epoch [4/4], Batch [241600/260984], Loss: 2.2925\n",
            "Train Epoch [4/4], Batch [241700/260984], Loss: 2.3030\n",
            "Train Epoch [4/4], Batch [241800/260984], Loss: 2.4700\n",
            "Train Epoch [4/4], Batch [241900/260984], Loss: 2.3232\n",
            "Train Epoch [4/4], Batch [242000/260984], Loss: 2.3529\n",
            "Train Epoch [4/4], Batch [242100/260984], Loss: 2.3389\n",
            "Train Epoch [4/4], Batch [242200/260984], Loss: 2.3383\n",
            "Train Epoch [4/4], Batch [242300/260984], Loss: 2.3143\n",
            "Train Epoch [4/4], Batch [242400/260984], Loss: 2.1813\n",
            "Train Epoch [4/4], Batch [242500/260984], Loss: 2.3041\n",
            "Train Epoch [4/4], Batch [242600/260984], Loss: 2.2405\n",
            "Train Epoch [4/4], Batch [242700/260984], Loss: 2.3202\n",
            "Train Epoch [4/4], Batch [242800/260984], Loss: 2.1307\n",
            "Train Epoch [4/4], Batch [242900/260984], Loss: 2.4549\n",
            "Train Epoch [4/4], Batch [243000/260984], Loss: 2.3648\n",
            "Train Epoch [4/4], Batch [243100/260984], Loss: 2.2704\n",
            "Train Epoch [4/4], Batch [243200/260984], Loss: 2.2222\n",
            "Train Epoch [4/4], Batch [243300/260984], Loss: 2.3801\n",
            "Train Epoch [4/4], Batch [243400/260984], Loss: 2.3635\n",
            "Train Epoch [4/4], Batch [243500/260984], Loss: 2.2359\n",
            "Train Epoch [4/4], Batch [243600/260984], Loss: 2.2253\n",
            "Train Epoch [4/4], Batch [243700/260984], Loss: 2.2480\n",
            "Train Epoch [4/4], Batch [243800/260984], Loss: 2.3310\n",
            "Train Epoch [4/4], Batch [243900/260984], Loss: 2.5037\n",
            "Train Epoch [4/4], Batch [244000/260984], Loss: 2.1701\n",
            "Train Epoch [4/4], Batch [244100/260984], Loss: 2.1593\n",
            "Train Epoch [4/4], Batch [244200/260984], Loss: 2.0961\n",
            "Train Epoch [4/4], Batch [244300/260984], Loss: 2.3453\n",
            "Train Epoch [4/4], Batch [244400/260984], Loss: 2.1464\n",
            "Train Epoch [4/4], Batch [244500/260984], Loss: 2.3419\n",
            "Train Epoch [4/4], Batch [244600/260984], Loss: 2.3734\n",
            "Train Epoch [4/4], Batch [244700/260984], Loss: 2.4453\n",
            "Train Epoch [4/4], Batch [244800/260984], Loss: 2.3400\n",
            "Train Epoch [4/4], Batch [244900/260984], Loss: 2.1046\n",
            "Train Epoch [4/4], Batch [245000/260984], Loss: 2.2634\n",
            "Train Epoch [4/4], Batch [245100/260984], Loss: 2.4849\n",
            "Train Epoch [4/4], Batch [245200/260984], Loss: 2.2435\n",
            "Train Epoch [4/4], Batch [245300/260984], Loss: 2.3153\n",
            "Train Epoch [4/4], Batch [245400/260984], Loss: 2.2716\n",
            "Train Epoch [4/4], Batch [245500/260984], Loss: 2.2207\n",
            "Train Epoch [4/4], Batch [245600/260984], Loss: 2.2930\n",
            "Train Epoch [4/4], Batch [245700/260984], Loss: 2.2483\n",
            "Train Epoch [4/4], Batch [245800/260984], Loss: 2.2836\n",
            "Train Epoch [4/4], Batch [245900/260984], Loss: 2.2608\n",
            "Train Epoch [4/4], Batch [246000/260984], Loss: 2.2539\n",
            "Train Epoch [4/4], Batch [246100/260984], Loss: 2.3299\n",
            "Train Epoch [4/4], Batch [246200/260984], Loss: 2.2528\n",
            "Train Epoch [4/4], Batch [246300/260984], Loss: 2.4743\n",
            "Train Epoch [4/4], Batch [246400/260984], Loss: 2.2350\n",
            "Train Epoch [4/4], Batch [246500/260984], Loss: 2.3570\n",
            "Train Epoch [4/4], Batch [246600/260984], Loss: 2.3981\n",
            "Train Epoch [4/4], Batch [246700/260984], Loss: 2.2280\n",
            "Train Epoch [4/4], Batch [246800/260984], Loss: 2.4165\n",
            "Train Epoch [4/4], Batch [246900/260984], Loss: 2.2833\n",
            "Train Epoch [4/4], Batch [247000/260984], Loss: 2.4047\n",
            "Train Epoch [4/4], Batch [247100/260984], Loss: 2.2841\n",
            "Train Epoch [4/4], Batch [247200/260984], Loss: 2.2945\n",
            "Train Epoch [4/4], Batch [247300/260984], Loss: 2.2172\n",
            "Train Epoch [4/4], Batch [247400/260984], Loss: 2.2611\n",
            "Train Epoch [4/4], Batch [247500/260984], Loss: 2.2836\n",
            "Train Epoch [4/4], Batch [247600/260984], Loss: 2.4106\n",
            "Train Epoch [4/4], Batch [247700/260984], Loss: 2.3326\n",
            "Train Epoch [4/4], Batch [247800/260984], Loss: 2.3718\n",
            "Train Epoch [4/4], Batch [247900/260984], Loss: 2.2887\n",
            "Train Epoch [4/4], Batch [248000/260984], Loss: 2.1727\n",
            "Train Epoch [4/4], Batch [248100/260984], Loss: 2.2354\n",
            "Train Epoch [4/4], Batch [248200/260984], Loss: 2.4662\n",
            "Train Epoch [4/4], Batch [248300/260984], Loss: 2.2215\n",
            "Train Epoch [4/4], Batch [248400/260984], Loss: 2.1962\n",
            "Train Epoch [4/4], Batch [248500/260984], Loss: 2.2404\n",
            "Train Epoch [4/4], Batch [248600/260984], Loss: 2.3852\n",
            "Train Epoch [4/4], Batch [248700/260984], Loss: 2.2676\n",
            "Train Epoch [4/4], Batch [248800/260984], Loss: 2.0397\n",
            "Train Epoch [4/4], Batch [248900/260984], Loss: 2.2811\n",
            "Train Epoch [4/4], Batch [249000/260984], Loss: 2.2970\n",
            "Train Epoch [4/4], Batch [249100/260984], Loss: 2.4517\n",
            "Train Epoch [4/4], Batch [249200/260984], Loss: 2.4461\n",
            "Train Epoch [4/4], Batch [249300/260984], Loss: 2.2505\n",
            "Train Epoch [4/4], Batch [249400/260984], Loss: 2.3311\n",
            "Train Epoch [4/4], Batch [249500/260984], Loss: 2.2913\n",
            "Train Epoch [4/4], Batch [249600/260984], Loss: 2.3182\n",
            "Train Epoch [4/4], Batch [249700/260984], Loss: 2.3560\n",
            "Train Epoch [4/4], Batch [249800/260984], Loss: 2.3304\n",
            "Train Epoch [4/4], Batch [249900/260984], Loss: 2.3181\n",
            "Train Epoch [4/4], Batch [250000/260984], Loss: 2.4396\n",
            "Train Epoch [4/4], Batch [250100/260984], Loss: 2.2431\n",
            "Train Epoch [4/4], Batch [250200/260984], Loss: 2.4399\n",
            "Train Epoch [4/4], Batch [250300/260984], Loss: 2.3096\n",
            "Train Epoch [4/4], Batch [250400/260984], Loss: 2.3457\n",
            "Train Epoch [4/4], Batch [250500/260984], Loss: 2.2481\n",
            "Train Epoch [4/4], Batch [250600/260984], Loss: 2.2839\n",
            "Train Epoch [4/4], Batch [250700/260984], Loss: 2.3424\n",
            "Train Epoch [4/4], Batch [250800/260984], Loss: 2.3201\n",
            "Train Epoch [4/4], Batch [250900/260984], Loss: 2.2551\n",
            "Train Epoch [4/4], Batch [251000/260984], Loss: 2.2930\n",
            "Train Epoch [4/4], Batch [251100/260984], Loss: 2.2748\n",
            "Train Epoch [4/4], Batch [251200/260984], Loss: 2.3356\n",
            "Train Epoch [4/4], Batch [251300/260984], Loss: 2.2218\n",
            "Train Epoch [4/4], Batch [251400/260984], Loss: 2.1961\n",
            "Train Epoch [4/4], Batch [251500/260984], Loss: 2.2888\n",
            "Train Epoch [4/4], Batch [251600/260984], Loss: 2.3200\n",
            "Train Epoch [4/4], Batch [251700/260984], Loss: 2.3179\n",
            "Train Epoch [4/4], Batch [251800/260984], Loss: 2.3193\n",
            "Train Epoch [4/4], Batch [251900/260984], Loss: 2.3378\n",
            "Train Epoch [4/4], Batch [252000/260984], Loss: 2.2900\n",
            "Train Epoch [4/4], Batch [252100/260984], Loss: 2.2511\n",
            "Train Epoch [4/4], Batch [252200/260984], Loss: 2.0978\n",
            "Train Epoch [4/4], Batch [252300/260984], Loss: 2.2656\n",
            "Train Epoch [4/4], Batch [252400/260984], Loss: 2.2779\n",
            "Train Epoch [4/4], Batch [252500/260984], Loss: 2.1304\n",
            "Train Epoch [4/4], Batch [252600/260984], Loss: 2.2515\n",
            "Train Epoch [4/4], Batch [252700/260984], Loss: 2.3138\n",
            "Train Epoch [4/4], Batch [252800/260984], Loss: 2.3203\n",
            "Train Epoch [4/4], Batch [252900/260984], Loss: 2.1771\n",
            "Train Epoch [4/4], Batch [253000/260984], Loss: 2.1174\n",
            "Train Epoch [4/4], Batch [253100/260984], Loss: 2.2746\n",
            "Train Epoch [4/4], Batch [253200/260984], Loss: 2.3450\n",
            "Train Epoch [4/4], Batch [253300/260984], Loss: 2.3044\n",
            "Train Epoch [4/4], Batch [253400/260984], Loss: 2.3767\n",
            "Train Epoch [4/4], Batch [253500/260984], Loss: 2.4143\n",
            "Train Epoch [4/4], Batch [253600/260984], Loss: 2.2319\n",
            "Train Epoch [4/4], Batch [253700/260984], Loss: 2.3721\n",
            "Train Epoch [4/4], Batch [253800/260984], Loss: 2.4600\n",
            "Train Epoch [4/4], Batch [253900/260984], Loss: 2.3807\n",
            "Train Epoch [4/4], Batch [254000/260984], Loss: 2.1664\n",
            "Train Epoch [4/4], Batch [254100/260984], Loss: 2.2823\n",
            "Train Epoch [4/4], Batch [254200/260984], Loss: 2.2877\n",
            "Train Epoch [4/4], Batch [254300/260984], Loss: 2.2548\n",
            "Train Epoch [4/4], Batch [254400/260984], Loss: 2.2090\n",
            "Train Epoch [4/4], Batch [254500/260984], Loss: 2.2607\n",
            "Train Epoch [4/4], Batch [254600/260984], Loss: 2.2310\n",
            "Train Epoch [4/4], Batch [254700/260984], Loss: 2.4061\n",
            "Train Epoch [4/4], Batch [254800/260984], Loss: 2.5442\n",
            "Train Epoch [4/4], Batch [254900/260984], Loss: 2.3125\n",
            "Train Epoch [4/4], Batch [255000/260984], Loss: 2.3415\n",
            "Train Epoch [4/4], Batch [255100/260984], Loss: 2.3896\n",
            "Train Epoch [4/4], Batch [255200/260984], Loss: 2.3858\n",
            "Train Epoch [4/4], Batch [255300/260984], Loss: 2.1470\n",
            "Train Epoch [4/4], Batch [255400/260984], Loss: 2.2153\n",
            "Train Epoch [4/4], Batch [255500/260984], Loss: 2.3809\n",
            "Train Epoch [4/4], Batch [255600/260984], Loss: 2.2876\n",
            "Train Epoch [4/4], Batch [255700/260984], Loss: 2.3470\n",
            "Train Epoch [4/4], Batch [255800/260984], Loss: 2.1936\n",
            "Train Epoch [4/4], Batch [255900/260984], Loss: 2.2868\n",
            "Train Epoch [4/4], Batch [256000/260984], Loss: 2.2632\n",
            "Train Epoch [4/4], Batch [256100/260984], Loss: 2.4241\n",
            "Train Epoch [4/4], Batch [256200/260984], Loss: 2.1870\n",
            "Train Epoch [4/4], Batch [256300/260984], Loss: 2.4663\n",
            "Train Epoch [4/4], Batch [256400/260984], Loss: 2.4098\n",
            "Train Epoch [4/4], Batch [256500/260984], Loss: 2.3692\n",
            "Train Epoch [4/4], Batch [256600/260984], Loss: 2.3865\n",
            "Train Epoch [4/4], Batch [256700/260984], Loss: 2.3633\n",
            "Train Epoch [4/4], Batch [256800/260984], Loss: 2.2988\n",
            "Train Epoch [4/4], Batch [256900/260984], Loss: 2.3023\n",
            "Train Epoch [4/4], Batch [257000/260984], Loss: 2.2516\n",
            "Train Epoch [4/4], Batch [257100/260984], Loss: 2.2811\n",
            "Train Epoch [4/4], Batch [257200/260984], Loss: 2.3820\n",
            "Train Epoch [4/4], Batch [257300/260984], Loss: 2.3098\n",
            "Train Epoch [4/4], Batch [257400/260984], Loss: 2.2692\n",
            "Train Epoch [4/4], Batch [257500/260984], Loss: 2.3473\n",
            "Train Epoch [4/4], Batch [257600/260984], Loss: 2.4153\n",
            "Train Epoch [4/4], Batch [257700/260984], Loss: 2.2142\n",
            "Train Epoch [4/4], Batch [257800/260984], Loss: 2.5263\n",
            "Train Epoch [4/4], Batch [257900/260984], Loss: 2.3617\n",
            "Train Epoch [4/4], Batch [258000/260984], Loss: 2.3533\n",
            "Train Epoch [4/4], Batch [258100/260984], Loss: 2.1641\n",
            "Train Epoch [4/4], Batch [258200/260984], Loss: 2.1596\n",
            "Train Epoch [4/4], Batch [258300/260984], Loss: 2.0992\n",
            "Train Epoch [4/4], Batch [258400/260984], Loss: 2.4860\n",
            "Train Epoch [4/4], Batch [258500/260984], Loss: 2.2390\n",
            "Train Epoch [4/4], Batch [258600/260984], Loss: 2.3145\n",
            "Train Epoch [4/4], Batch [258700/260984], Loss: 2.3381\n",
            "Train Epoch [4/4], Batch [258800/260984], Loss: 2.2396\n",
            "Train Epoch [4/4], Batch [258900/260984], Loss: 2.3570\n",
            "Train Epoch [4/4], Batch [259000/260984], Loss: 2.2238\n",
            "Train Epoch [4/4], Batch [259100/260984], Loss: 2.4013\n",
            "Train Epoch [4/4], Batch [259200/260984], Loss: 2.3630\n",
            "Train Epoch [4/4], Batch [259300/260984], Loss: 2.3751\n",
            "Train Epoch [4/4], Batch [259400/260984], Loss: 2.4054\n",
            "Train Epoch [4/4], Batch [259500/260984], Loss: 2.4293\n",
            "Train Epoch [4/4], Batch [259600/260984], Loss: 2.2706\n",
            "Train Epoch [4/4], Batch [259700/260984], Loss: 2.4508\n",
            "Train Epoch [4/4], Batch [259800/260984], Loss: 2.2908\n",
            "Train Epoch [4/4], Batch [259900/260984], Loss: 2.3222\n",
            "Train Epoch [4/4], Batch [260000/260984], Loss: 2.3493\n",
            "Train Epoch [4/4], Batch [260100/260984], Loss: 2.2562\n",
            "Train Epoch [4/4], Batch [260200/260984], Loss: 2.4139\n",
            "Train Epoch [4/4], Batch [260300/260984], Loss: 2.4408\n",
            "Train Epoch [4/4], Batch [260400/260984], Loss: 2.3698\n",
            "Train Epoch [4/4], Batch [260500/260984], Loss: 2.3363\n",
            "Train Epoch [4/4], Batch [260600/260984], Loss: 2.4443\n",
            "Train Epoch [4/4], Batch [260700/260984], Loss: 2.1862\n",
            "Train Epoch [4/4], Batch [260800/260984], Loss: 2.3631\n",
            "Train Epoch [4/4], Batch [260900/260984], Loss: 2.4347\n",
            "Validation Epoch [4/4], Batch [0/13160], Loss: 2.4862\n",
            "Validation Epoch [4/4], Batch [50/13160], Loss: 2.4572\n",
            "Validation Epoch [4/4], Batch [100/13160], Loss: 2.4758\n",
            "Validation Epoch [4/4], Batch [150/13160], Loss: 2.4483\n",
            "Validation Epoch [4/4], Batch [200/13160], Loss: 2.5072\n",
            "Validation Epoch [4/4], Batch [250/13160], Loss: 2.4443\n",
            "Validation Epoch [4/4], Batch [300/13160], Loss: 2.6065\n",
            "Validation Epoch [4/4], Batch [350/13160], Loss: 2.4796\n",
            "Validation Epoch [4/4], Batch [400/13160], Loss: 2.5593\n",
            "Validation Epoch [4/4], Batch [450/13160], Loss: 2.4061\n",
            "Validation Epoch [4/4], Batch [500/13160], Loss: 2.5369\n",
            "Validation Epoch [4/4], Batch [550/13160], Loss: 2.5876\n",
            "Validation Epoch [4/4], Batch [600/13160], Loss: 2.5614\n",
            "Validation Epoch [4/4], Batch [650/13160], Loss: 2.4348\n",
            "Validation Epoch [4/4], Batch [700/13160], Loss: 2.4447\n",
            "Validation Epoch [4/4], Batch [750/13160], Loss: 2.4061\n",
            "Validation Epoch [4/4], Batch [800/13160], Loss: 2.4578\n",
            "Validation Epoch [4/4], Batch [850/13160], Loss: 2.3246\n",
            "Validation Epoch [4/4], Batch [900/13160], Loss: 2.5805\n",
            "Validation Epoch [4/4], Batch [950/13160], Loss: 2.5799\n",
            "Validation Epoch [4/4], Batch [1000/13160], Loss: 2.5990\n",
            "Validation Epoch [4/4], Batch [1050/13160], Loss: 2.5267\n",
            "Validation Epoch [4/4], Batch [1100/13160], Loss: 2.5089\n",
            "Validation Epoch [4/4], Batch [1150/13160], Loss: 2.4826\n",
            "Validation Epoch [4/4], Batch [1200/13160], Loss: 2.3925\n",
            "Validation Epoch [4/4], Batch [1250/13160], Loss: 2.5101\n",
            "Validation Epoch [4/4], Batch [1300/13160], Loss: 2.3236\n",
            "Validation Epoch [4/4], Batch [1350/13160], Loss: 2.6581\n",
            "Validation Epoch [4/4], Batch [1400/13160], Loss: 2.3470\n",
            "Validation Epoch [4/4], Batch [1450/13160], Loss: 2.4504\n",
            "Validation Epoch [4/4], Batch [1500/13160], Loss: 2.5105\n",
            "Validation Epoch [4/4], Batch [1550/13160], Loss: 2.4659\n",
            "Validation Epoch [4/4], Batch [1600/13160], Loss: 2.5487\n",
            "Validation Epoch [4/4], Batch [1650/13160], Loss: 2.5471\n",
            "Validation Epoch [4/4], Batch [1700/13160], Loss: 2.5858\n",
            "Validation Epoch [4/4], Batch [1750/13160], Loss: 2.5306\n",
            "Validation Epoch [4/4], Batch [1800/13160], Loss: 2.5244\n",
            "Validation Epoch [4/4], Batch [1850/13160], Loss: 2.3545\n",
            "Validation Epoch [4/4], Batch [1900/13160], Loss: 2.4112\n",
            "Validation Epoch [4/4], Batch [1950/13160], Loss: 2.6523\n",
            "Validation Epoch [4/4], Batch [2000/13160], Loss: 2.5734\n",
            "Validation Epoch [4/4], Batch [2050/13160], Loss: 2.5181\n",
            "Validation Epoch [4/4], Batch [2100/13160], Loss: 2.3876\n",
            "Validation Epoch [4/4], Batch [2150/13160], Loss: 2.5074\n",
            "Validation Epoch [4/4], Batch [2200/13160], Loss: 2.3746\n",
            "Validation Epoch [4/4], Batch [2250/13160], Loss: 2.5872\n",
            "Validation Epoch [4/4], Batch [2300/13160], Loss: 2.3079\n",
            "Validation Epoch [4/4], Batch [2350/13160], Loss: 2.5483\n",
            "Validation Epoch [4/4], Batch [2400/13160], Loss: 2.3981\n",
            "Validation Epoch [4/4], Batch [2450/13160], Loss: 2.4359\n",
            "Validation Epoch [4/4], Batch [2500/13160], Loss: 2.4833\n",
            "Validation Epoch [4/4], Batch [2550/13160], Loss: 2.5244\n",
            "Validation Epoch [4/4], Batch [2600/13160], Loss: 2.5315\n",
            "Validation Epoch [4/4], Batch [2650/13160], Loss: 2.3767\n",
            "Validation Epoch [4/4], Batch [2700/13160], Loss: 2.6207\n",
            "Validation Epoch [4/4], Batch [2750/13160], Loss: 2.5615\n",
            "Validation Epoch [4/4], Batch [2800/13160], Loss: 2.5531\n",
            "Validation Epoch [4/4], Batch [2850/13160], Loss: 2.5296\n",
            "Validation Epoch [4/4], Batch [2900/13160], Loss: 2.5106\n",
            "Validation Epoch [4/4], Batch [2950/13160], Loss: 2.6046\n",
            "Validation Epoch [4/4], Batch [3000/13160], Loss: 2.5062\n",
            "Validation Epoch [4/4], Batch [3050/13160], Loss: 2.4547\n",
            "Validation Epoch [4/4], Batch [3100/13160], Loss: 2.4722\n",
            "Validation Epoch [4/4], Batch [3150/13160], Loss: 2.5152\n",
            "Validation Epoch [4/4], Batch [3200/13160], Loss: 2.4411\n",
            "Validation Epoch [4/4], Batch [3250/13160], Loss: 2.4140\n",
            "Validation Epoch [4/4], Batch [3300/13160], Loss: 2.5650\n",
            "Validation Epoch [4/4], Batch [3350/13160], Loss: 2.5158\n",
            "Validation Epoch [4/4], Batch [3400/13160], Loss: 2.4636\n",
            "Validation Epoch [4/4], Batch [3450/13160], Loss: 2.5387\n",
            "Validation Epoch [4/4], Batch [3500/13160], Loss: 2.6015\n",
            "Validation Epoch [4/4], Batch [3550/13160], Loss: 2.4681\n",
            "Validation Epoch [4/4], Batch [3600/13160], Loss: 2.4477\n",
            "Validation Epoch [4/4], Batch [3650/13160], Loss: 2.5623\n",
            "Validation Epoch [4/4], Batch [3700/13160], Loss: 2.5374\n",
            "Validation Epoch [4/4], Batch [3750/13160], Loss: 2.5461\n",
            "Validation Epoch [4/4], Batch [3800/13160], Loss: 2.4591\n",
            "Validation Epoch [4/4], Batch [3850/13160], Loss: 2.5292\n",
            "Validation Epoch [4/4], Batch [3900/13160], Loss: 2.5653\n",
            "Validation Epoch [4/4], Batch [3950/13160], Loss: 2.4786\n",
            "Validation Epoch [4/4], Batch [4000/13160], Loss: 2.4581\n",
            "Validation Epoch [4/4], Batch [4050/13160], Loss: 2.4581\n",
            "Validation Epoch [4/4], Batch [4100/13160], Loss: 2.4191\n",
            "Validation Epoch [4/4], Batch [4150/13160], Loss: 2.4732\n",
            "Validation Epoch [4/4], Batch [4200/13160], Loss: 2.4875\n",
            "Validation Epoch [4/4], Batch [4250/13160], Loss: 2.5056\n",
            "Validation Epoch [4/4], Batch [4300/13160], Loss: 2.4807\n",
            "Validation Epoch [4/4], Batch [4350/13160], Loss: 2.3391\n",
            "Validation Epoch [4/4], Batch [4400/13160], Loss: 2.3857\n",
            "Validation Epoch [4/4], Batch [4450/13160], Loss: 2.4291\n",
            "Validation Epoch [4/4], Batch [4500/13160], Loss: 2.4354\n",
            "Validation Epoch [4/4], Batch [4550/13160], Loss: 2.4004\n",
            "Validation Epoch [4/4], Batch [4600/13160], Loss: 2.6059\n",
            "Validation Epoch [4/4], Batch [4650/13160], Loss: 2.5658\n",
            "Validation Epoch [4/4], Batch [4700/13160], Loss: 2.5636\n",
            "Validation Epoch [4/4], Batch [4750/13160], Loss: 2.5530\n",
            "Validation Epoch [4/4], Batch [4800/13160], Loss: 2.3988\n",
            "Validation Epoch [4/4], Batch [4850/13160], Loss: 2.6656\n",
            "Validation Epoch [4/4], Batch [4900/13160], Loss: 2.5799\n",
            "Validation Epoch [4/4], Batch [4950/13160], Loss: 2.5667\n",
            "Validation Epoch [4/4], Batch [5000/13160], Loss: 2.4563\n",
            "Validation Epoch [4/4], Batch [5050/13160], Loss: 2.5908\n",
            "Validation Epoch [4/4], Batch [5100/13160], Loss: 2.4285\n",
            "Validation Epoch [4/4], Batch [5150/13160], Loss: 2.4932\n",
            "Validation Epoch [4/4], Batch [5200/13160], Loss: 2.4693\n",
            "Validation Epoch [4/4], Batch [5250/13160], Loss: 2.5047\n",
            "Validation Epoch [4/4], Batch [5300/13160], Loss: 2.4431\n",
            "Validation Epoch [4/4], Batch [5350/13160], Loss: 2.5111\n",
            "Validation Epoch [4/4], Batch [5400/13160], Loss: 2.5193\n",
            "Validation Epoch [4/4], Batch [5450/13160], Loss: 2.4947\n",
            "Validation Epoch [4/4], Batch [5500/13160], Loss: 2.5349\n",
            "Validation Epoch [4/4], Batch [5550/13160], Loss: 2.4755\n",
            "Validation Epoch [4/4], Batch [5600/13160], Loss: 2.4898\n",
            "Validation Epoch [4/4], Batch [5650/13160], Loss: 2.4905\n",
            "Validation Epoch [4/4], Batch [5700/13160], Loss: 2.4467\n",
            "Validation Epoch [4/4], Batch [5750/13160], Loss: 2.4501\n",
            "Validation Epoch [4/4], Batch [5800/13160], Loss: 2.5494\n",
            "Validation Epoch [4/4], Batch [5850/13160], Loss: 2.4832\n",
            "Validation Epoch [4/4], Batch [5900/13160], Loss: 2.5450\n",
            "Validation Epoch [4/4], Batch [5950/13160], Loss: 2.5972\n",
            "Validation Epoch [4/4], Batch [6000/13160], Loss: 2.5343\n",
            "Validation Epoch [4/4], Batch [6050/13160], Loss: 2.4286\n",
            "Validation Epoch [4/4], Batch [6100/13160], Loss: 2.4612\n",
            "Validation Epoch [4/4], Batch [6150/13160], Loss: 2.4438\n",
            "Validation Epoch [4/4], Batch [6200/13160], Loss: 2.5765\n",
            "Validation Epoch [4/4], Batch [6250/13160], Loss: 2.4293\n",
            "Validation Epoch [4/4], Batch [6300/13160], Loss: 2.4933\n",
            "Validation Epoch [4/4], Batch [6350/13160], Loss: 2.3994\n",
            "Validation Epoch [4/4], Batch [6400/13160], Loss: 2.4070\n",
            "Validation Epoch [4/4], Batch [6450/13160], Loss: 2.5150\n",
            "Validation Epoch [4/4], Batch [6500/13160], Loss: 2.5528\n",
            "Validation Epoch [4/4], Batch [6550/13160], Loss: 2.5759\n",
            "Validation Epoch [4/4], Batch [6600/13160], Loss: 2.4924\n",
            "Validation Epoch [4/4], Batch [6650/13160], Loss: 2.4134\n",
            "Validation Epoch [4/4], Batch [6700/13160], Loss: 2.5281\n",
            "Validation Epoch [4/4], Batch [6750/13160], Loss: 2.3836\n",
            "Validation Epoch [4/4], Batch [6800/13160], Loss: 2.6444\n",
            "Validation Epoch [4/4], Batch [6850/13160], Loss: 2.5636\n",
            "Validation Epoch [4/4], Batch [6900/13160], Loss: 2.6305\n",
            "Validation Epoch [4/4], Batch [6950/13160], Loss: 2.4729\n",
            "Validation Epoch [4/4], Batch [7000/13160], Loss: 2.4333\n",
            "Validation Epoch [4/4], Batch [7050/13160], Loss: 2.6282\n",
            "Validation Epoch [4/4], Batch [7100/13160], Loss: 2.5111\n",
            "Validation Epoch [4/4], Batch [7150/13160], Loss: 2.6256\n",
            "Validation Epoch [4/4], Batch [7200/13160], Loss: 2.4410\n",
            "Validation Epoch [4/4], Batch [7250/13160], Loss: 2.4476\n",
            "Validation Epoch [4/4], Batch [7300/13160], Loss: 2.4873\n",
            "Validation Epoch [4/4], Batch [7350/13160], Loss: 2.5000\n",
            "Validation Epoch [4/4], Batch [7400/13160], Loss: 2.4267\n",
            "Validation Epoch [4/4], Batch [7450/13160], Loss: 2.4484\n",
            "Validation Epoch [4/4], Batch [7500/13160], Loss: 2.4982\n",
            "Validation Epoch [4/4], Batch [7550/13160], Loss: 2.4578\n",
            "Validation Epoch [4/4], Batch [7600/13160], Loss: 2.5288\n",
            "Validation Epoch [4/4], Batch [7650/13160], Loss: 2.4514\n",
            "Validation Epoch [4/4], Batch [7700/13160], Loss: 2.4722\n",
            "Validation Epoch [4/4], Batch [7750/13160], Loss: 2.4907\n",
            "Validation Epoch [4/4], Batch [7800/13160], Loss: 2.4390\n",
            "Validation Epoch [4/4], Batch [7850/13160], Loss: 2.6006\n",
            "Validation Epoch [4/4], Batch [7900/13160], Loss: 2.4014\n",
            "Validation Epoch [4/4], Batch [7950/13160], Loss: 2.4628\n",
            "Validation Epoch [4/4], Batch [8000/13160], Loss: 2.4958\n",
            "Validation Epoch [4/4], Batch [8050/13160], Loss: 2.4708\n",
            "Validation Epoch [4/4], Batch [8100/13160], Loss: 2.4451\n",
            "Validation Epoch [4/4], Batch [8150/13160], Loss: 2.5562\n",
            "Validation Epoch [4/4], Batch [8200/13160], Loss: 2.5918\n",
            "Validation Epoch [4/4], Batch [8250/13160], Loss: 2.5995\n",
            "Validation Epoch [4/4], Batch [8300/13160], Loss: 2.5086\n",
            "Validation Epoch [4/4], Batch [8350/13160], Loss: 2.4285\n",
            "Validation Epoch [4/4], Batch [8400/13160], Loss: 2.5976\n",
            "Validation Epoch [4/4], Batch [8450/13160], Loss: 2.3683\n",
            "Validation Epoch [4/4], Batch [8500/13160], Loss: 2.5256\n",
            "Validation Epoch [4/4], Batch [8550/13160], Loss: 2.5964\n",
            "Validation Epoch [4/4], Batch [8600/13160], Loss: 2.4318\n",
            "Validation Epoch [4/4], Batch [8650/13160], Loss: 2.4129\n",
            "Validation Epoch [4/4], Batch [8700/13160], Loss: 2.5345\n",
            "Validation Epoch [4/4], Batch [8750/13160], Loss: 2.5699\n",
            "Validation Epoch [4/4], Batch [8800/13160], Loss: 2.6197\n",
            "Validation Epoch [4/4], Batch [8850/13160], Loss: 2.5793\n",
            "Validation Epoch [4/4], Batch [8900/13160], Loss: 2.5085\n",
            "Validation Epoch [4/4], Batch [8950/13160], Loss: 2.4761\n",
            "Validation Epoch [4/4], Batch [9000/13160], Loss: 2.4730\n",
            "Validation Epoch [4/4], Batch [9050/13160], Loss: 2.5173\n",
            "Validation Epoch [4/4], Batch [9100/13160], Loss: 2.4272\n",
            "Validation Epoch [4/4], Batch [9150/13160], Loss: 2.5273\n",
            "Validation Epoch [4/4], Batch [9200/13160], Loss: 2.5062\n",
            "Validation Epoch [4/4], Batch [9250/13160], Loss: 2.6076\n",
            "Validation Epoch [4/4], Batch [9300/13160], Loss: 2.4717\n",
            "Validation Epoch [4/4], Batch [9350/13160], Loss: 2.5195\n",
            "Validation Epoch [4/4], Batch [9400/13160], Loss: 2.5387\n",
            "Validation Epoch [4/4], Batch [9450/13160], Loss: 2.6146\n",
            "Validation Epoch [4/4], Batch [9500/13160], Loss: 2.3333\n",
            "Validation Epoch [4/4], Batch [9550/13160], Loss: 2.5113\n",
            "Validation Epoch [4/4], Batch [9600/13160], Loss: 2.5698\n",
            "Validation Epoch [4/4], Batch [9650/13160], Loss: 2.4569\n",
            "Validation Epoch [4/4], Batch [9700/13160], Loss: 2.5127\n",
            "Validation Epoch [4/4], Batch [9750/13160], Loss: 2.4444\n",
            "Validation Epoch [4/4], Batch [9800/13160], Loss: 2.4425\n",
            "Validation Epoch [4/4], Batch [9850/13160], Loss: 2.5928\n",
            "Validation Epoch [4/4], Batch [9900/13160], Loss: 2.5023\n",
            "Validation Epoch [4/4], Batch [9950/13160], Loss: 2.5786\n",
            "Validation Epoch [4/4], Batch [10000/13160], Loss: 2.4260\n",
            "Validation Epoch [4/4], Batch [10050/13160], Loss: 2.4681\n",
            "Validation Epoch [4/4], Batch [10100/13160], Loss: 2.4729\n",
            "Validation Epoch [4/4], Batch [10150/13160], Loss: 2.5276\n",
            "Validation Epoch [4/4], Batch [10200/13160], Loss: 2.5508\n",
            "Validation Epoch [4/4], Batch [10250/13160], Loss: 2.4425\n",
            "Validation Epoch [4/4], Batch [10300/13160], Loss: 2.5732\n",
            "Validation Epoch [4/4], Batch [10350/13160], Loss: 2.4099\n",
            "Validation Epoch [4/4], Batch [10400/13160], Loss: 2.4546\n",
            "Validation Epoch [4/4], Batch [10450/13160], Loss: 2.3647\n",
            "Validation Epoch [4/4], Batch [10500/13160], Loss: 2.5874\n",
            "Validation Epoch [4/4], Batch [10550/13160], Loss: 2.4858\n",
            "Validation Epoch [4/4], Batch [10600/13160], Loss: 2.5077\n",
            "Validation Epoch [4/4], Batch [10650/13160], Loss: 2.4331\n",
            "Validation Epoch [4/4], Batch [10700/13160], Loss: 2.4946\n",
            "Validation Epoch [4/4], Batch [10750/13160], Loss: 2.5146\n",
            "Validation Epoch [4/4], Batch [10800/13160], Loss: 2.5593\n",
            "Validation Epoch [4/4], Batch [10850/13160], Loss: 2.6443\n",
            "Validation Epoch [4/4], Batch [10900/13160], Loss: 2.4670\n",
            "Validation Epoch [4/4], Batch [10950/13160], Loss: 2.4460\n",
            "Validation Epoch [4/4], Batch [11000/13160], Loss: 2.5317\n",
            "Validation Epoch [4/4], Batch [11050/13160], Loss: 2.5110\n",
            "Validation Epoch [4/4], Batch [11100/13160], Loss: 2.5403\n",
            "Validation Epoch [4/4], Batch [11150/13160], Loss: 2.4617\n",
            "Validation Epoch [4/4], Batch [11200/13160], Loss: 2.6437\n",
            "Validation Epoch [4/4], Batch [11250/13160], Loss: 2.6383\n",
            "Validation Epoch [4/4], Batch [11300/13160], Loss: 2.3497\n",
            "Validation Epoch [4/4], Batch [11350/13160], Loss: 2.3661\n",
            "Validation Epoch [4/4], Batch [11400/13160], Loss: 2.4552\n",
            "Validation Epoch [4/4], Batch [11450/13160], Loss: 2.4891\n",
            "Validation Epoch [4/4], Batch [11500/13160], Loss: 2.4772\n",
            "Validation Epoch [4/4], Batch [11550/13160], Loss: 2.6137\n",
            "Validation Epoch [4/4], Batch [11600/13160], Loss: 2.4325\n",
            "Validation Epoch [4/4], Batch [11650/13160], Loss: 2.3560\n",
            "Validation Epoch [4/4], Batch [11700/13160], Loss: 2.4548\n",
            "Validation Epoch [4/4], Batch [11750/13160], Loss: 2.5275\n",
            "Validation Epoch [4/4], Batch [11800/13160], Loss: 2.6202\n",
            "Validation Epoch [4/4], Batch [11850/13160], Loss: 2.3690\n",
            "Validation Epoch [4/4], Batch [11900/13160], Loss: 2.4948\n",
            "Validation Epoch [4/4], Batch [11950/13160], Loss: 2.5929\n",
            "Validation Epoch [4/4], Batch [12000/13160], Loss: 2.4387\n",
            "Validation Epoch [4/4], Batch [12050/13160], Loss: 2.4890\n",
            "Validation Epoch [4/4], Batch [12100/13160], Loss: 2.4898\n",
            "Validation Epoch [4/4], Batch [12150/13160], Loss: 2.5702\n",
            "Validation Epoch [4/4], Batch [12200/13160], Loss: 2.4388\n",
            "Validation Epoch [4/4], Batch [12250/13160], Loss: 2.5591\n",
            "Validation Epoch [4/4], Batch [12300/13160], Loss: 2.5502\n",
            "Validation Epoch [4/4], Batch [12350/13160], Loss: 2.6088\n",
            "Validation Epoch [4/4], Batch [12400/13160], Loss: 2.5396\n",
            "Validation Epoch [4/4], Batch [12450/13160], Loss: 2.5107\n",
            "Validation Epoch [4/4], Batch [12500/13160], Loss: 2.4662\n",
            "Validation Epoch [4/4], Batch [12550/13160], Loss: 2.3784\n",
            "Validation Epoch [4/4], Batch [12600/13160], Loss: 2.4960\n",
            "Validation Epoch [4/4], Batch [12650/13160], Loss: 2.5588\n",
            "Validation Epoch [4/4], Batch [12700/13160], Loss: 2.7190\n",
            "Validation Epoch [4/4], Batch [12750/13160], Loss: 2.5737\n",
            "Validation Epoch [4/4], Batch [12800/13160], Loss: 2.4595\n",
            "Validation Epoch [4/4], Batch [12850/13160], Loss: 2.5706\n",
            "Validation Epoch [4/4], Batch [12900/13160], Loss: 2.3638\n",
            "Validation Epoch [4/4], Batch [12950/13160], Loss: 2.4008\n",
            "Validation Epoch [4/4], Batch [13000/13160], Loss: 2.5607\n",
            "Validation Epoch [4/4], Batch [13050/13160], Loss: 2.4881\n",
            "Validation Epoch [4/4], Batch [13100/13160], Loss: 2.5468\n",
            "Validation Epoch [4/4], Batch [13150/13160], Loss: 2.5106\n",
            "Epoch [4/4], Train Loss: 2.3057, CV Loss: 2.4943\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 50.98540496826172%\n",
            "CV Accuracy: 30.805818557739258%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: save model to pickle file\n",
        "import pickle\n",
        "with open('my_model.pkl', 'wb') as f:\n",
        "  pickle.dump(model, f)\n"
      ],
      "metadata": {
        "id": "lacX8quHWXIi"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: load pickle file\n",
        "\n",
        "with open('my_model.pkl', 'rb') as f:\n",
        "  model = pickle.load(f)\n"
      ],
      "metadata": {
        "id": "r9qmFHLouuif"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test set\n"
      ],
      "metadata": {
        "id": "XG_Or84ZGYLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vectors_labels_test(input_file_path):\n",
        "    with open(input_file_path, \"r\", encoding=\"utf-8\") as input_file:\n",
        "        input_text = input_file.read()\n",
        "\n",
        "    arabic_words = extract_arabic_words2(input_text)\n",
        "\n",
        "    words2 = arabic_words.split()\n",
        "    words_array2 = [list(word2) for word2 in words2]\n",
        "\n",
        "    output_without_spaces = arabic_words.replace(\" \", \"\")\n",
        "    array_of_chars = list(output_without_spaces)\n",
        "\n",
        "\n",
        "\n",
        "    num_feature = 30\n",
        "    min_word_count = 1\n",
        "    num_thread = 5\n",
        "    window_size = 10\n",
        "    down_sampling = 0.001\n",
        "    iteration = 20\n",
        "\n",
        "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "    model_fastText = FastText(words_array2,\n",
        "                            vector_size=num_feature,\n",
        "                            window=window_size,\n",
        "                            min_count=min_word_count,\n",
        "                            workers=num_thread)\n",
        "\n",
        "\n",
        "    j=0\n",
        "    chars =[]\n",
        "    chars_ids =[]\n",
        "    char_vectors =[]\n",
        "    for word in words_array2:\n",
        "        for char in word:\n",
        "            chars.append(char)\n",
        "            chars_ids.append(j)\n",
        "            vector = model_fastText.wv[char]\n",
        "            char_vectors.append(vector)\n",
        "            j=j+1\n",
        "    return chars, char_vectors\n",
        "\n",
        "#testing on test set\n",
        "test_chars , test_char_vectors = get_vectors_labels_test(\"test_no_diacritics.txt\")\n",
        "print(len(test_chars))\n",
        "print(len(test_char_vectors))\n",
        "print(test_chars[:4])\n",
        "print(test_char_vectors[:4])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1SN_xiy11kn",
        "outputId": "afc78dc7-9824-4fcc-c7d9-503b402a9323"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "417359\n",
            "417359\n",
            "['ل', 'ي', 'س', 'ل']\n",
            "[array([-0.06572159,  0.37408006, -0.00900088,  0.201948  , -0.6701244 ,\n",
            "       -0.15967184,  0.49180537, -0.17840898, -0.225788  , -0.11249827,\n",
            "        0.1526382 ,  0.25382394, -0.467847  ,  0.11619159,  0.02250389,\n",
            "       -0.07813775, -0.09147421, -0.05372789, -0.46988225,  0.18384594,\n",
            "        0.53914964,  0.00878673,  0.1135644 , -0.15464863,  0.72732115,\n",
            "       -0.1918299 ,  0.05576751,  0.26127416,  0.14233221, -0.5202197 ],\n",
            "      dtype=float32), array([ 0.0606834 , -0.29521686,  0.8848596 ,  0.04648639,  0.05363831,\n",
            "       -0.11667493, -0.8786195 ,  0.02480554, -0.2538165 ,  0.27396232,\n",
            "        0.29898322,  0.09297802, -0.07608764, -0.2286924 , -0.28239256,\n",
            "       -0.14778797,  0.07466491,  0.2739046 , -0.5420493 , -0.01865689,\n",
            "        0.22764352, -0.27037624, -0.17710064,  0.1410111 ,  0.23976085,\n",
            "        0.42523053, -0.00422423,  0.02255716, -0.18304661,  0.08887133],\n",
            "      dtype=float32), array([-0.4350845 , -0.23389651,  0.00677163, -0.27542293, -0.41822967,\n",
            "       -0.18113613, -0.11996029,  0.8541028 , -0.49357045, -1.0138905 ,\n",
            "       -0.35171106, -0.28696954,  0.39686516,  0.15567446,  0.07053178,\n",
            "        0.49533904,  0.38464516, -0.33256298, -0.34406108, -0.55893904,\n",
            "        0.28318095,  0.35695145,  0.2654658 ,  0.06957457,  0.06322252,\n",
            "        0.17880878, -0.10346176, -0.44817987, -0.2825017 , -0.2094058 ],\n",
            "      dtype=float32), array([-0.06572159,  0.37408006, -0.00900088,  0.201948  , -0.6701244 ,\n",
            "       -0.15967184,  0.49180537, -0.17840898, -0.225788  , -0.11249827,\n",
            "        0.1526382 ,  0.25382394, -0.467847  ,  0.11619159,  0.02250389,\n",
            "       -0.07813775, -0.09147421, -0.05372789, -0.46988225,  0.18384594,\n",
            "        0.53914964,  0.00878673,  0.1135644 , -0.15464863,  0.72732115,\n",
            "       -0.1918299 ,  0.05576751,  0.26127416,  0.14233221, -0.5202197 ],\n",
            "      dtype=float32)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#loading the model\n",
        "import pickle\n",
        "import io\n",
        "\n",
        "# class CPU_Unpickler(pickle.Unpickler):\n",
        "#     def find_class(self, module, name):\n",
        "#         if module == 'torch.storage' and name == '_load_from_bytes':\n",
        "#             return lambda b: torch.load(io.BytesIO(b), map_location='cpu')\n",
        "#         else:\n",
        "#             return super().find_class(module, name)\n",
        "# model = CPU_Unpickler(open('my_model.pkl', 'rb')).load()\n",
        "\n",
        "# prompt: load pickle file\n",
        "\n",
        "with open('my_model.pkl', 'rb') as f:\n",
        "  model = pickle.load(f)\n",
        "\n",
        "\n",
        "#predicting on test set\n",
        "def predict_test(model,test_char_vectors):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        test_char_vectors = torch.tensor(test_char_vectors).to(device)\n",
        "        test_char_vectors = test_char_vectors\n",
        "        outputs = model(test_char_vectors)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        predicted = predicted.tolist()\n",
        "        # for i in range(len(test_chars)):\n",
        "        #     test_chars[i] = test_chars[i] + NAME2DIACRITIC[DIACRITIC2NAME[predicted[i]]]\n",
        "        # test_chars = ''.join(test_chars)\n",
        "        return predicted\n",
        "predicted_chars = predict_test(model,test_char_vectors)\n",
        "print(len(predicted_chars))\n",
        "\n",
        "#saving the output to a file csv that has 2 columns (id, prediction)\n",
        "import csv\n",
        "j=0\n",
        "with open('output.csv', 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"ID\", \"label\"])\n",
        "    for i in range(len(test_chars)):\n",
        "        writer.writerow([i, predicted_chars[i]])\n",
        "        j=j+1\n",
        "\n",
        "\n",
        "print(j)"
      ],
      "metadata": {
        "id": "I5SL1ynCGZ25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f75fd3fe-1fc1-49ef-c649-ca8e972dc413"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:879: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "417359\n",
            "417359\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3v6CkEtUQcFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ShGqmBa5S3ow"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}