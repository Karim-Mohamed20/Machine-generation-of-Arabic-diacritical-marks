# -*- coding: utf-8 -*-
"""NLP_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VlAatdLSL4lrcqbGgWE67uW5p-ZUmtdt
"""

!wget --no-check-certificate -O example.zip https://drive.google.com/uc?id=1VTGxBlf-wyO6CjAKWikaVnZZ-YPQpYJB
!unzip example.zip

# Commented out IPython magic to ensure Python compatibility.
# %pip uninstall tensorflow
# %pip install tensorflow
# %pip install keras
# %pip install gensim
# %pip install nltk
# %pip install torch
# %pip install fasttext

import re
from collections import Counter
import pandas as pd
import numpy as np
# import tensorflow as tf
import nltk, re
# from keras.preprocessing.text import Tokenizer
from datetime import datetime
from gensim.models import *
import logging
import fasttext
# from rnn_utils import *
# %matplotlib inline


''' D_NAMES: This is a list containing names of various Arabic diacritics. Each
 element of the list represents a specific diacritic type. '''
D_NAMES = ['Fathatan', 'Dammatan', 'Kasratan', 'Fatha', 'Damma', 'Kasra', 'Shadda', 'Sukun']

##############################################################################################

''' NAME2DIACRITIC: This uses a dictionary comprehension to create a mapping
from diacritic names to their corresponding Unicode characters.'''
NAME2DIACRITIC = dict((name, chr(code)) for name, code in zip(D_NAMES, range(0x064B, 0x0653)))

##############################################################################################

''' DIACRITIC2NAME: This is the inverse of the previous dictionary.'''
DIACRITIC2NAME = dict((code, name) for name, code in NAME2DIACRITIC.items())

##############################################################################################

''' ARABIC_DIACRITICS: This creates a frozenset containing the Unicode
 characters of all the diacritics.'''
ARABIC_DIACRITICS = frozenset(NAME2DIACRITIC.values())


# Remove all standard diacritics from the text, leaving the letters only.
def clear_diacritics(text):
    assert isinstance(text, str)
    return ''.join([l for l in text if l not in ARABIC_DIACRITICS])


# Return the diacritics from the text while keeping their original positions.
def extract_diacritics(text):
    assert isinstance(text, str)
    diacritics = []
    classes = []
    temp = ''
    for i in range(1, len(text)):
        temp = ''
        if text[i] in ARABIC_DIACRITICS:
            if text[i-1] == NAME2DIACRITIC['Shadda']:
                diacritics[-1] = (DIACRITIC2NAME[text[i-1]], DIACRITIC2NAME[text[i]])
                temp = (DIACRITIC2NAME[text[i-1]], DIACRITIC2NAME[text[i]])
                if (temp == ('Shadda', 'Fatha')):
                    classes.pop()
                    classes.append(8)
                elif (temp == ('Shadda', 'Fathatan')):
                    classes.pop()
                    classes.append(9)
                elif (temp == ('Shadda', 'Damma')):
                    classes.pop()
                    classes.append(10)
                elif (temp == ('Shadda', 'Dammatan')):
                    classes.pop()
                    classes.append(11)
                elif (temp == ('Shadda', 'Kasra')):
                    classes.pop()
                    classes.append(12)
                elif (temp == ('Shadda', 'Kasratan')):
                    classes.pop()
                    classes.append(13)
            else:
                diacritics.append(DIACRITIC2NAME[text[i]])
                temp = DIACRITIC2NAME[text[i]]
                if (temp == 'Fatha'):
                    classes.append(0)
                elif (temp == 'Fathatan'):
                    classes.append(1)
                elif (temp == 'Damma'):
                    classes.append(2)
                elif (temp == 'Dammatan'):
                    classes.append(3)
                elif (temp == 'Kasra'):
                    classes.append(4)
                elif (temp == 'Kasratan'):
                    classes.append(5)
                elif (temp == 'Sukun'):
                    classes.append(6)
                elif (temp == 'Shadda'):
                    classes.append(7)
        elif text[i - 1] not in ARABIC_DIACRITICS:
            diacritics.append('')
            classes.append(14)

    if text[-1] not in ARABIC_DIACRITICS:
        diacritics.append('')
        classes.append(14)
    return diacritics, classes


def extract_arabic_words2(text):
    arabic_pattern = re.compile('[\u0600-\u06FF]+')
    arabic_matches = arabic_pattern.findall(text)
    result = ' '.join(arabic_matches)
    processed_text = re.sub(r'[؛،\.؟]+', '', result)
    final_processed_text = re.sub(r'\s+', ' ', processed_text)
    return final_processed_text


input_file_path = "train.txt"  # Replace with your input file path

def get_vectors_labels(input_file_path):
    with open(input_file_path, "r", encoding="utf-8") as input_file:
        input_text = input_file.read()

    arabic_words = extract_arabic_words2(input_text)

    output_words = clear_diacritics(arabic_words)
    words = output_words.split()
    words2 = arabic_words.split()
    words_array = [list(word) for word in words]
    words_array2 = [list(word2) for word2 in words2]

    output_without_spaces = arabic_words.replace(" ", "")
    output_without_spaces2 = output_words.replace(" ", "")
    array_of_chars = list(output_without_spaces)
    _,classes_extraction = extract_diacritics (output_without_spaces)


    num_feature = 30
    min_word_count = 1
    num_thread = 5
    window_size = 10
    down_sampling = 0.001
    iteration = 20

    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
    model_fastText = FastText(words_array,
                            vector_size=num_feature,
                            window=window_size,
                            min_count=min_word_count,
                            workers=num_thread)


    j=0
    chars =[]
    char_vectors =[]
    char_classes=[]
    for word in words_array:
        for char in word:
            chars.append(char)
            char_classes.append(classes_extraction[j])
            vector = model_fastText.wv[char]
            char_vectors.append(vector)
            j=j+1
    return chars, char_classes, char_vectors

# print (j)
# print(chars[1])
# print(char_classes[1])
# print(char_vectors[1])

train_chars , train_char_classes, train_char_vectors = get_vectors_labels(input_file_path)
cv_chars , cv_char_classes, cv_char_vectors = get_vectors_labels("val.txt")

print (len(train_chars))
print (len(train_char_classes))
print (len(train_char_vectors))
print (train_chars[1])
print (train_char_classes[1])
print (train_char_vectors[1])

print (len(cv_chars))
print (len(cv_char_classes))
print (len(cv_char_vectors))

# old code

# !pip install tensorflow
# !pip install keras
# !pip install gensim
# import re
# from collections import Counter
# import pandas as pd
# import numpy as np
# import tensorflow as tf
# import nltk, re
# from keras.preprocessing.text import Tokenizer
# from datetime import datetime
# from gensim.models import *
# import logging
# # from rnn_utils import *
# %matplotlib inline


# ''' D_NAMES: This is a list containing names of various Arabic diacritics. Each
#  element of the list represents a specific diacritic type. '''
# D_NAMES = ['Fathatan', 'Dammatan', 'Kasratan', 'Fatha', 'Damma', 'Kasra', 'Shadda', 'Sukun']

# ##############################################################################################

# ''' NAME2DIACRITIC: This uses a dictionary comprehension to create a mapping
# from diacritic names to their corresponding Unicode characters.'''
# NAME2DIACRITIC = dict((name, chr(code)) for name, code in zip(D_NAMES, range(0x064B, 0x0653)))

# ##############################################################################################

# ''' DIACRITIC2NAME: This is the inverse of the previous dictionary.'''
# DIACRITIC2NAME = dict((code, name) for name, code in NAME2DIACRITIC.items())

# ##############################################################################################

# ''' ARABIC_DIACRITICS: This creates a frozenset containing the Unicode
#  characters of all the diacritics.'''
# ARABIC_DIACRITICS = frozenset(NAME2DIACRITIC.values())


# # Remove all standard diacritics from the text, leaving the letters only.
# def clear_diacritics(text):
#     assert isinstance(text, str)
#     return ''.join([l for l in text if l not in ARABIC_DIACRITICS])


# # Return the diacritics from the text while keeping their original positions.
# def extract_diacritics(text):
#     assert isinstance(text, str)
#     diacritics = []
#     classes = []
#     temp = ''
#     for i in range(1, len(text)):
#         temp = ''
#         if text[i] in ARABIC_DIACRITICS:
#             if text[i-1] == NAME2DIACRITIC['Shadda']:
#                 diacritics[-1] = (DIACRITIC2NAME[text[i-1]], DIACRITIC2NAME[text[i]])
#                 temp = (DIACRITIC2NAME[text[i-1]], DIACRITIC2NAME[text[i]])
#                 if (temp == ('Shadda', 'Fatha')):
#                     classes.pop()
#                     classes.append(8)
#                 elif (temp == ('Shadda', 'Fathatan')):
#                     classes.pop()
#                     classes.append(9)
#                 elif (temp == ('Shadda', 'Damma')):
#                     classes.pop()
#                     classes.append(10)
#                 elif (temp == ('Shadda', 'Dammatan')):
#                     classes.pop()
#                     classes.append(11)
#                 elif (temp == ('Shadda', 'Kasra')):
#                     classes.pop()
#                     classes.append(12)
#                 elif (temp == ('Shadda', 'Kasratan')):
#                     classes.pop()
#                     classes.append(13)
#             else:
#                 diacritics.append(DIACRITIC2NAME[text[i]])
#                 temp = DIACRITIC2NAME[text[i]]
#                 if (temp == 'Fatha'):
#                     classes.append(0)
#                 elif (temp == 'Fathatan'):
#                     classes.append(1)
#                 elif (temp == 'Damma'):
#                     classes.append(2)
#                 elif (temp == 'Dammatan'):
#                     classes.append(3)
#                 elif (temp == 'Kasra'):
#                     classes.append(4)
#                 elif (temp == 'Kasratan'):
#                     classes.append(5)
#                 elif (temp == 'Sukun'):
#                     classes.append(6)
#                 elif (temp == 'Shadda'):
#                     classes.append(7)
#         elif text[i - 1] not in ARABIC_DIACRITICS:
#             diacritics.append('')
#             classes.append(14)

#     if text[-1] not in ARABIC_DIACRITICS:
#         diacritics.append('')
#         classes.append(14)
#     return diacritics, classes


# def extract_arabic_words2(text):
#     arabic_pattern = re.compile('[\u0600-\u06FF]+')
#     arabic_matches = arabic_pattern.findall(text)
#     result = ' '.join(arabic_matches)
#     processed_text = re.sub(r'[؛،\.]+', '', result)
#     final_processed_text = re.sub(r'\s+', ' ', processed_text)
#     return final_processed_text


# input_file_path = "train.txt"  # Replace with your input file path
# with open(input_file_path, "r", encoding="utf-8") as input_file:
#     input_text = input_file.read()

# arabic_words = extract_arabic_words2(input_text)

# output_words = clear_diacritics(arabic_words)
# words = output_words.split()
# words2 = arabic_words.split()
# words_array = [list(word) for word in words]
# words_array2 = [list(word2) for word2 in words2]

# output_without_spaces = arabic_words.replace(" ", "")
# output_without_spaces2 = output_words.replace(" ", "")
# array_of_chars = [char for char in output_without_spaces]
# _,classes_extraction = extract_diacritics (output_without_spaces)


# num_feature = 10
# min_word_count = 1
# num_thread = 5
# window_size = 10
# down_sampling = 0.001
# iteration = 20

# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
# model_fastText = FastText(words_array,
#                         vector_size=num_feature,
#                         window=window_size,
#                         min_count=min_word_count,
#                         workers=num_thread)


# j=0
# chars =[]
# char_vectors =[]
# char_classes=[]
# for word in words_array:
#   for char in word:
#     chars.append(char)
#     char_classes.append(classes_extraction[j])
#     vector = model_fastText.wv[char]
#     char_vectors.append(vector)
#     j=j+1

# print (j)
# print(chars[1])
# print(char_classes[1])
# print(char_vectors[1])

# # prompt: save char_vectors and char_classes to disk to free some ram

# import pickle
# with open('char_vectors.pickle', 'wb') as handle:
#   pickle.dump(char_vectors, handle, protocol=pickle.HIGHEST_PROTOCOL)

# with open('char_classes.pickle', 'wb') as handle:
#   pickle.dump(char_classes, handle, protocol=pickle.HIGHEST_PROTOCOL)

print(len(char_vectors))
print(len(char_classes))
print(char_vectors[:4])
print(char_classes[:4])

# prompt: generate pytorch class "TashkelaSet" that inherits from Dataset that takes char_vector as input X and char_classes as labels + define a function prepare_data that takes the path for the train.txt and val.txt and returns dataloaders


import torch
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
import pandas as pd
import numpy as np
import tensorflow as tf
import nltk, re
from keras.preprocessing.text import Tokenizer
from datetime import datetime
from gensim.models import *
import logging

class TashkelaSet(Dataset):
  def __init__(self, X, y):
    self.X = X
    self.y = y

  def __len__(self):
    return len(self.X)

  def __getitem__(self, idx):
    return self.X[idx], self.y[idx]

def prepare_data(train_X, train_y):

  train_set = TashkelaSet(train_X, train_y)

  train_loader = DataLoader(train_set, batch_size=100, shuffle=True)

  return train_loader

"""**Model**"""

import torch
import torch.nn as nn

class SimpleRNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, num_layers=1):
        super(SimpleRNN, self).__init__()
        self.hidden_size = hidden_size
        # RNN layer
        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)

        # Fully connected layer
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x, h0):
        # Forward pass through the RNN
        out, hn = self.rnn(x, h0)

        # Select the output from the last time step
        out = out[:, -1, :]

        # Fully connected layer
        out = self.fc(out)

        return out, hn

#####################
def train_model(model, train_loader):
    """
    Function for training the model
    """
    # define the optimization
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    # define the loss function
    criterion = nn.CrossEntropyLoss()
    # epochs
    epochs = 10
    # loop over the epochs
    for epoch in range(epochs):
        # initialize the hidden state
        h0 = torch.zeros(1, 3, model.hidden_size)
        # loop over the dataset
        for inputs, labels in train_loader:
            # zero the gradients
            optimizer.zero_grad()
            # compute the model output
            yhat, h0 = model(inputs, h0)
            # calculate loss
            loss = criterion(yhat, labels)
            # credit assignment
            loss.backward()
            # update model weights
            optimizer.step()
        # print the loss
        print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')
#####################

def evaluate_model(model, test_loader):
    """
    Function for evaluating the model
    """
    # initialize the hidden state
    h0 = torch.zeros(1, 3, model.hidden_size)
    # initialize the accuracy
    correct = 0
    total = 0
    # deactivating autograd
    with torch.no_grad():
        # loop over the test dataset
        for inputs, labels in test_loader:
            # compute the model output
            yhat, h0 = model(inputs, h0)
            # get predictions from the maximum value
            _, predicted = torch.max(yhat.data, 1)
            # update total
            total += labels.size(0)
            # update correct
            correct += (predicted == labels).sum().item()
    # compute the accuracy
    accuracy = 100 * correct / total
    # print the accuracy
    print(f'Accuracy: {accuracy:.2f}')

# # prompt: the above model consumes ram , rewrite it fixing this issue , and test it on 'train.txt' and 'val.txt'   with reasonable batch number


# import re
# from collections import Counter
# import pandas as pd
# import numpy as np
# import tensorflow as tf
# import nltk, re
# from keras.preprocessing.text import Tokenizer
# from datetime import datetime
# from gensim.models import *
# import logging
# from diacritic names to their corresponding Unicode characters.'''
# import torch
# from torch.utils.data import Dataset
# from torch.utils.data import DataLoader
# import torch.nn as nn
# print(len(char_vectors))
# print(len(char_classes))
# print(char_vectors[:4])
# print(char_classes[:4])




# class TashkelaSet(Dataset):
#   def __init__(self, X, y):
#     self.X = X
#     self.y = y

#   def __len__(self):
#     return len(self.X)

#   def __getitem__(self, idx):
#     return self.X[idx], self.y[idx]

# def prepare_data(train_X, train_y):

#   train_set = TashkelaSet(train_X, train_y)

#   train_loader = DataLoader(train_set, batch_size=4*256*10, shuffle=True)

#   return train_loader


# # **Model**


# class SimpleRNN(nn.Module):
#     def __init__(self, input_size, hidden_size, output_size, num_layers=1):
#         super(SimpleRNN, self).__init__()

#         # RNN layer
#         self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)

#         # Fully connected layer
#         self.fc = nn.Linear(hidden_size, output_size)

#     def forward(self, x, h0):
#         # Forward pass through the RNN
#         out, hn = self.rnn(x, h0)

#         # Select the output from the last time step
#         out = out[:, -1, :]

#         # Fully connected layer
#         out = self.fc(out)

#         return out, hn

# #####################
# def train_model(model, train_loader):
#     """
#     Function for training the model
#     """
#     # define the optimization
#     optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
#     # define the loss function
#     criterion = nn.CrossEntropyLoss()
#     # epochs
#     epochs = 10
#     # loop over the epochs
#     for epoch in range(epochs):
#         # initialize the hidden state
#         h0 = torch.zeros(1, 3, hidden_size)
#         # loop over the dataset
#         for inputs, labels in train_loader:
#             # zero the gradients
#             optimizer.zero_grad()
#             # compute the model output
#             yhat, h0 = model(inputs, h0)
#             # calculate loss
#             loss = criterion(yhat, labels)
#             # credit assignment
#             loss.backward()
#             # update model weights
#             optimizer.step()
#         # print the loss
#         print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')
# #####################

# def evaluate_model(model, test_loader):
#     """
#     Function for evaluating the model
#     """
#     # initialize the hidden state
#     h0 = torch.zeros(1, 3, hidden_size)
#     # initialize the accuracy
#     correct = 0
#     total = 0
#     # deactivating autograd
#     with torch.no_grad():
#         # loop over the test dataset
#         for inputs, labels in test_loader:
#             # compute the model output
#             yhat, h0 = model(inputs, h0)
#             # get predictions from the maximum value
#             _, predicted = torch.max(yhat.data, 1)
#             # update total
#             total += labels.size(0)
#             # update correct
#             correct += (predicted == labels).sum().item()
#     # compute the accuracy
#     accuracy = 100 * correct / total
#     # print the accuracy
#     print(f'Accuracy: {accuracy:.2f}')




# train_loader = prepare_data(train_X, train_y)
# test_loader = prepare_data(test_X, test_y)
# model = SimpleRNN(input_size=3, hidden_size=128, output_size=4)
# train_model(model, train_loader)
# evaluate_model(model, test_loader)



# prompt: now test the whole code in action , train , validate/evaluate and feel free to add necessary code , that name of the train file is train.txt  and validation set is val.txt

# **Data Preparation**

train_path = 'train.txt'
val_path = 'val.txt'

train_loader = prepare_data(char_vectors, char_classes)

# **Model Definition**

input_size = len(char_vectors)
hidden_size = 128
output_size = len(char_classes)

model = SimpleRNN(input_size, hidden_size, output_size)

# **Training**

# train_model(model, train_loader)

# **Evaluation**

# evaluate_model(model, val_loader)



"""# new tests

"""

# prompt: generate pytorch class "TashkelaSet" that inherits from Dataset that takes char_vector as input X and char_classes as labels + define a function prepare_data that takes the path for the train.txt and val.txt and returns dataloaders


# import torch
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
# import pandas as pd
# import numpy as np
# # import tensorflow as tf
# import nltk, re
# from keras.preprocessing.text import Tokenizer
# from datetime import datetime
# from gensim.models import *
# import logging

class TashkelaSet(Dataset):
  def __init__(self, X, y):
    self.X = X
    self.y = y

  def __len__(self):
    return len(self.X)

  def __getitem__(self, idx):
    return self.X[idx], self.y[idx]

def prepare_data(train_X, train_y, cv_X, cv_y):

  train_set = TashkelaSet(train_X, train_y)
  cv_set = TashkelaSet(cv_X, cv_y)

  train_loader = DataLoader(train_set, batch_size=32, shuffle=True)
  cv_loader = DataLoader(cv_set, batch_size=32, shuffle=True)
  return train_loader, cv_loader

train_loader, cv_loader = prepare_data(train_char_vectors, train_char_classes, cv_char_vectors, cv_char_classes)

import torch
import torch.nn as nn

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
class RNN(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, num_classes):
        super(RNN, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)#,bidirectional=True)
        # self.batch_norm = nn.BatchNorm1d(2 * hidden_size)  # Add Batch Normalization
        self.fc = nn.Linear(hidden_size, num_classes)
        #softmax layer
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):
        # Set initial hidden and cell states
        # h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)
        # c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)
        h0 = torch.zeros(self.num_layers , x.size(0), self.hidden_size).to(device)  # Adjust the num_layers
        c0 = torch.zeros(self.num_layers , x.size(0), self.hidden_size).to(device)  # Adjust the num_layers
        #adjust the shape of the input to be (batch_size, seq_length, input_size)
        x = x.view(x.size(0), -1, x.size(1))


        # Forward propagate LSTM
        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)
        # out = self.batch_norm(out[:, -1, :])

        # Decode the hidden state of the last time step
        out = self.fc(out[:, -1, :])
        out = self.softmax(out)
        return out
# def train_model(model, train_loader, cv_loader, criterion, optimizer, num_epochs=4):
#     train_losses = []
#     cv_losses = []
#     for epoch in range(num_epochs):
#         model.train()
#         running_loss = 0.0
#         for inputs, labels in train_loader:
#             inputs = inputs.to(device)
#             labels = labels.to(device)
#             # Forward pass
#             outputs = model(inputs)
#             loss = criterion(outputs, labels)
#             # Backward and optimize
#             optimizer.zero_grad()
#             loss.backward()
#             optimizer.step()
#             running_loss += loss.item()
#         train_losses.append(running_loss / len(train_loader))
#         model.eval()
#         with torch.no_grad():
#             running_loss = 0.0
#             for inputs, labels in cv_loader:
#                 inputs = inputs.to(device)
#                 labels = labels.to(device)
#                 outputs = model(inputs)
#                 loss = criterion(outputs, labels)
#                 running_loss += loss.item()
#             cv_losses.append(running_loss / len(cv_loader))
#         print('Epoch [{}/{}], Train Loss: {:.4f}, CV Loss: {:.4f}'.format(epoch+1, num_epochs, train_losses[-1], cv_losses[-1]))
#         scheduler.step()
#     # Save the model state dictionary
#     torch.save(model.state_dict(), 'LSTM_model_state.pth')
#     return train_losses, cv_losses
def train_model(model, train_loader, cv_loader, criterion, optimizer, num_epochs=4):
    train_losses = []
    cv_losses = []

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0

        for batch_idx, (inputs, labels) in enumerate(train_loader):
            inputs = inputs.to(device)
            labels = labels.to(device)

            # Forward pass
            outputs = model(inputs)
            loss = criterion(outputs, labels)

            # Backward and optimize
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

            if batch_idx % 100 == 0:
                print(f'Train Epoch [{epoch + 1}/{num_epochs}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}')

        train_losses.append(running_loss / len(train_loader))

        model.eval()
        with torch.no_grad():
            running_loss = 0.0

            for batch_idx, (inputs, labels) in enumerate(cv_loader):
                inputs = inputs.to(device)
                labels = labels.to(device)

                outputs = model(inputs)
                loss = criterion(outputs, labels)
                running_loss += loss.item()

                if batch_idx % 50 == 0:
                    print(f'Validation Epoch [{epoch + 1}/{num_epochs}], Batch [{batch_idx}/{len(cv_loader)}], Loss: {loss.item():.4f}')

            cv_losses.append(running_loss / len(cv_loader))

        print('Epoch [{}/{}], Train Loss: {:.4f}, CV Loss: {:.4f}'.format(epoch + 1, num_epochs, train_losses[-1], cv_losses[-1]))
        # scheduler.step()

    # Save the model state dictionary
    torch.save(model.state_dict(), 'LSTM_model_state.pth')

    return train_losses, cv_losses



'''to load:
# Create an instance of the model
model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)

# Load the saved state dictionary
model.load_state_dict(torch.load('best_model.pth'))

'''

def evaluate_model(model, loader):
    model.eval()
    with torch.no_grad():
        correct = 0
        total = 0

        for inputs, labels in loader:
            inputs = inputs.to(device)
            labels = labels.to(device)
            outputs = model(inputs)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum()
    return 100 * correct / total

from torch.optim.lr_scheduler import StepLR

# Add this before the training loop
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)
input_size = 30
hidden_size = 128
num_layers = 2
num_classes = 15
model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# scheduler = StepLR(optimizer, step_size=3, gamma=0.1)
train_losses, cv_losses = train_model(model, train_loader, cv_loader, criterion, optimizer,num_epochs=4)

import matplotlib.pyplot as plt
plt.plot(train_losses, label='Training loss')
plt.plot(cv_losses, label='CV loss')
plt.legend()
plt.show()

print(f'Train Accuracy: {evaluate_model(model, train_loader)}%')
print(f'CV Accuracy: {evaluate_model(model, cv_loader)}%')

# prompt: save model to pickle file
import pickle
with open('my_model.pkl', 'wb') as f:
  pickle.dump(model, f)

# prompt: load pickle file

with open('my_model.pkl', 'rb') as f:
  model = pickle.load(f)

"""Test set

"""

def get_vectors_labels_test(input_file_path):
    with open(input_file_path, "r", encoding="utf-8") as input_file:
        input_text = input_file.read()

    arabic_words = extract_arabic_words2(input_text)

    words2 = arabic_words.split()
    words_array2 = [list(word2) for word2 in words2]

    output_without_spaces = arabic_words.replace(" ", "")
    array_of_chars = list(output_without_spaces)



    num_feature = 30
    min_word_count = 1
    num_thread = 5
    window_size = 10
    down_sampling = 0.001
    iteration = 20

    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
    model_fastText = FastText(words_array2,
                            vector_size=num_feature,
                            window=window_size,
                            min_count=min_word_count,
                            workers=num_thread)


    j=0
    chars =[]
    chars_ids =[]
    char_vectors =[]
    for word in words_array2:
        for char in word:
            chars.append(char)
            chars_ids.append(j)
            vector = model_fastText.wv[char]
            char_vectors.append(vector)
            j=j+1
    return chars, char_vectors

#testing on test set
test_chars , test_char_vectors = get_vectors_labels_test("test_no_diacritics.txt")
print(len(test_chars))
print(len(test_char_vectors))
print(test_chars[:4])
print(test_char_vectors[:4])

#loading the model
import pickle
import io

# class CPU_Unpickler(pickle.Unpickler):
#     def find_class(self, module, name):
#         if module == 'torch.storage' and name == '_load_from_bytes':
#             return lambda b: torch.load(io.BytesIO(b), map_location='cpu')
#         else:
#             return super().find_class(module, name)
# model = CPU_Unpickler(open('my_model.pkl', 'rb')).load()

# prompt: load pickle file

with open('my_model.pkl', 'rb') as f:
  model = pickle.load(f)


#predicting on test set
def predict_test(model,test_char_vectors):
    model.eval()
    with torch.no_grad():
        correct = 0
        total = 0
        test_char_vectors = torch.tensor(test_char_vectors).to(device)
        test_char_vectors = test_char_vectors
        outputs = model(test_char_vectors)
        _, predicted = torch.max(outputs.data, 1)
        predicted = predicted.tolist()
        # for i in range(len(test_chars)):
        #     test_chars[i] = test_chars[i] + NAME2DIACRITIC[DIACRITIC2NAME[predicted[i]]]
        # test_chars = ''.join(test_chars)
        return predicted
predicted_chars = predict_test(model,test_char_vectors)
print(len(predicted_chars))

#saving the output to a file csv that has 2 columns (id, prediction)
import csv
j=0
with open('output.csv', 'w', newline='') as file:
    writer = csv.writer(file)
    writer.writerow(["ID", "label"])
    for i in range(len(test_chars)):
        writer.writerow([i, predicted_chars[i]])
        j=j+1


print(j)



